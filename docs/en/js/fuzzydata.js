$(document).ready(function () {indexDict['en'] = [{ "title" : "Product Features ", 
"url" : "product-features.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Product Features ", 
"snippet" : "Lyve Cloud offers several features designed to support a variety of use cases. Customers can easily store, analyze, and manage data on secure, cost-efficient Seagate storage. Lyve Cloud provides an object storage solution which allows customers to move data to and from storage buckets through an HTT...", 
"body" : "Lyve Cloud offers several features designed to support a variety of use cases. Customers can easily store, analyze, and manage data on secure, cost-efficient Seagate storage. Lyve Cloud provides an object storage solution which allows customers to move data to and from storage buckets through an HTTPS interface. Bucket access can be easily managed by admins with user-specific access control lists. With Lyve Cloud’s flexible application programming interface (API), customers can plug-in their favourite S3-compatible applications to scale data, run big data analytics, audit storage activity, and manage usage across the platform. Storage management Lyve Cloud’s single-tier design breaks away from traditional storage classes to provide uninterrupted movement of data. Objects stored in Lyve Cloud can be uploaded, downloaded, updated, and erased at any time. Using S3 Select API calls, customers can easily connect to third-party clients to move and manage data. Applications are authenticated to Lyve Cloud using an access key and secret key provisioned at service account creation. Once authenticated, applications will access buckets and objects using the defined permissions set in the service account (read-only, write-only, or all operations). All S3 API activity and actions within the Lyve Cloud console are tracked with Audit logs . Audit logs record all S3-supported API calls and activities on the console to access audit functions and track any suspicious activity. Lyve Cloud also offers features to help prevent unintended data modifications and provide versioning. Using Object Immutability prevents objects from being deleted or overwritten by any user, including the account owner, for a specified retention duration. Object Immutability also supports Amazon S3 Object Lock to reinforce Write-Once-Read-Many (WORM) policies. Customers can simply toggle-on Object Immutability at bucket creation to enable this feature, which also enables Versioning. Versioning allows customers to protect, recover, and restore every iteration of an object stored in a bucket in case of accidental deletions or failures. Versioning remains enabled even if Object Immutability is later disabled. Lyve Cloud offers Global Account Management to allow customers to create buckets in three different regions or create service accounts to access buckets in different regions. Currently, there are three regions called US-West-1 (California), US-East-1 (Virginia), and AP-Southeast-1 (Singapore). This provides simplified management of multiple regions on Lyve Cloud console and the ability to increase redundancy and availability. For more information, see Understanding Global Accounts. Customers also use Lyve Cloud Sub-Accounts to create, provision and manage additional sub-accounts to maintain a multi-level account structure. Each sub-account can function as its own storage account with the ability to manage its own users, create buckets and upload data. Learn more about Managing Lyve Cloud storage . Storage Analytics Customers can now analyze, process, move, and transform massive amounts of data on Lyve Cloud using Lyve Cloud Analytics. This platform uses big data frameworks such as Apache Spark, Trino, and ML to satisfy a variety of use cases, including scheduling, monitoring, machine learning, and more. For more information, visit Getting Started with Analytics. Lyve Cloud’s flexible backend was designed to complement a variety of compute applications. As a vendor-agnostic solution, Lyve Cloud can connect to public cloud environments such as AWS, Azure, and Google to utilize their analytics services on Lyve Cloud storage. This functionality allows businesses to consolidate, query, and perform big data analysis on cost-efficient Seagate storage. Customers can visit Lyve Cloud Marketplace to utilize validated Lyve Cloud partner solutions for compute such as Zadara zCompute and Equinix Metal. Access management and security Access management Account administrators have several tools to authorize access to Lyve Cloud users. Identity and access management (IAM) allows Lyve Cloud Administrators to manage users and their access to the console. Access is managed with user-defined roles that offer varying levels of accessibility. IAM users can use Configuring multi-factor authentication (MFA) for additional verification during login. Configuring Federated Login requires Security Assertion Markup Language (SAML) protocol to provide a single sign-on authentication method through an organization’s IDP (identity provider). Security Lyve Cloud offers security features to protect data in flight and at rest. To ensure data is protected in flight, Lyve Cloud aligns with Transport Layer Security (TLS) 1.2 protocol and leverages 256-bit Advanced Encryption Standard (AES) Galois\/Counter Mode (GCM) encryption, establishing secure communications to the client. By default, all data is encrypted before it is stored. Learn more by visiting the Data Security Overview and the Lyve Cloud Data Security Whitepaper . " }, 
{ "title" : "Availability &Durability ", 
"url" : "product-features.html#availability--durability", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Product Features \/ Availability &Durability ", 
"snippet" : "Lyve Cloud data centers are located in multiple geographic locations, including Northern California, Virginia, and Singapore, with dedicated operations staff to ensure the Lyve Cloud services are available with a monthly uptime of 99.9%. Data durability refers to long-term data protection against bi...", 
"body" : "Lyve Cloud data centers are located in multiple geographic locations, including Northern California, Virginia, and Singapore, with dedicated operations staff to ensure the Lyve Cloud services are available with a monthly uptime of 99.9%. Data durability refers to long-term data protection against bit rot or other forms of corruption over long periods. Due to Lyve Cloud’s industry-leading architecture, Lyve Cloud can achieve 11 9s of data durability making data loss statistically insignificant. " }, 
{ "title" : "Quick Start Guide ", 
"url" : "quick-start-guide.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide ", 
"snippet" : "Understanding the home page dashboard...", 
"body" : "[video] Understanding the home page dashboard " }, 
{ "title" : "Understanding Account ID ", 
"url" : "understanding-account-id.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Understanding Account ID ", 
"snippet" : "Account ID is a unique identification that is associated with the Lyve Cloud account. An account ID is unique across all Lyve Cloud accounts globally and can include your company name, which is created during the onboarding process. The account ID helps to identify and distinguish resources in one a...", 
"body" : "Account ID is a unique identification that is associated with the Lyve Cloud account. An account ID is unique across all Lyve Cloud accounts globally and can include your company name, which is created during the onboarding process. The account ID helps to identify and distinguish resources in one account from the resources in another account. While creating, the length of the account ID must be between 3 and 63 characters, where only lowercase characters, numbers, and \"-\" are allowed. You cannot change the account ID once it is created. The account ID is used to create the unique URL for the account's console URL with the following format: https:\/\/<account_ID>.console.lyvecloud.seagate.com A single URL is used to access the Lyve Cloud console, which is authenticated by the account ID. For more information, see Signing in to Lyve Cloud. " }, 
{ "title" : "Signing in to Lyve Cloud ", 
"url" : "signing-in-to-lyve-cloud.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Understanding Account ID \/ Signing in to Lyve Cloud ", 
"snippet" : "To use the Lyve Cloud console, you must sign in using your account credentials. To sign in to Lyve Cloud, you will need a login URL, which contains a unique account ID. The account ID can include your company name chosen during onboarding. You cannot change the account ID once it is created. The acc...", 
"body" : "To use the Lyve Cloud console, you must sign in using your account credentials. To sign in to Lyve Cloud, you will need a login URL, which contains a unique account ID. The account ID can include your company name chosen during onboarding. You cannot change the account ID once it is created. The account ID is unique across Lyve Cloud accounts. A single URL is used to access Lyve Cloud console which is authenticated by the account ID, and the URL has the following format: https:\/\/<account_ID>.console.lyvecloud.seagate.com . " }, 
{ "title" : "", 
"url" : "signing-in-to-lyve-cloud.html#-52055", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Understanding Account ID \/ Signing in to Lyve Cloud \/ ", 
"snippet" : "If you know the Lyve Cloud account login URL After successful onboarding, you will receive a welcome email. This email contains the Lyve Cloud URL. Using this URL, you can sign in to Lyve Cloud by creating a password. This URL is in the following format: https:\/\/<account_ID>.console.lyvecloud.seagat...", 
"body" : "If you know the Lyve Cloud account login URL After successful onboarding, you will receive a welcome email. This email contains the Lyve Cloud URL. Using this URL, you can sign in to Lyve Cloud by creating a password. This URL is in the following format: https:\/\/<account_ID>.console.lyvecloud.seagate.com . The user can bookmark or save this URL to log into the console. If you do not know the Lyve Cloud account login URL If you have not saved the URL: https:\/\/<account_ID>.console.lyvecloud.seagate.com you will not be able to login to Lyve Cloud console. But when you try to log in using the URL https:\/\/console.lyvecloud.seagate.com , you are directed to enter the Lyve Cloud Account ID to access the console. Login workflow In this case, if: You have the Account ID Enter the Account ID on the login page, followed by the registered email address. An email with all the details of the Lyve Cloud account is sent, which contains the Lyve Cloud URL to log in. You do not have the Account ID If you do not have the Account ID, you will receive your account ID by providing your registered email address. You must select Get Help on the Login page. After selecting Get Help, you are then directed to enter your email address. If the email address is not registered with Lyve Cloud, you must contact the support team at support.lyvecloud@seagate.com . Finding your Account ID in the Lyve Cloud console If you have already signed in to the Lyve Cloud account, you can view the Account ID from the Header pane. Select the username in the top right to view the Account ID. The following image highlights the Account ID in the console. " }, 
{ "title" : "Using the Lyve Cloud console ", 
"url" : "using-the-lyve-cloud-console.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Using the Lyve Cloud console ", 
"snippet" : "The Lyve Cloud console includes three panes: the header pane, left menu, and main view. The following image displays the three panes of the Lyve Cloud console. Header pane: Select the Lyve Cloud logo to return to the dashboard. The Start Here button, Help link, user name, and icon are displayed on t...", 
"body" : "The Lyve Cloud console includes three panes: the header pane, left menu, and main view. The following image displays the three panes of the Lyve Cloud console. Header pane: Select the Lyve Cloud logo to return to the dashboard. The Start Here button, Help link, user name, and icon are displayed on the top right corner of the header pane. Open the Start Here window to find quickstart resources. View our documentation and training videos under Help . To exit Lyve Cloud, select the user name and then select Logout . Left menu: The left menu is organized as follows: Home page : It is the landing page after you login to the console. It shows the number of buckets, reports, and usage and more. For more information, see Understanding the home page dashboard. Marketplace : This section displays and provides more information on partner solutions like Backup and Recovery, Surveillance, Compute, etc. that are certified with Lyve Cloud. STORAGE Buckets : Allows you to create and manage buckets. Permissions : Allows you to set the permissions for buckets. Service Accounts : Generates access credentials that enable S3 applications to perform S3 operations on the bucket. IDENTITY & ACCESS Users : Allows you to create users and set user roles. MFA : Allows you to add an additional factor to the login to prevent unauthorized access. Federated Login : Allows you to enable federated single sign-on (SSO) from your organization's Identity Provider (IdP). Notification Recipients : Allows you to add recipients to receive service and other important Lyve Cloud notifications via email. SETTINGS Settings : Allows you to enable and disable audit logs. These logs are detailed records of activities in the Lyve Cloud console and S3 API operations. Billing: Allows you to see each months' costs, and download and view previous monthly invoices. Support : Allows you to open new support tickets for any issues related to Lyve Cloud services. The non-administrator roles can only see a subset of the menu options. Main view: Displays the information corresponding to the left menu item selected. Quick Start Guide Provisioning storage buckets Understanding Global Accounts " }, 
{ "title" : "Understanding the home page dashboard ", 
"url" : "understanding-the-home-page-dashboard.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Using the Lyve Cloud console \/ Understanding the home page dashboard ", 
"snippet" : "After you log in to the Lyve Cloud console, you are taken to the dashboard's home page, and the headings on the page are displayed without data. However, if you have created buckets and are storing data in the buckets, the dashboard displays important details in the different sections. The dashboard...", 
"body" : "After you log in to the Lyve Cloud console, you are taken to the dashboard's home page, and the headings on the page are displayed without data. However, if you have created buckets and are storing data in the buckets, the dashboard displays important details in the different sections. The dashboard displays statistics of the storage system, usage, and estimated cost. A graphical view of usage trends, bucket count, and average usage are available. Home page Master Account Home page Buckets: Displays the total number of buckets. Month-to-Date Usage: Displays the average usage of the account from the beginning of the month until the current date. Estimated Cost: Displays the estimated monthly storage costs based on the current month’s usage trends. This cost is displayed in US dollars. General Reports: Daily Average Usage : Displays the daily average from a series of four usage snapshots within a 24-hour period of data stored in all the buckets. Date range selection: Select a current month, last six months, or custom time range to view usage trends. This month is a default selection that displays the daily average usage trend for the current month to date. Selecting the Last 6 months shows the usage trend of the last six months. Each data point displays the monthly average for that month. Selecting a Custom time range allows you to choose the monthly time range, and the data points display the monthly average usage. Download the usage data in CSV format by selecting Download . Use the Date range selection to select the length of time of the report. This report shows the Date, Region Name, Bucket Name, Usage(byte), Usage (GB) in the excel sheet. Usage Report: : Displays the usage of all the sub-accounts in the master account. The Sub-accounts Usage graph displays the usage of each sub-account on the same graph. The graph has different colour lines per account. Hovering over a particular day\/month (depending on view scale) displays a tooltip with the time information for all selected accounts with the line colour, account name, and usage value per sub-account account. Accounts Summary : Displays the summary of each sub-account. Customers : Lists the account ID of each sub-account in the master account. Users : Lists the number of users for each sub-account. Service Accounts : Lists the number of service accounts for each sub-account. Buckets : Lists the number of buckets created by each sub-account. Average Usage : Lists the average amount of data used per day for each sub-account, from the beginning of the month to the current date. Created On : Displays the date when the sub-account is added to the Lyve Cloud master account. Quick Start Guide Using the Lyve Cloud console Provisioning storage buckets Understanding Global Accounts " }, 
{ "title" : "Provisioning storage buckets ", 
"url" : "provisioning-storage-buckets.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Provisioning storage buckets ", 
"snippet" : "This section explains the step-by-step procedure for provisioning storage buckets in Lyve Cloud. Step 1: Creating buckets First, create a bucket to add data. To create a bucket On the left-hand menu, select Buckets , then select  Create Bucket . Enter the Bucket Name and Region and select Create . O...", 
"body" : "[video] This section explains the step-by-step procedure for provisioning storage buckets in Lyve Cloud. Step 1: Creating buckets First, create a bucket to add data. To create a bucket On the left-hand menu, select Buckets , then select  Create Bucket . Enter the Bucket Name and Region and select Create . Optionally, enable Object Immutability . After the bucket is created, it is listed on the Buckets page. For more information, see Managing buckets. Step 2: Creating bucket permissions Next, create and apply permissions to at least one bucket. Permissions define the type of operations that applications perform on the bucket: Read , Write , or All Operations (read, write, delete, and list). To create bucket permissions: On the left-hand menu, select Permissions , then select  Create Bucket Permission . On the Create Bucket Permission dialog: Name : Enter the name for permission. Permission names can contain any alphanumeric characters, dashes (“-“), underscores (“_”), or spaces. Select one from the following: Which buckets does this permission apply to? One or more existing buckets All buckets in this account with a prefix All buckets in this account Select Actions to assign privileges as All Operations, Read only, Write only Select Create to save the permission for a bucket. The Description of the permission assigned to the buckets is displayed. Alternatively, you may import policy permission files to create new permissions. See Using policy permission files . For more information on buckets, see Managing bucket access permissions. Step 3: Creating service accounts Finally, after creating permissions for a bucket, create a service account to allow applications to authenticate and use these permissions. Applications use service account credentials in API calls to access buckets to add and delete data. To create a service account: On the left-hand menu, select Service Accounts , then  Create Service Account . Enter the Service Account Name, then select applicable  Permissions from the available list. Select Create . A confirmation displays the access key and secret key required to access the bucket. Copy these account credentials or download them in CSV or JSON format before you close the dialog. The access key and secret key cannot be retrieved later. For more information on service accounts, see Managing service accounts . Quick Start Guide Using the Lyve Cloud console Understanding the home page dashboard Understanding Global Accounts " }, 
{ "title" : "Understanding Global Accounts ", 
"url" : "understanding-global-accounts.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Quick Start Guide \/ Understanding Global Accounts ", 
"snippet" : "Lyve Cloud Global Accounts let customers create buckets in different regions for increased provisioning and data access. For more information, see Creating buckets . Once you create buckets in different global accounts: The Lyve Cloud console lists all the buckets created for an account. For more in...", 
"body" : "Lyve Cloud Global Accounts let customers create buckets in different regions for increased provisioning and data access. For more information, see Creating buckets . Once you create buckets in different global accounts: The Lyve Cloud console lists all the buckets created for an account. For more information, see  Listing buckets. Listing buckets using the S3 API displays the buckets for the region that is specified in the API command. You can copy objects between different Lyve Cloud regions using S3 API commands. To access data from buckets created in different global regions: Make direct requests to one of the Lyve Cloud S3 API endpoints. For more information on S3 access points, see  S3 API endpoints. Lyve Cloud does not provide an S3 API global endpoint to access data across different global accounts. You must use the region specific endpoint to provision storage. Quick Start Guide Using the Lyve Cloud console Understanding the home page dashboard Provisioning storage buckets " }, 
{ "title" : "S3 API endpoints ", 
"url" : "s3-api-endpoints.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ S3 API endpoints ", 
"snippet" : "The following table shows Lyve Cloud regions where Lyve Cloud is currently available and the endpoints for these regions. Region Endpoint Comment US-East-1 (N. Virginia) https:\/\/s3.us-east-1.lyvecloud.seagate.com Standard Region US-East-2 (N. Virginia) https:\/\/s3.us-east-2.lyvecloud.seagate.com HIPA...", 
"body" : "The following table shows Lyve Cloud regions where Lyve Cloud is currently available and the endpoints for these regions. Region Endpoint Comment US-East-1 (N. Virginia) https:\/\/s3.us-east-1.lyvecloud.seagate.com Standard Region US-East-2 (N. Virginia) https:\/\/s3.us-east-2.lyvecloud.seagate.com HIPAA US-West-1 (N. California) https:\/\/s3.us-west-1.lyvecloud.seagate.com Standard Region AP-Southeast-1 (Singapore) https:\/\/s3.ap-southeast-1.lyvecloud.seagate.com Standard Region US-Central-1 (Oklahoma) https:\/\/s3.us-central-1.lyvecloud.seagate.com Tape Migration and Storage EU-West-1 (London) https:\/\/s3.eu-west-1.lyvecloud.seagate.com Standard Region US-Central-2 (Texas) https:\/\/s3.us-central-2.lyvecloud.seagate.com Standard Region Lyve Cloud supports path-style requests and virtual hosted-style requests available with AWS S3. Use the URL format to access a bucket using a path-style endpoint or virtual hosted-style endpoint. Lyve Cloud does not provide an S3 API global endpoint to access data across different regions. Region Path-style endpoint Virtual hosted-style endpoint US-East-1 (Virginia) https:\/\/s3.us-east-1.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.us-east-1.lyvecloud.seagate.com US-East-2 (Virginia) https:\/\/s3.us-east-2.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.us-east-2.lyvecloud.seagate.com US-West-1 (California) https:\/\/s3.us-west-1.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.us-west-1.lyvecloud.seagate.com AP-Southeast-1 (Singapore) https:\/\/s3.ap-southeast-1.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.ap-southeast-1.lyvecloud.seagate.com US-Central-1 (Oklahoma) https:\/\/s3.us-central-1.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.us-central-1.lyvecloud.seagate.com EU-West-1 (London) https:\/\/s3.eu-west-1.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.eu-west-1.lyvecloud.seagate.com US-Central-2 (Texas) https:\/\/s3.us-central-2.lyvecloud.seagate.com\/[bucket_name] https:\/\/[bucket_name].s3.us-central-2.lyvecloud.seagate.com " }, 
{ "title" : "Administrator's Guide ", 
"url" : "administrator-s-guide.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide ", 
"snippet" : "This guide provides instructions on how to create buckets, manage bucket permissions and service accounts to authenticate and access data stored in the buckets. It describes identity and access management (IAM) to manage access your Lyve Cloud resources. The Lyve Cloud console dashboard displays the...", 
"body" : "This guide provides instructions on how to create buckets, manage bucket permissions and service accounts to authenticate and access data stored in the buckets. It describes identity and access management (IAM) to manage access your Lyve Cloud resources. The Lyve Cloud console dashboard displays the storage system's overall statistics. See graphical views of usage trend, numerical values of buckets, and average usage. " }, 
{ "title" : "Console high-level workflow ", 
"url" : "console-high-level-workflow.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Console high-level workflow ", 
"snippet" : "This section explains the console workflow as determined by user roles. For more information on your assigned role, see About user roles. There are three roles available in Lyve Cloud:  Administrator Storage Administrator Auditor Storage management workflow (Storage Admin role) The storage admin use...", 
"body" : "This section explains the console workflow as determined by user roles. For more information on your assigned role, see About user roles. There are three roles available in Lyve Cloud:  Administrator Storage Administrator Auditor Storage management workflow (Storage Admin role) The storage admin user can perform all the storage operations as an Admin role user in Lyve Cloud including managing buckets, managing permissions, and creating service accounts. The storage admin user is restricted from altering settings for Identity and Access Management (IAM) and Lyve Cloud account billing. Auditor workflow Users with the Auditor role have read only access throughout the Lyve Cloud console, and are not permitted to perform any storage operations or alter settings. " }, 
{ "title" : "Administrators workflow (Admin role) ", 
"url" : "console-high-level-workflow.html#administrators-workflow--admin-role-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Console high-level workflow \/ Administrators workflow (Admin role) ", 
"snippet" : "Administrators can perform all actions available in the Lyve Cloud console. Once you sign in to the Lyve Cloud console, a dashboard is displayed. The dashboard shows details of buckets and usage-related information. For more information, see Understanding the home page dashboard. To manage Lyve Clou...", 
"body" : "Administrators can perform all actions available in the Lyve Cloud console. Once you sign in to the Lyve Cloud console, a dashboard is displayed. The dashboard shows details of buckets and usage-related information. For more information, see Understanding the home page dashboard. To manage Lyve Cloud storage: Storage is managed and provisioned in buckets. For more information, see Creating buckets. Once you create a  bucket, you must assign it Permissions, and define what operations are allowed for buckets. See  Creating bucket access permissions. After you assign permissions to a bucket, create a service account. Service accounts are used by applications to authenticate API calls accessing the bucket. For more information, see Creating a service account. Use Identity and Access Management (IAM) features to secure access to your Lyve Cloud account. For more information, see Identity and Access Management (IAM). " }, 
{ "title" : "Console session management ", 
"url" : "console-session-management.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Console session management ", 
"snippet" : "The user login session management increases the strength and security of the Lyve Cloud session. To provide more secure access, non-persistent sessions invalidate a Lyve Cloud console session cookie when the browser is closed. By default, a user session timeout is 24 hours. Users are not required to...", 
"body" : "The user login session management increases the strength and security of the Lyve Cloud session. To provide more secure access, non-persistent sessions invalidate a Lyve Cloud console session cookie when the browser is closed. By default, a user session timeout is 24 hours. Users are not required to log in with their credentials for up to 24 hours only if the Lyve Cloud session is active. The session is active after successful authentication by the user. The Lyve Cloud console automatically signs out the user after 24 hours. When users close and re-open the browser, they get a prompt for re-authentication. In summary, the Lyve Cloud console requires re-authentication in the following cases: When you sign out of the Lyve Cloud session The browser is closed without any active session, or the active Lyve Cloud tab is closed. The authenticated session is more than 24 hours. The Lyve Cloud console session remains active in the following cases: At least one Lyve Cloud active session is open, and the authentication session is less than 24 hours. " }, 
{ "title" : "Supported browsers ", 
"url" : "supported-browsers.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Console session management \/ Supported browsers ", 
"snippet" : "The Lyve Cloud Console supports the following browsers: Browser Version Google Chrome Last three versions Mozilla Firefox Last three versions Microsoft Edge Last three versions Apple Safari Last three versions...", 
"body" : "The Lyve Cloud Console supports the following browsers: Browser Version Google Chrome Last three versions Mozilla Firefox Last three versions Microsoft Edge Last three versions Apple Safari Last three versions " }, 
{ "title" : "Managing buckets ", 
"url" : "managing-buckets.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing buckets ", 
"snippet" : "Lyve Cloud allows you to store objects (like files) in buckets (like folders). Before you add or store any object, you must create a bucket. When you create a bucket, you must specify the region where you want to create the bucket. Role-based access to buckets Bucket access levels are defined by the...", 
"body" : "Lyve Cloud allows you to store objects (like files) in buckets (like folders). Before you add or store any object, you must create a bucket. When you create a bucket, you must specify the region where you want to create the bucket. Role-based access to buckets Bucket access levels are defined by the user roles. The following table describes console access to bucket features based on the user's role: Actions Admin Storage Admin Auditor (Read only) Create bucket ✓ ✓ × Edit bucket ✓ ✓ × Delete ✓ ✓ × List and View ✓ ✓ ✓ " }, 
{ "title" : "Creating buckets ", 
"url" : "managing-buckets.html#creating-buckets", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing buckets \/ Creating buckets ", 
"snippet" : "To create a bucket: On the left-hand menu, select Buckets . On the Buckets page, select Create Bucket . Enter the Bucket Name . Remember the following while creating a bucket name: The bucket name must be unique across all of Lyve Cloud. A bucket name containing a dot (.) is not allowed. After you c...", 
"body" : "To create a bucket: On the left-hand menu, select Buckets . On the Buckets page, select Create Bucket . Enter the Bucket Name . Remember the following while creating a bucket name: The bucket name must be unique across all of Lyve Cloud. A bucket name containing a dot (.) is not allowed. After you create a bucket, you cannot change the bucket name. Select the Region (Metro) from the drop-down, where you want the bucket to reside. For more information, see Understanding Global Accounts. US - Virginia (us-east-1) US - California (us-west-1) AP - Singapore - (AP-Southeast-1) You must create your first bucket in a region, using the console Optionally, enable Object Immutability. For more information, see Using object immutability. If Object Immutability is not enabled when a bucket is created, you cannot turn it On later. However if you switch it On while creating a bucket, you can later switch it Off and again switch it On. If you enable Object Immutability, you can also Set Duration to retain the objects. For more information, see  Setting duration. After you create a bucket, it is listed on the Buckets page. Sometimes there may be a delay in creating a bucket. " }, 
{ "title" : "Editing bucket properties ", 
"url" : "managing-buckets.html#editing-bucket-properties", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing buckets \/ Editing bucket properties ", 
"snippet" : "The Buckets page displays the bucket list. It also displays the labels for each bucket, such as Immutable , Versioned , and Logged . For more information on the labels, see Using object immutability and  Managing Audit Logs. To edit a bucket: On the left-hand menu, select Buckets . On the Buckets pa...", 
"body" : "The Buckets page displays the bucket list. It also displays the labels for each bucket, such as Immutable , Versioned , and Logged . For more information on the labels, see Using object immutability and  Managing Audit Logs. To edit a bucket: On the left-hand menu, select Buckets . On the Buckets page, choose and select the Name to edit. Perform any of the following actions in the bucket properties: S3 endpoint URL allows copying the S3 endpoint URL to the clipboard. This URL is used to access the bucket. For more information on the S3 endpoint URL see  S3 API endpoints. Object Immutability : You may choose to switch Off Object Immutability if it is enabled. For more information, see Using object immutabilitySet Duration: You can set duration only when Object Immutability is switched On . Select the pencil icon to edit the retention duration. For more information, see Setting duration. S3 API Audit Logs: Select the toggle switch to enable or disable the audit logs for this bucket. For more information on audit logs, see  Managing Audit Logs. After you enable the audit logs for the selected bucket, the bucket is labelled as Logged, and once you disable the audit logs, the label is removed. Delete Bucket: Select Delete , to delete a bucket. Before deleting a bucket, please make sure to: Delete all data from the bucket. Delete all Permissions referencing this bucket. Deleting a bucket associated with bucket permissions is allowed only if you have applied permission to all buckets or all buckets with a prefix in the account. Verify that the bucket is not set as the target bucket for Audit Logs. " }, 
{ "title" : "Listing buckets ", 
"url" : "managing-buckets.html#listing-buckets-3248", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing buckets \/ Listing buckets ", 
"snippet" : "To view the bucket list: On the left-hand menu, select Buckets . This view displays the labels for each bucket, such as Immutable , and Logged . For more information on the labels, see  Using object immutability and  Managing Audit Logs. By default, the Buckets page displays 10 buckets at a time. To...", 
"body" : "To view the bucket list: On the left-hand menu, select Buckets . This view displays the labels for each bucket, such as Immutable , and Logged . For more information on the labels, see  Using object immutability and  Managing Audit Logs. By default, the Buckets page displays 10 buckets at a time. To increase or decrease the number of buckets per page, select the Rows per page arrow and select 10, 25, 50, or All. Select the left or right arrow to move between the pages. The following table displays the description to the column names of the bucket list. Column Name Description Name Displays the name of the bucket. Region Displays the region where the bucket is residing. You can select the region while creating a bucket. For more information, see Creating buckets. Usage Displays the total amount of data stored in the bucket in KiB, MiB, or GiB. Created On Displays when the bucket was created in YYYY-MM-DD format. Immutable, Versioned, Logged Displays the bucket labels. Immutable: The label indicates that the bucket is in compliance mode. To disable the compliance mode, see Editing bucket propertiesVersioned: The label indicates that the bucket is versioned. The bucket version is not suspended even after you disable the Object Immutability Logged: The label indicates that audit logs are enabled for the bucket. To disable the audit logs for buckets, see Editing bucket properties. For more information on these labels, see Using object immutability and  Managing Audit Logs. " }, 
{ "title" : "Video: How to create a bucket? ", 
"url" : "managing-buckets.html#video--how-to-create-a-bucket-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing buckets \/ Video: How to create a bucket? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Using object immutability ", 
"url" : "using-object-immutability.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Using object immutability ", 
"snippet" : "Object immutability prevents objects from being deleted or overwritten by any user or application for a specified retention duration. This is especially useful when you want to meet regulatory data requirements or other scenarios where it is imperative that data cannot be changed or deleted.  Object...", 
"body" : "Object immutability prevents objects from being deleted or overwritten by any user or application for a specified retention duration. This is especially useful when you want to meet regulatory data requirements or other scenarios where it is imperative that data cannot be changed or deleted.  Object immutability must be used when you are certain that you do not want anyone, including the Administrator, to delete the objects during their retention duration. When you switch on object immutability, you must also  Set Duration  and specify the retention period. Video: How to prevent objects from being deleted? How does versioning work in object immutability? Versioning allows saving multiple variants of an object in the same bucket. It allows you to preserve, retrieve, and restore every version of an object stored in the bucket. Versioning enables the recovery of objects from any unintended or accidental user actions and application failures. After switching on object immutability for a bucket, versioning is automatically enabled, Lyve Cloud automatically creates and stores an object version each time when: A new object is uploaded An existing object is overwritten An object is deleted Versioning may increase your storage capacity utilization. For example, if you accidentally delete an object, instead of removing it permanently from Lyve Cloud, this deleted object becomes the current object version. You can then restore the previously available version. When you create a bucket and switch on object immutability, you can switch off object immutability afterwards. However, versioning cannot be suspended for that bucket. Switching on object immutability, the bucket is labelled as  Immutable  and  Versioned . Switching off object immutability only removes the Immutable label. " }, 
{ "title" : "Setting duration ", 
"url" : "using-object-immutability.html#settingduration", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Using object immutability \/ Setting duration ", 
"snippet" : "The duration for immutability can be specified in days or years at the object level. When you set the duration, objects remain locked and cannot be overwritten or deleted. By default, the duration is set to 30 days.  Setting the duration applies to individual object versions, and different versions ...", 
"body" : "The duration for immutability can be specified in days or years at the object level. When you set the duration, objects remain locked and cannot be overwritten or deleted. By default, the duration is set to 30 days.  Setting the duration applies to individual object versions, and different versions of a single object can have different durations set. For example, if you  Set Duration  to 10 days and then create an object A, object A will have its retention duration set to 10 days. If you later change the duration to 20 days and upload an object A again, in that case: The retention duration for the first version of object A remains to 10 days. The later version of the same object is set to 20 days. When you place an object in the bucket, Lyve Cloud calculates the retention duration for an object version by adding the specified duration to the object version's creation timestamp. The calculated date is stored in an object's metadata and protects the object version until the retention duration ends. When retention duration ends for an object, you can retain or manually delete an object. By default, object immutability is switched off, and you can switch it on only while creating a bucket. Once object immutability is switched on, Lyve Cloud automatically enables versioning for the bucket.  For step-by-step instructions see below. To set object immutability: Enable object immutability when creating a new bucket, see Creating buckets. or When editing a bucket: Edit a bucket. On the left-hand menu, select Buckets . On the Buckets page, select the bucket Name to edit. To set object immutability, turn on the On\/Off toggle. Optionally, edit the default value of Set Duration ,. Select the number of Day(s) or Year(s) from the list to set the retention duration. The number of days or years must be a positive integer and less than or equal to 100 years or 36500 days. Optionally, check the Delete objects after the retention duration ends check box. " }, 
{ "title" : "Managing bucket access permissions ", 
"url" : "managing-bucket-access-permissions.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions ", 
"snippet" : "Permissions are used to control access to buckets and define which actions the service accounts are allowed for a bucket. Bucket permission and Policy permission are two options available for granting permission to your buckets. Bucket permission : Bucket permission is used to set Read only , Write ...", 
"body" : "Permissions are used to control access to buckets and define which actions the service accounts are allowed for a bucket. Bucket permission and Policy permission are two options available for granting permission to your buckets. Bucket permission : Bucket permission is used to set Read only , Write only , or All operations permission for selected buckets. Using Bucket permission, you can grant access permissions to your bucket and the objects in the bucket. Only the admin and storage admin can associate permissions for the buckets. The permissions attached to the bucket apply to all of the objects in the bucket. For more information, see Creating bucket permission . Policy permission : Policy permission is used for creating policy permission by uploading a JSON file. You can also import a file which is compatible with the AWS IAM policy file. Using the Policy permission, you can allow or deny requests at a granular level based on the elements in the policy, resources, and aspects or conditions of the request. For more information, see Creating policy permission . Role-based access to permission management The following table describes access to permission management features based on your role. Actions Admin Storage Admin Auditor (Read only) Create permission ✓ ✓ × Edit ✓ ✓ × Delete ✓ ✓ × Status ✓ ✓ × List and view ✓ ✓ ✓ Viewing permissions By default, the Permissions page displays 10 permissions at a time. You can sort the columns in the table. To view all permissions: In the left-hand navigation, select Permissions . The following table describes the columns used to list permissions. Column Name Description Name Displays name of the permission. Description Displays the permission description. Type Displays the type of permission created. The type can be Policy permission and Bucket permission. Service Accounts Displays the number of service accounts using that specific permission. You can hover the mouse on the number to view the names of the attached service account and the question mark icon to view the tooltip. Creation On Displays the date and time when the permission was created in the year, day, month YY:DD:MM AM\/PM format. Select the arrow next to Rows per page to change the number of permissions to list per page. " }, 
{ "title" : "Creating bucket access permissions ", 
"url" : "managing-bucket-access-permissions.html#creating-bucket-access-permissions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Creating bucket access permissions ", 
"snippet" : "You can create bucket permissions without any buckets in the account, only if you apply the permission to all buckets in the account or all buckets with a prefix. To create bucket permissions: On the left-hand menu, select Permissions . On the Permissions page, select Create bucket permission . On t...", 
"body" : "You can create bucket permissions without any buckets in the account, only if you apply the permission to all buckets in the account or all buckets with a prefix. To create bucket permissions: On the left-hand menu, select Permissions . On the Permissions page, select Create bucket permission . On the Create bucket permission dialog, enter the following: Name : Enter a name for the permission. Which buckets does this permission apply to?: Select any one from the following: One or more existing buckets: Choose one or more buckets from the Buckets list. Buckets: The buckets field is displayed on when you select One or more existing buckets. All buckets in this account with a prefix: The bucket names must use the same few initial characters. For example, if four unique buckets for customer01 are created, such as customer01rawdata, customer01zipdata, customer01media and customer01, enter a prefix of the bucket names to assign and apply the permission. In this case, use the same beginning characters for each bucket for our prefix, customer01. Only one prefix is allowed for a single permission. The prefix field allows a maximum of 64 characters. All buckets in the account: Apply permission to all current and future buckets in the account. Actions : Select actions to assign privileges as: All Operations: Allows all operations in all buckets meeting the conditions defined under Which buckets this permission applies to? . Read only: This option allows you to perform a read only operation on one or more selected buckets and its objects. Write only: This option allows you to write objects into the selected buckets without reading them back. Once you select the desired options, the description of the permissions is displayed for that bucket permission. Select Create to save the permission for a bucket. The permissions list page displays all permissions. To manage permissions, see Editing bucket permissions  and  Deleting bucket permissions. " }, 
{ "title" : "Creating a policy permission ", 
"url" : "managing-bucket-access-permissions.html#creating-a-policy-permission", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Creating a policy permission ", 
"snippet" : "Lyve Cloud allows the migration of AWS IAM policy files to the Lyve Cloud policy permission, making it simple to start working with service accounts based on existing policies. A policy file uses a JSON file format that is compatible with an AWS IAM policy. Working with policy files allows you to sp...", 
"body" : "Lyve Cloud allows the migration of AWS IAM policy files to the Lyve Cloud policy permission, making it simple to start working with service accounts based on existing policies. A policy file uses a JSON file format that is compatible with an AWS IAM policy. Working with policy files allows you to specify the Condition element. Query the exact request values to determine when a policy is in effect, or list specific actions such as, Action: [\"s3:GetObject\",\"s3:PuObject\"] and specify the Resource element for several buckets and objects. For more information, see Example of policy permission file. How to get an IAM policy file from AWS? You must manually copy policy permission details from AWS IAM policy to use in Lyve Cloud: Login to AWS Management Console using the credentials. Select Services on the top left to view the list of services. Select IAM in Security, Identity, & Compliance . Under Access Management , select Policies and use the Search field to find the relevant policy to copy the policy details. Select the JSON tab, copy the policy details into a new file, and then save it as a JSON file. " }, 
{ "title" : "Using a policy permission file ", 
"url" : "managing-bucket-access-permissions.html#using-a-policy-permission-file", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Creating a policy permission \/ Using a policy permission file ", 
"snippet" : "The following table lists the mandatory, optional, and invalid elements in a policy permission file. Invalid elements must be removed from the file before importing, as these elements are not used in the Lyve Cloud policy permission file. Remove tags from elements available in AWS IAM policy, as tag...", 
"body" : "The following table lists the mandatory, optional, and invalid elements in a policy permission file. Invalid elements must be removed from the file before importing, as these elements are not used in the Lyve Cloud policy permission file. Remove tags from elements available in AWS IAM policy, as tags cannot be used in the policy permission file. Elements Mandatory\/Optional\/Invalid Description Statement Mandatory Contains a single statement or an array of individual statements. Resource Mandatory Specifies object(s) or bucket(s) that is related to the statement. Effect Mandatory Allows or denies access to the resource. Action Mandatory Describes specific action(s) that will be allowed or denied. Version Mandatory It defines the version of the policy language and specifies the language syntax rules that are to be used to process a policy file. Condition Optional Allows you to specify conditions when a policy is in effect. The Condition element includes expressions that match the condition keys and values in the policy file against keys and values in the request. Specifying invalid condition keys returns an error. For more information, see Known Issues. Sid Optional A statement ID. The statement ID must be unique when assigned to statements in the statement array. This value is used as sub ID for policy document's ID. Id Optional A policy identifier, such as UUID (GUID). Principal Invalid Specifies the service account that is allowed or denied to access a resource. NotPrincipal Invalid The service accounts that are not specified, are allowed or denied access to the resource. NotAction Invalid Specifies that it matches everything except the specified list of actions. If this element is part of the permission file, you need to replace it with the Action element. NotResource Invalid Specifies that it matches every resource except the available specified list. If this element is part of the permission file, you need to replace it with the resource element. " }, 
{ "title" : "Example of policy permission file ", 
"url" : "managing-bucket-access-permissions.html#example-of-policy-permission-file", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Creating a policy permission \/ Example of policy permission file ", 
"snippet" : "In the following example, the policy permission has three statements: Statement1 : Allows object listing with a prefix David in the bucket mybucket . It is done using a Condition element. Statement2 : Allows read and write operations for objects with the prefix David in bucket mybucket. Statement3 :...", 
"body" : "In the following example, the policy permission has three statements: Statement1 : Allows object listing with a prefix David in the bucket mybucket . It is done using a Condition element. Statement2 : Allows read and write operations for objects with the prefix David in bucket mybucket. Statement3 : Denies delete object operation for two resources: All the objects in mybucket\/David\/* All the objects in mycorporatebucket\/share\/marketing\/* {   \"Version\": \"2012-10-17\",   \n\"Statement\": [     \n{        \n\"Sid\": \"statement1\",       \n\"Action\": [\"s3:ListBucket\"],       \n\"Effect\": \"Allow\",       \n\"Resource\": [\"arn:aws:s3:::mybucket\"],       \n\"Condition\": {\"StringLike\": {\"s3:prefix\": [\"David\/*\"]}}     \n},     \n{       \n\"Sid\": \"statement2\",       \n\"Action\": [         \"s3:GetObject\",         \"s3:PutObject\"       ],       \n\"Effect\": \"Allow\",       \"Resource\": [\"arn:aws:s3:::mybucket\/David\/*\"]     \n},    \n{       \n\"Sid\": \"statement3\",       \n\"Action\": [\"s3:DeleteObject\"],       \n\"Effect\": \"Deny\",       \n\"Resource\": [\"arn:aws:s3:::mybucket\/David\/*\",       \n\"arn:aws:s3:::mycorporatebucket\/share\/marketing\/*\"]     \n}   \n] \n} Use the following policy to limit the bucket access to specific IP's: {  \"Version\": \"2012-10-17\",  \n \"Statement\": [    \n {\n       \"Sid\": \"Sid-1\",\n      \"Action\": [\"s3:*\"], \n      \"Effect\": \"Deny\", \n      \"Resource\": [\"arn:aws:s3:::mybucket\"], \n      \"Condition\": {\"NotIpAddress\": {\"aws:SourceIp\": [\"134.204.220.36\/32\"]}}    \n },\n    { \n     \"Sid\": \"Sid-2\", \n     \"Action\": [ \n      \"s3:*\"   \n    ],    \n   \"Effect\": \"Allow\", \n      \"Resource\": [\"arn:aws:s3:::mybucket\", \"arn:aws:s3:::mybucket\/*\"]  \n } \n ]\n} To create policy permission: On the left-hand menu, select Permissions . On the Permissions page, select Create Policy Permission . On the Create Policy Permission dialog: Enter a Name . Displays a Description for the permission. You can also edit the description. Drag and drop a policy permission file, or browse to upload a file. Once the new policy permission file is available, download or replace the existing file. Select Create. You might encounter errors if the policy permission file (JSON) has any additional or missing elements. The following is the list of possible error messages. Read them carefully and update the policy permission file accordingly. Error Message Resolution File Import Failed: Invalid JSON file. Check the JSON file structure. File Import Failed: Effect field is required. Add this element to the policy permission file. File Import Failed: Resource field is required. File Import Failed: Action field is required. File Import Failed: Statement is required. File Import Failed: Version field value is empty. Add a value to this element. File Import Failed: Action canot be empty. File Import Failed: Resource canot be empty. File Import Failed: Condition canot be empty. File Import Failed: Effect value is invalid. Add a valid value to this element. File Import Failed: Action value < action> is not valid. File Import Failed: Resource value < resource> is not valid. File Import Failed: Condition name is not valid: <condition entered> . Choose a valid condition name, such as StringLike . File Import Failed: Condition key is not valid: <condition key entered> . Choose a valid condition key, such as s3:prefix . " }, 
{ "title" : "Editing bucket permissions ", 
"url" : "managing-bucket-access-permissions.html#editing-bucket-permissions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Editing bucket permissions ", 
"snippet" : "Edit existing permissions to change selected buckets and their associated actions. To edit permissions: On the left-hand menu, select Permissions . On the Permissions page, select the ellipsis of the permission to modify, and select Edit . To modify Policy Permission -type permissions: In the Edit P...", 
"body" : "Edit existing permissions to change selected buckets and their associated actions. To edit permissions: On the left-hand menu, select Permissions . On the Permissions page, select the ellipsis of the permission to modify, and select Edit . To modify Policy Permission -type permissions: In the Edit Policy Permission dialog, edit the following: Name Description Policy File: download or replace the existing file. To modify Bucket Permission -type permissions: In the Edit Bucket Permission dialog, edit the following: Name Which buckets this permission applies to? Actions Select Save . These changes take effect as soon as the updated permission is saved, and any subsequent application API calls will be affected. " }, 
{ "title" : "Deleting bucket permissions ", 
"url" : "managing-bucket-access-permissions.html#deleting-bucket-permissions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing bucket access permissions \/ Deleting bucket permissions ", 
"snippet" : "Permissions used by any service accounts cannot be deleted. To delete permissions: In the menu, select Permissions . On the Permissions page, select the ellipsis (...) Select Delete, and select OK in the confirmation. After you delete a permission, you cannot restore. However, you can create a new p...", 
"body" : "Permissions used by any service accounts cannot be deleted. To delete permissions: In the menu, select Permissions . On the Permissions page, select the ellipsis (...) Select Delete, and select OK in the confirmation. After you delete a permission, you cannot restore. However, you can create a new permission and reuse that permission name. " }, 
{ "title" : "Managing service accounts ", 
"url" : "managing-service-accounts.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts ", 
"snippet" : "Service accounts allow applications to authenticate and access Lyve Cloud buckets and objects. The appropriate access and secret keys are generated when you create a service account. This information must be saved during the account creation, as you cannot recover key details afterwards. You must cr...", 
"body" : "Service accounts allow applications to authenticate and access Lyve Cloud buckets and objects. The appropriate access and secret keys are generated when you create a service account. This information must be saved during the account creation, as you cannot recover key details afterwards. You must create buckets and assign permission to buckets before creating a service account. For more information, see Creating buckets and Creating bucket access permissions. Role-based access to manage service accounts The following table describes access to service account features based on your role. Actions Admin Storage Admin Auditor (Read only) Create service account ✓ ✓ × Edit ✓ ✓ × Clone ✓ ✓ × Delete ✓ ✓ × Status ✓ ✓ × List and view ✓ ✓ ✓ Service Account Expiration ✓ × × Viewing service accounts The service account list displays the Access Key, expiration period, and the status of the service account. The Expires in column displays any of the following: Expired : If the service account is already expired. Never Expires : The expiration period for the service account is not configured. Value : Displays the remaining days for the service account to expire. To view the service account list: On the left-hand menu, select Service Accounts . You can view the list of service accounts. You can increase the number of service accounts per page. You can change the name from Service_Account_1 to Service_Account_01 You can add permission3 (new permission) to permission0, permission1 and permission2 (existing). Else you can remove permission0 (existing) from the available list. You can perform the following operations by selecting the ellipses for each service account: Edit service account Disable service account Clone service account Delete service account " }, 
{ "title" : "Creating a service account ", 
"url" : "managing-service-accounts.html#creating-a-service-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Creating a service account ", 
"snippet" : "You must have at least one associated permission before creating a service account. To set the duration of keys generated after service account creation, you must first configure the expiration period. If the expiration duration is not set, the service account will not have an expiration set, and th...", 
"body" : "You must have at least one associated permission before creating a service account. To set the duration of keys generated after service account creation, you must first configure the expiration period. If the expiration duration is not set, the service account will not have an expiration set, and the secret credentials will never expire. For more information, see Setting expiration duration. To create a service account: On the left-hand menu, select Service Accounts . On the Service Accounts page, select Create Service Account . Enter the Service Account Name . Select Permissions from the available list, and select Create . Selecting permissions with different Actions (All operations, read only), the action with the least priority is applied to the account. When you configure the expiration duration, the Secret Key Expiration Duration displays the days when the secret key expires. Otherwise, the expiration duration is displayed as Never . To change the expiration duration, see Setting expiration duration. If an administrator configures a new expiration duration during the same time frame as the storage administrator creates a service account, the storage administrator receives an information message about the new expiration duration. A confirmation displays the access key and secret keys required to access the bucket. Before closing the dialogue, you must copy or download the service account credentials containing the access and secret keys. Download the key in CSV or JSON format, as the secret key details cannot be retrieved later. The following image displays a generated access key and secret key. Once you create the service account, it may take a few minutes to replicate across other regions. If you cannot access your storage in a particular region, try after some time. Sometimes there may be a delay in creating a service account. " }, 
{ "title" : "Editing service accounts ", 
"url" : "managing-service-accounts.html#editing-service-accounts", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Editing service accounts ", 
"snippet" : "Editing allows you to edit the service account name and permissions. Editing does not generate a new secret key (credentials) for a service account. To generate new credentials, you must create a new or clone an existing service account. While editing the service account, the access key and expirati...", 
"body" : "Editing allows you to edit the service account name and permissions. Editing does not generate a new secret key (credentials) for a service account. To generate new credentials, you must create a new or clone an existing service account. While editing the service account, the access key and expiration period for the service account is displayed. However, you cannot edit them. The expiration period is set when you create a service account. For more information on the expiration period, see Configuring expiration period . You cannot edit a service account if the expiration period is over. If you edit Service_Account_1 , When you save this service account, the name and permission of the service account are changed. However, the secret credentials and expiration period remain the same as the original. To edit a service account: On the left menu, select Service Accounts . On the Service Accounts page, select the service account to modify and select Edit . In the Edit Service Account dialog, you can edit the service account name and modify permissions. Select or deselect the permissions to associate with the service account, and scroll to view all available permissions for the account. Select Save to save changes for the service account. " }, 
{ "title" : "Changing the status of a service account ", 
"url" : "managing-service-accounts.html#changing-the-status-of-a-service-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Changing the status of a service account ", 
"snippet" : "The service account is enabled by default. You can disable the service account anytime. Disabling a service account prevents you from using the secret key to authenticate. You cannot change the status of the service account if the expiration period is over. To change the status of a service account:...", 
"body" : "The service account is enabled by default. You can disable the service account anytime. Disabling a service account prevents you from using the secret key to authenticate. You cannot change the status of the service account if the expiration period is over. To change the status of a service account: On the left-hand menu, select Service Accounts to view the list of service accounts. Set Status to Enabled or Disabled to change the account status. " }, 
{ "title" : "Deleting service accounts ", 
"url" : "managing-service-accounts.html#deleting-service-accounts", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Deleting service accounts ", 
"snippet" : "Before you delete a service account, you can disable the key, and once you are sure that the service account is no longer needed, you can then delete the key. Deleting a service account permanently prevents you from using the secret key to authenticate. To delete a service account: On the left-hand ...", 
"body" : "Before you delete a service account, you can disable the key, and once you are sure that the service account is no longer needed, you can then delete the key. Deleting a service account permanently prevents you from using the secret key to authenticate. To delete a service account: On the left-hand menu, select Service Accounts . On the Service Accounts page, select Delete . Select Yes to delete the service account. You cannot restore a deleted account. However, you can reuse the service account name to recreate a new service account. " }, 
{ "title" : "Cloning service accounts ", 
"url" : "managing-service-accounts.html#cloning-service-accounts", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Cloning service accounts ", 
"snippet" : "Cloning a service account is a quick and easy way to create a duplicate service account. The values of the service account, like the service account name, associated permissions, etc., are the same as the original service account. However, it generates new access and secret keys. The name of the ser...", 
"body" : "Cloning a service account is a quick and easy way to create a duplicate service account. The values of the service account, like the service account name, associated permissions, etc., are the same as the original service account. However, it generates new access and secret keys. The name of the service account appears as a Copy of <service account name> , and you can change the name and associate different or same permission to this service account. To clone a service account: On the left-hand menu, select Service Accounts to view the list of service accounts. Select the ellipses to clone the service account. Select Clone , and edit the required fields of the service account. New secret credentials are generated once you create a service account. For more information, see Creating a service account. " }, 
{ "title" : "Service Account settings ", 
"url" : "service-account-settings.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Service Account settings ", 
"snippet" : "Adding the expiration duration to the service account enhances the security level of the service account. The existing Service Accounts are set as Never Expires . By default, the key never expires when creating a service account unless you configure an expiration duration. You can change the default...", 
"body" : "Adding the expiration duration to the service account enhances the security level of the service account. The existing Service Accounts are set as Never Expires . By default, the key never expires when creating a service account unless you configure an expiration duration. You can change the default setting by setting an expiry duration for all newly created service accounts; see Setting expiration duration. This limits the validity of the service account, which needs to be changed again after the expiration duration. After the expiration date, the secret key cannot be used for authentication but will stay associated with the service account until you delete it. If you disable or delete a service account, any workload that uses the service account will immediately lose access to the resources. As a best practice, change your secret keys regularly. You can create a new secret key by doing the following: Create a new service account or Clone the service account. Disable the old service account. Confirm that the old key is no longer in use. Delete the old service account. " }, 
{ "title" : "Setting expiration duration ", 
"url" : "service-account-settings.html#setting-expiration-duration", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing service accounts \/ Service Account settings \/ Setting expiration duration ", 
"snippet" : "Setting an expiration duration enables you to enforce additional security. The more often you change the service account keys, the less likely it is to be leaked. Hence, periodically invalidating your service account keys and creating new keys adds to security. The Service Account Expiration default...", 
"body" : "Setting an expiration duration enables you to enforce additional security. The more often you change the service account keys, the less likely it is to be leaked. Hence, periodically invalidating your service account keys and creating new keys adds to security. The Service Account Expiration defaults to Off (disabled). The service account key never expires when creating a service account without setting an expiration period. You can turn On (enable) the expiration and set the duration in days or years. All service accounts created after you turn On have an expiration period. For example, if you set the expiration duration as 365 days, any service account created after setting the duration has an expiration period of 365 days. Based on the specified days, the service account expires at the end of the expiration date at 23:59:59 PM, regardless of the time the service account is created. For example, Setting the expiration duration to 30 days on the 1st of the month at 10:15:00 AM, the service account expires on day 30 at 23:59:59 PM. To set expiration duration: On the left-hand menu, select Settings . Enable the Service Account Expiration toggle. Enter the number of  Day(s)  or Years to set the expiration duration, and select Save . After the configuration is complete, all new service accounts created will have an expiration duration. Once it expires, you cannot perform any actions; however, you can only delete the service account. You can configure the expiration duration as 90 days and create a service account. The Secret Key Expiration Duration in the create service account dialogue is set to 90 days. This value is displayed on pages where you create a service account, edit a service account, and list the service account. All service accounts created after configuring expiration duration will be, by default, set to 90 days. After 90 days, the service account status will appear as Expired . " }, 
{ "title" : "Managing Audit Logs ", 
"url" : "managing-audit-logs.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs ", 
"snippet" : "Audit logs are detailed records of activities in the Lyve Cloud console and S3 API operations. Audit logs are used to access audit functions and track any suspicious activity. All audit logs are written to the selected target bucket when you enable audit logging. The target bucket must be immutable,...", 
"body" : "Audit logs are detailed records of activities in the Lyve Cloud console and S3 API operations. Audit logs are used to access audit functions and track any suspicious activity. All audit logs are written to the selected target bucket when you enable audit logging. The target bucket must be immutable, which keeps audit logs immutable. For more information, see Using object immutability. You cannot switch off object immutability for the target bucket. You can maintain three types of audit logs: S3 API audit logs : This log records all supported S3 API calls. For more information, see Supported S3 API calls. S3 API audit logs are recorded in the S3-<BUCKET-NAME>-<TIMESTAMP>.gz format, where the BUCKET-NAME is the name of a bucket being logged. For more information, see Example of S3 API audit logs. IAM audit logs : This log includes all events corresponding to identity and access management actions. IAM audit logs are recorded in the IAM-<TIMESTAMP>.gz format. For more information, see Example of IAM audit log . Console audit logs : This log includes all the events that originated from the Lyve Cloud console's actions. The console audit log is recorded in the console-<TIMESTAMP>.gz format. For more information, see Example of the console audit log . Switching on the Console Audit Logs enables both the Console audit logs and IAM audit logs that are written to the target bucket. The audit log files have TIMESTAMP format: yyyy-MM-dd-HH-mm-ss' and are set to the UTC time zone. Audit log files keep sufficient information to establish which events occurred, when they occurred, and who caused them. Administrators can delete these audit log files manually after the specified retention duration ends. This helps you to cost-effectively manage the buckets. For more information, see Using object immutability. Lyve Cloud periodically saves audit logs for specified buckets. The maximum size of a log file is 500 MB. If the file size reaches 500 MB, then that log file is saved, and the logs are continued to be written in a new file. Log files are saved to the target bucket as console audit log files, IAM audit log files, or S3 API audit log files. Role-based access to permission The following table describes access to enable and disable audit logs based on your role. Actions Admin Storage Admin Auditor (Read only) Enable\/disable S3 API audit logs ✓ × × Enable\/disable console audit logs ✓ × × Edit audit log target bucket ✓ × × View audit log settings ✓ × ✓ " }, 
{ "title" : "Video: How to manage audit log settings in the Lyve Cloud console? ", 
"url" : "managing-audit-logs.html#video--how-to-manage-audit-log-settings-in-the-lyve-cloud-console-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Video: How to manage audit log settings in the Lyve Cloud console? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Enabling S3 API audit logs ", 
"url" : "s3-api-audit-logs.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling S3 API audit logs ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "s3-api-audit-logs.html#-41589", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling S3 API audit logs \/ ", 
"snippet" : "S3 API audit logs keep detailed records of activity in the Lyve Cloud console as well as S3 API operations. To enable S3 API audit logs, you must select buckets to be logged from the target buckets available in the account. See the Example of S3 API audit logs ....", 
"body" : "S3 API audit logs keep detailed records of activity in the Lyve Cloud console as well as S3 API operations. To enable S3 API audit logs, you must select buckets to be logged from the target buckets available in the account. See the Example of S3 API audit logs . " }, 
{ "title" : "Example of S3 API audit logs ", 
"url" : "s3-api-audit-logs.html#example-of-s3-api-audit-logs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling S3 API audit logs \/ \/ Example of S3 API audit logs ", 
"snippet" : "The following is an example of an S3 API audit log file. {   \"serviceAccountCreatorId\": \"john.doe@email.com\",  \"auditEntry\": {  \"api\": {   \"name\": \"PutObject\",  \"bucket\": \"bucket-1\",  \"object\": \"values-v2.yaml\",  \"status\": \"OK\",    \"statusCode\": 200,         \"timeToResponse\": \"2246401314ns\" },      ...", 
"body" : "The following is an example of an S3 API audit log file. {   \n\"serviceAccountCreatorId\": \n\"john.doe@email.com\", \n\"auditEntry\": \n { \n \"api\": \n{  \n\"name\": \"PutObject\", \n\"bucket\": \"bucket-1\", \n\"object\": \"values-v2.yaml\", \n\"status\": \"OK\",    \n\"statusCode\": 200,         \n\"timeToResponse\": \"2246401314ns\" },     \n\"time\": \"2021-01-22T10:49:30.699378337Z\",     \n\"version\": \"1\",     \n\"requestID\": \"165C883E70C2A5D0\",     \n\"userAgent\": \"aws-sdk-java\/1.12.25 Linux\/4.15.0-135-generic OpenJDK_64-Bit_Server_VM\/11.0.12+7 java\/11.0.12 vendor\/Oracle_Corporation cfg\/retry-  mode\/legacy\",     \n\"remotehost\": \"127.0.0.1\",     \n\"deploymentid\": \"ef46b1cb-6be1-4aa2-9c14-e7ffbc11986b\",    \n \"requestHeader\":{         \n \"User-Agent\": \"aws-sdk-java\/1.12.25 Linux\/4.15.0-135-generic OpenJDK_64-Bit_Server_VM\/11.0.12+7 java\/11.0.12 vendor\/Oracle_Corporation cfg\/retry-mode\/legacy\",         \n \"X-Amz-Date\": \"20210122T104928Z\",         \n \"Content-Type\": \"text\/yaml\",         \n \"Authorization\": \"AWS4-HMAC-SHA256 Credential=AHPEVYIPHVQ3XNOY\/20210122\/us-east-1\/s3\/aws4_request, SignedHeaders=content-type;host;x-amz-content-sha256;x-amz-date, Signature=<redacted>\",         \n \"Content-Length\": \"5637\",        \n \"X-Amz-Content-Sha256\": \"UNSIGNED-PAYLOAD\", \n \"X-Amz-Server-Side-Encryption\": \"AES256\" },    \n \"responseHeader\":{  \n \"ETag\": \"219857b61eb0c3dc9a3916a0992fc803\",        \n \"Vary\": \"Origin\",        \n \"Server\": \"LyveCloud\/DEVELOPMENT.2020-06-22T03-43-44Z\",        \n \"Accept-Ranges\": \"bytes\",        \n \"Content-Length\": \"0\",        \n \"X-Amz-Request-Id\": \"165C883E70C2A5D0\",        \n \"X-Xss-Protection\": \"1; mode=block\",        \n \"Content-Security-Policy\": \"block-all-mixed-content\",        \n \"X-Amz-Server-Side-Encryption\": \"AES256\" \n } \n },   \n\"serviceAccountName\": \"serv-acc-01\" \n}\n The following table describes the parameters specified in the S3 API audit log file. Parameter name Description serviceAccountCreatorId A user who created the service account. name Specifies the API name. bucket Specifies the bucket name. object Specifies the object name. status Specifies the HTTP status. statusCode Specifies the HTTP status code. timeToResponse Time for the entire request to complete. time The timestamp in UTC zone. version Represents the current version of Audit Log structure. requestID A unique request identifier. userAgent Specifies the User-Agent request header remotehost Displays IP address of the client who sent the request deploymentid A unique deployment identifier. requestHeader Specifies the request header content. responseHeader Specifies the response header content. serviceAccountName Displays the name of Service Account associated with buckets. To enable S3 API audit logs: On the left-hand menu, select Settings . On the Audit Logs Settings page, set S3 API Audit Logs to ON to begin saving audit logs. On the Audit Log Target Bucket dialog, select the target bucket from the list to store the logs. Set the target bucket only if you are setting the target bucket to write audit logs for the first time. However, if you have already set the target bucket while enabling console audit logs, you are not forced to select the target bucket. Only the buckets that are immutable are displayed in the list. Select Save . After you enable the S3 API audit log: To change the target bucket: On the Audit Log Settings page, a new section Audit Log Target Bucket is displayed. This section displays the target bucket name and bucket region. To change the target bucket, see Editing audit log target bucket. To set the S3 API audit logs. Select which buckets will have audit log. All buckets must be logged : Selecting this option allows you to set and enforce logging for all available buckets in the account. By default, this option is selected. Individually set per bucket : Selecting this option allows you to edit each bucket to enable logging manually. The All buckets must be logged and Individually set per bucket options are available only after you enable S3 API audit logs, and the target bucket is set to store logs. After selecting the Individually set per bucket option, you must choose each bucket individually and then enable the S3 API audit logs. To enable S3 API audit logs for an individual bucket, see Editing bucket properties. Once S3 audit logs are enabled, the selected bucket in the account is labeled as Logged . " }, 
{ "title" : "Disabling S3 API audit logs ", 
"url" : "s3-api-audit-logs.html#disabling-s3-api-audit-logs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling S3 API audit logs \/ Disabling S3 API audit logs ", 
"snippet" : "While enabling S3 API Audit Logs , if you select the Individually set per bucket option and later disable audit logs, the S3 API Audit Logs option will be unavailable in that individual bucket. To disable S3 API audit logs: On the left-hand menu, select Settings . On the Audit Log Settings page, set...", 
"body" : "While enabling S3 API Audit Logs , if you select the Individually set per bucket option and later disable audit logs, the S3 API Audit Logs option will be unavailable in that individual bucket. To disable S3 API audit logs: On the left-hand menu, select Settings . On the Audit Log Settings page, set S3 API Audit Logs to Off . After you switch off S3 API audit logs, the Logged label is removed from all buckets. " }, 
{ "title" : "Enabling console audit log ", 
"url" : "console-audit-log.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log ", 
"snippet" : "Enabling console audit logs enables both Console Audit Logs and IAM Audit logs . Before you enable, become familiar with the console audit logs. For more information, see Example of the console audit log ....", 
"body" : "Enabling console audit logs enables both Console Audit Logs and IAM Audit logs . Before you enable, become familiar with the console audit logs. For more information, see Example of the console audit log . " }, 
{ "title" : "Example of a console audit log ", 
"url" : "console-audit-log.html#example-of-a-console-audit-log", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log \/ Example of a console audit log ", 
"snippet" : "The following is an example of the console audit log file. {     \"ConsoleVersion\": \"DEVELOPMENT\",    \"DeploymentID\": \"dell2\",    \"LoginTime\": \"2021-01-25T09:19:11.622206Z\",    \"UserIdentity\": {       \"EventSource\": \"https:\/\/dell2.console.localhost:32428\",       \"UserName\": \"john.doe@email.com\",     ...", 
"body" : "The following is an example of the console audit log file. {    \n\"ConsoleVersion\": \"DEVELOPMENT\",    \n\"DeploymentID\": \"dell2\",    \n\"LoginTime\": \"2021-01-25T09:19:11.622206Z\",    \n\"UserIdentity\": {       \n \"EventSource\": \"https:\/\/dell2.console.localhost:32428\",       \n \"UserName\": \"john.doe@email.com\",       \n \"Role\": \"admin\",       \n \"IPAddress\": \"10.244.142.100:34310\"   \n },    \n \"ConsoleEvent\": \n {      \n \"Eventname\": \"add-new-notification-recipient\",      \n \"Status\": \"Error while inserting data to table: \",       \n \"StatusCode\": 13,       \n \"EventResponse\": \"{\\\"Action\\\":\\\"Add NotificationRecipient\\\",\\\"FirstName\\\":\\\"Fname\\\",\\\"LastName\\\":\\\"Lname\\\",\\\"Email\\\":\\\"john.doe@email.com\\\",\\\"Partner\\\":\\\"dell2\\\",\\\"AddedBy\\\":\\\"john.doe@email.com\\\"}\",       \n \"EventTime\": \"2021-01-25 09:37:01.505980988 +0000 UTC m=+1421.228517562\"  \n } \n}   The following table includes console operations that are recorded in the console audit log. The Event name column displays the names inside the console audit log as an eventname parameter value. Event name Console operation create-bucket Create bucket delete-bucket Delete bucket create-permission Create permission set-object-immutablility Set object immutability create-permission-from- imported-file Create permission from an imported file edit-permission Edit permission delete-permission Delete permission create-service-account Create service account edit-service-account Edit service account service-account-status-change Service account status change service-account-deletion Delete service account add-user Add user user-password-reset User password reset edit-user Edit user user-enabled-disabled User enabled\/disabled user-logout User log out create-support-ticket Create support ticket edit-support-ticket Edit support ticket new-comment New comment add-new-notification- recipient Add new notification recipient remove-notification-recipient Remove notification recipient edit-notification-recipient Edit notification recipient on-off-s3-api-audit-log On\/off S3 API audit log on-off-s3-console-audit-log On\/off Console audit log s3-api-audit-log-setting S3 API audit log setting s3-api-audit-log-bucket-setting S3 API audit log bucket setting The following table describes the parameters specified in the console audit log file. Parameter name Description consoleVersion Displays the console version deploymentid The unique deployment identifier loginTime The timestamp in UTC zone eventSource The console URL path userName The login ID of the user role The Lyve Cloud user role ipAddress The user identity IP address eventname Specifies the console operation status Displays the human-readable message statusCode Displays the status numeric code. For more information, see Status Code table. eventResponse Displays the resulting action performed by the event name eventTime Displays the timestamp in UTC zone " }, 
{ "title" : "Status code ", 
"url" : "console-audit-log.html#status-code", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log \/ Status code ", 
"snippet" : "The following tables provides descriptions for the StatusCode parameter. Status code Error Error details 0 OK Code OK is returned on success. 1 Cancelled Code The operation is cancelled by the client. 2 Unknown Code Specifies an unknown error. For example, errors raised by APIs that do not return en...", 
"body" : "The following tables provides descriptions for the StatusCode parameter. Status code Error Error details 0 OK Code OK is returned on success. 1 Cancelled Code The operation is cancelled by the client. 2 Unknown Code Specifies an unknown error. For example, errors raised by APIs that do not return enough error information. 3 InvalidArgument Code The client specifies an invalid argument. 4 DeadlineExceeded Code The operation has expired before completion. This error may be returned even if the operation has completed successfully, however, the response is delayed. For example, a successful response from a server could have been delayed long enough for the deadline to expire. 5 NotFound Code Requested entity (file or directory) was not found. 6 AlreadyExists CodeA An attempt to create an entity failed because one entity already exists. 7 PermissionDenied Code The caller does not have permission to execute the specified operation. 8 ResourceExhausted Code Some resource has exhausted. 9 FailedPrecondition Code The operation was rejected because the system is not in a state to execute the operation. For example, directory to be deleted may not be empty. 10 Aborted CodeT The operation was aborted due to a concurrent issue like sequencer check failures, transaction aborts, etc. 11 OutOfRange Code The operation was attempted past the valid range. For example, seeking or reading past end of a file. 12 Unimplemented Code The operation is not implemented, supported, or enabled for this service. 13 Internal Code Indicates internal errors, where some invariants have broken. 14 Unavailable Code The service is currently unavailable. 15 DataLoss Code Indicates unrecoverable data loss or corruption. 16 Unauthenticated Code The request does not have valid authentication credentials for the operation. " }, 
{ "title" : "Example of an IAM audit log ", 
"url" : "console-audit-log.html#example-of-an-iam-audit-log", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log \/ Example of an IAM audit log ", 
"snippet" : "The following is an example of an IAM audit log file. {  \"created_date\":\"2021-01-20T02:04:12.000Z\",  \"organization\":\"random-org\",  \"org_type\":\"TENANT\",  \"source\":\"console\",  \"created_by\":\"IAM\",  \"content\":{    \"date\": \"2016-02-23T19:57:29.532Z\",    \"type\": \"sapi\",    \"description\": \"\",    \"connectio...", 
"body" : "The following is an example of an IAM audit log file. {  \n\"created_date\":\"2021-01-20T02:04:12.000Z\",  \n\"organization\":\"random-org\",  \n\"org_type\":\"TENANT\",  \n\"source\":\"console\",  \n\"created_by\":\"IAM\",  \n\"content\":{    \n \"date\": \"2016-02-23T19:57:29.532Z\",    \n \"type\": \"sapi\",    \n \"description\": \"\",    \n \"connection\": \"\",    \n \"connection_id\": \"\",    \n \"client_id\": \"AaiyAPdpYdesoKnqjj8HJqRn4T5titww\",    \n \"client_name\": \"My application Name\",    \n \"ip\": \"190.257.209.19\",    \n \"hostname\": \"190.257.209.19\",    \n \"user_id\": \"auth0|56c75c4e42b6359e98374bc2\",    \n \"user_name\": \"\",    \n \"audience\": \"\",    \n \"scope\": \"\",    \n \"strategy\": \"\",    \n \"strategy_type\": \"\",    \n \"log_id\": \"\",    \n \"isMobile\": false,    \n \"details\": {},    \n \"user_agent\": \"\",    \n \"location_info\": {      \n \"country_code\": \"\",      \n \"country_code3\": \"\",      \n \"country_name\": \"\",      \n \"city_name\": \"\",      \n \"latitude\": \"\",      \n \"longitude\": \"\",      \n \"time_zone\": \"\",      \n \"continent_code\": \"\"    \n }  \n }  \n\"bucket_name\":\"\"\n} The following table describes the parameters of IAM audit log file. Parameter name Description created_date Date when the event occurred in ISO 8601 format. organization Name of the account. org_type Displays the type of organization Partner|Tenant. source Displays source of the Log. created_by Displays which service created the log. content IAM log content. bucket_name Target bucket name where the log files are stored. Optional and can be left blank. The following table describes the data field for contents in the IAM audit log file. Parameter name Description date Date when the event occurred in ISO 8601 format. type Type of event. For more information, see Event code list associated with each log event. description Description of the event. connection Name of the connection for the event. connection_id ID of the connection for the event. client_id ID of the client (application). client_name Name of the client (application). ip The IP address of the log event source. hostname Hostname where the event is applied. user_id User ID involved in the event. user_name User name involved in the event. audience API audience for whom the event is applied. scope Scope permissions applied to the event. strategy Name of the strategy involved in the event. strategy_type Type of strategy involved in the event. log_id Unique identifier of the event. isMobile Specifies if the client is a mobile device (true), desktop, laptop, or server (false). details Additional details about the event (the structure is dependent upon event type). user_agent User agents details from the client device that caused the event. location_info Displays information about the location that triggered this event based on the IP. The following table describes the data field for location_info . Parameter name Description country_code Displays the country code in two-letter Alpha-2 ISO 3166-1 format. country_code3 Displays the country code in a three-letter Alpha-3 ISO 3166-1 format. country_name Full country name. city_name Full city name. latitude Global latitude (horizontal) position. longitude Global longitude (vertical) position. time_zone Time zone name. continent_cide Displays continent of the country. For example, AF (Africa), AN (Antarctica), AS (Asia), EU (Europe), NA (North America), OC (Oceania) or SA (South America). The following table describes the event code associated with each log event. Event Code Description admin_update_launch Update launched. api_limit The maximum number of requests to the authentication API in given time has been reached. cls Passwordless login code\/link has been sent. coff AD\/LDAP connector is offline. con AD\/LDAP connector is online and working. cs Passwordless login code has been sent. depnote Deprecation notice. du User has been deleted. f Failed login. fc Failed by connector. fce Failed to change user email. fco Origin is not in the allowed origins list for the specified application. fcoa Failed cross-origin authentication. fcp Failed change password. fcph Failed post change password hook. fcpn Failed change phone number. fcpr Failed change password request. fcpro ailed to provision a AD\/LDAP connector. fcu Failed to change username. fd Failed to generate delegation token. fdeac Failed to activate device. fdeaz Device authorization request failed. fdecc User did not confirm device. fdu Failed user deletion. feacft Failed to exchange authorization code for access token. feccft Failed exchange of access token for a client credentials grant. fede Failed to exchange device code for access token. fens Failed exchange for native social login. feoobft Failed exchange of password and OOB challenge for access token. feotpft Failed exchange of password and OTP challenge for access token. fepft Failed exchange of password for access token. fepotpft Failed exchange of passwordless OTP for access token. fercft Failed exchange of password and MFA recovery code for access token. fertft Failed exchange of refresh token for access token. ferrt Failed exchange of rotating refresh token. flo User logout failed. fn Failed to send email notification. fp Failed login (incorrect password). fs Failed signup. fsa Failed silent auth. fu Failed login (invalid email\/username). fui Failed to import users. fv Failed to send verification email. fvr Failed to process verification email request. gd_auth_failed Multi-factor authentication failed. This could happen due to a wrong code entered for SMS\/Voice\/Email\/TOTP factors, or a system failure. gd_auth_rejected A user rejected a Multi-factor authentication request via push-notification. gd_auth_succeed Multi-factor authentication success. gd_enrollment_complete A first time MFA user has successfully enrolled using one of the factors. gd_otp_rate_limit_exceed A user, during enrollment or authentication, enters an incorrect code more than the maximum allowed number of times. Ex: A user enrolling in SMS enters the 6-digit code wrong more than 10 times in a row. gd_recovery_failed A user enters a wrong recovery code when attempting to authenticate. gd_recovery_rate_limit_exceed A user enters a wrong recovery code too many times. gd_recovery_succeed A user successfully authenticates with a recovery code. gd_send_pn Push notification for MFA sent successfully sent. gd_send_sms SMS for MFA successfully sent. gd_send_sms_failure Attempt to send SMS for MFA failed. gd_send_voice Voice call for MFA successfully made. gd_send_voice_failure Attempt to make Voice call for MFA failed. gd_start_auth Second factor authentication event started for MFA. gd_start_enroll Multi-factor authentication enroll has started. gd_tenant_update Guardian tenant update. gd_unenroll Device used for second factor authentication has been unenrolled. gd_update_device_account Device used for second factor authentication has been updated. limit_delegation Rate limit exceeded to \/delegation endpoint. limit_mu An IP address is blocked with 100 failed login attempts using different usernames, all with incorrect passwords in 24 hours, or 50 sign-up attempts per minute from the same IP address. limit_wc An IP address is blocked with 10 failed login attempts into a single account from the same IP address. pwd_leak Someone behind the IP address: ip attempted to login with a leaked password. s Successful sign-on event. sapi Success API operation. sce Success change email. scoa Success cross-origin authentication. scp Success change password. scph Success post change password hook. scpn Success change phone number. scpr Success change password request. scu Success change username. sd Success delegation. sdu User successfully deleted. seacft Successful exchange of authorization code for access token. seccft Successful exchange of access token for a client credentials grant. sede Successful exchange of device code for access token. sens Native social login. seoobft Successful exchange of password and OOB challenge for access token. seotpft Successful exchange of password and OTP challenge for access token. sepft Successful exchange of password for access token. sercft Successful exchange of password and MFA recovery code for access token. sertft Successful exchange of refresh token for access token. srrt Successfully revoked a refresh token. slo User successfully signed out. ss Success signup. ssa Silent auth. sui Successfully imported users. sv Verification email. svr Verification email request. sys_os_update_end Update ended. sys_os_update_start Update started. sys_update_end Update ended. sys_update_start Update started. ublkdu User block setup by anomaly detection has been released. w Warnings during login. To enable the console audit log: On the left-hand menu, select Settings . On the Audit Logs Settings page, set Console Audit Logs to ON . On the Audit Log Target Bucket dialog, select the target bucket from the list to store the logs. You must set the target bucket only if you are setting the target bucket to write audit logs for the first time. However, if you have set the target bucket while enabling S3 API audit logs, you are not forced to select the target bucket again. Only the buckets that are in immutable are displayed in the list. Select Save . " }, 
{ "title" : "Disabling console audit logs ", 
"url" : "console-audit-log.html#disabling-console-audit-logs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log \/ Disabling console audit logs ", 
"snippet" : "To disable console audit logs: On the left-hand menu, select Settings . On the Audit Logs Settings page, set Console Audit Logs to Off . After you disable audit logs: Logs are no longer saved to the target bucket. The target bucket is still visible in the Audit Logs Target Bucket section....", 
"body" : "To disable console audit logs: On the left-hand menu, select Settings . On the Audit Logs Settings page, set Console Audit Logs to Off . After you disable audit logs: Logs are no longer saved to the target bucket. The target bucket is still visible in the Audit Logs Target Bucket section. " }, 
{ "title" : "Editing audit log target bucket ", 
"url" : "console-audit-log.html#editing-audit-log-target-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing Audit Logs \/ Enabling console audit log \/ Editing audit log target bucket ", 
"snippet" : "While editing the target bucket to save audit logs, only buckets that are immutable are displayed for selection. To disable console audit logs: On the left-hand menu, select Settings . In the Audit Log Target Bucket section, select Edit . On the Edit Audit Log Target Bucket dialog, select the target...", 
"body" : "While editing the target bucket to save audit logs, only buckets that are immutable are displayed for selection. To disable console audit logs: On the left-hand menu, select Settings . In the Audit Log Target Bucket section, select Edit . On the Edit Audit Log Target Bucket dialog, select the target bucket from the Select bucket list and select Save . " }, 
{ "title" : "Identity and Access Management (IAM) ", 
"url" : "identity-and-access-management--iam-.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) ", 
"snippet" : "Lyve Cloud’s identity and access management (IAM) enables you to manage users and access to Lyve Cloud console resources. Create users and assign roles, add multi-factor authentication for enhanced security, or set up federated login using your organization’s IDP (identity provider) and SAML 2.0. Ro...", 
"body" : "Lyve Cloud’s identity and access management (IAM) enables you to manage users and access to Lyve Cloud console resources. Create users and assign roles, add multi-factor authentication for enhanced security, or set up federated login using your organization’s IDP (identity provider) and SAML 2.0. Role-based access to IAM The following table describes access to IAM features based on your role. Actions Admin Storage Admin Auditor (read only) IAM ✓ × ✓ Users ✓ × ✓ MFA ✓ × ✓ SAML Federation ✓ × ✓ Notification Recipients ✓ × ✓ " }, 
{ "title" : "Managing Users and Roles ", 
"url" : "managing-users-and-roles.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles ", 
"snippet" : "The Users page allows you to create users and set user roles. A user is an individual customer who can perform various actions in the Lyve Cloud console based on the assigned role. A role restricts the actions a user may perform, which prevents unauthorized access to Lyve Cloud features....", 
"body" : "The Users page allows you to create users and set user roles. A user is an individual customer who can perform various actions in the Lyve Cloud console based on the assigned role. A role restricts the actions a user may perform, which prevents unauthorized access to Lyve Cloud features. " }, 
{ "title" : "About user roles ", 
"url" : "managing-users-and-roles.html#about-user-roles", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ About user roles ", 
"snippet" : "You can set distinct roles for Lyve Cloud users. These users can perform actions based on assigned roles, see Role-based access sections in the respective topics. Administrator - An administrator can perform all the operations in the Lyve Cloud console. Storage Administrator - The storage administra...", 
"body" : "You can set distinct roles for Lyve Cloud users. These users can perform actions based on assigned roles, see Role-based access sections in the respective topics. Administrator - An administrator can perform all the operations in the Lyve Cloud console. Storage Administrator - The storage administrator can manage all storage-related actions that includes managing buckets, permissions and service accounts in Lyve Cloud. Auditor - An auditor has read only access to the Lyve cloud console, and thus cannot perform any actions. " }, 
{ "title" : "About user and authentication types ", 
"url" : "managing-users-and-roles.html#about-user-and-authentication-types", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ About user and authentication types ", 
"snippet" : "About authentication types Set an authentication type while creating a user. The following are the available authentication types for each of the user types. Multi-factor authentication (MFA) is enabled by default for IAM users. Multi-Factor Authentication required two authentication methods: Passwo...", 
"body" : "About authentication types Set an authentication type while creating a user. The following are the available authentication types for each of the user types. Multi-factor authentication (MFA) is enabled by default for IAM users. Multi-Factor Authentication required two authentication methods: Password : Set a password as the first factor of authentication. All IAM users must set a password. For more information, see Registration workflow for password authentication type . A password policy is applied to all user accounts created and managed directly in Lyve Cloud. For more information, see Password policy. OTP or SMS : Set SMS or OTP as the second authentication factor. For more information, see Using multi-factor authentication (MFA). Federated users have the following authentication type: Federated: This option is available only when configuring SAML Federation for the account. For more information, see Configuring Federated Login. " }, 
{ "title" : "About user types ", 
"url" : "managing-users-and-roles.html#about-user-types", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ About user and authentication types \/ About user types ", 
"snippet" : "In Lyve Cloud, there are two distinct user types: Password user: Users whose username and password are managed in Lyve Cloud Federated user: Users who are authenticated via their organization's identity provider (IdP). Federated users only exist when SAML Federation is configured on the account....", 
"body" : "In Lyve Cloud, there are two distinct user types: Password user: Users whose username and password are managed in Lyve Cloud Federated user: Users who are authenticated via their organization's identity provider (IdP). Federated users only exist when SAML Federation is configured on the account. " }, 
{ "title" : "Adding a user ", 
"url" : "managing-users-and-roles.html#adding-a-user", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Adding a user ", 
"snippet" : "To add a user: On the left-hand menu, select Users . On the Users page, select Add User. In the Add New User dialog box, enter the following and select Add User : First Name : Enter the first name of the user. Last Name : Enter the last name of the user. Email : Enter the email address of the user. ...", 
"body" : "To add a user: On the left-hand menu, select Users . On the Users page, select Add User. In the Add New User dialog box, enter the following and select Add User : First Name : Enter the first name of the user. Last Name : Enter the last name of the user. Email : Enter the email address of the user. You cannot modify the email address after adding a user. Role : Select from the options Admin, Storage Admin, Auditor. Authentication Type : If SAML Federation is not configured, no selection is required, and the following are possible display options: Password If SAML Federation is configured, no selection is required, and the following are possible display options: Federated Password Select Add User . An invitation email is sent to the IAM user to complete the registration process. For information, see Registering an IAM user in Lyve Cloud. The following image displays the Add New User dialog box . " }, 
{ "title" : "Registering an IAM user in Lyve Cloud ", 
"url" : "managing-users-and-roles.html#registering-an-iam-user-in-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Registering an IAM user in Lyve Cloud ", 
"snippet" : "When an IAM user is registered in Lyve Cloud, the user receives an email invitation. They must register in Lyve Cloud by Multi-factor Authentication (MFA), a security method that is set by default for all accounts. The following image displays a sample email invite. This email invitation link expire...", 
"body" : "When an IAM user is registered in Lyve Cloud, the user receives an email invitation. They must register in Lyve Cloud by Multi-factor Authentication (MFA), a security method that is set by default for all accounts. The following image displays a sample email invite. This email invitation link expires within 72 hours. If the user doesn't select the link within 72 hours, they should select Forgot Password on the login page. For more information, see Registering after an email invitation link expires . Check your spam folder if you believe you did not receive an email invitation or contact the support team at support.lyveloud@seagate.com to complete the registration process. " }, 
{ "title" : "Registration workflow for password authentication type ", 
"url" : "managing-users-and-roles.html#registration-workflow-for-password-authentication-type", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Registering an IAM user in Lyve Cloud \/ Registration workflow for password authentication type ", 
"snippet" : "After the user is registered in Lyve Cloud and receives an email invitation, they should complete the registration. To complete the registration: Select the link provided in the invitation email to get started. Enter Password and Confirm password in the Create Password dialog and select Create . Ref...", 
"body" : "After the user is registered in Lyve Cloud and receives an email invitation, they should complete the registration. To complete the registration: Select the link provided in the invitation email to get started. Enter Password and Confirm password in the Create Password dialog and select Create . Refer to the Password Policy while creating a new password. Once a password is created, the user is taken to the Lyve Cloud Login page. " }, 
{ "title" : "Registering after an email invitation link expires ", 
"url" : "managing-users-and-roles.html#registering-after-an-email-invitation-link-expires", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Registering an IAM user in Lyve Cloud \/ Registering after an email invitation link expires ", 
"snippet" : "To complete registration after an email invitation expires: Select the Forgot Password link. This page appears after you select the Click on this link to get started link in the invitation mail. Follow step 2 onwards from the Registration workflow for Password Authentication Type ....", 
"body" : "To complete registration after an email invitation expires: Select the Forgot Password link. This page appears after you select the Click on this link to get started link in the invitation mail. Follow step 2 onwards from the Registration workflow for Password Authentication Type . " }, 
{ "title" : "Viewing and editing a user ", 
"url" : "managing-users-and-roles.html#viewing-and-editing-a-user", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Viewing and editing a user ", 
"snippet" : "An administrator can change the first name, last name, and the assigned role of an IAM user.  Only  administrators can edit or redefine roles for users, they cannot edit or change roles defined for themselves. If a change to an administrator role is desired, a different administrator must make the c...", 
"body" : "An administrator can change the first name, last name, and the assigned role of an IAM user.  Only  administrators can edit or redefine roles for users, they cannot edit or change roles defined for themselves. If a change to an administrator role is desired, a different administrator must make the change. Once a profile is edited, the respective user must log out of Lyve Cloud and log back in for role changes to take effect. The following table describes the column names in the user's table. Column Name Description First Name Displays the user's first name. Last Name Displays the user's last name. Email Displays the user's email address. Authentication Type Displays the user's authentication type. For more information, see  About MFARole Displays role of the selected user. About user roles. Status Displays the user's status as either Enabled or Disabled . To view or edit a user On the left-hand menu, select  Users.  On the  Users  page, find the user you want to change. Select the ellipses (...) in the right-most column of the user's role and select Edit . In the Edit User dialog box, edit the following and select Save . First Name: Enter the first name of the user. Last Name: Enter the last name of the user. Select a Role to modify from the following options Admin Storage Admin Auditor You can also see the Authentication Type of the user, which is read-only and not editable. While editing a user, you cannot modify the email address. " }, 
{ "title" : "Disabling or enabling a user ", 
"url" : "managing-users-and-roles.html#disabling-or-enabling-a-user", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Viewing and editing a user \/ Disabling or enabling a user ", 
"snippet" : "To enable or disable a user On the Users  page, select the ellipses against the user and select  Disable  or  Enable . Select the ellipsis and select  Disable  or  Enable . Select  Yes , in the confirmation box to change the status....", 
"body" : "To enable or disable a user On the Users  page, select the ellipses against the user and select  Disable  or  Enable . Select the ellipsis and select  Disable  or  Enable . Select  Yes , in the confirmation box to change the status. " }, 
{ "title" : "Deleting a user ", 
"url" : "managing-users-and-roles.html#deleting-a-user", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Viewing and editing a user \/ Deleting a user ", 
"snippet" : "To delete a user: On the left-hand menu, select  Users . On the  Users  page, find the user you want to change. Select the ellipsis and select  Delete . Select  Yes  in the confirmation to change the status....", 
"body" : "To delete a user: On the left-hand menu, select  Users . On the  Users  page, find the user you want to change. Select the ellipsis and select  Delete . Select  Yes  in the confirmation to change the status. " }, 
{ "title" : "Resetting password ", 
"url" : "managing-users-and-roles.html#resetting-password", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Viewing and editing a user \/ Resetting password ", 
"snippet" : "To reset user password: On the left-hand menu, select  Users . On the  Users  page, a list of users is displayed. Select the eclipses and then select  Reset Password . Select  Yes  to reset the password....", 
"body" : "To reset user password: On the left-hand menu, select  Users . On the  Users  page, a list of users is displayed. Select the eclipses and then select  Reset Password . Select  Yes  to reset the password. " }, 
{ "title" : "Video: How to manage users and assign roles ", 
"url" : "managing-users-and-roles.html#video--how-to-manage-users-and-assign-roles", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing Users and Roles \/ Viewing and editing a user \/ Video: How to manage users and assign roles ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Password policy ", 
"url" : "password-policy.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Password policy ", 
"snippet" : "A password policy is applied to all user accounts that are created and managed directly in Lyve Cloud. The following are the considerations for password policy. Password change history Password expiration Restricting password The password policy is not applicable for federated users. The following p...", 
"body" : "A password policy is applied to all user accounts that are created and managed directly in Lyve Cloud. The following are the considerations for password policy. Password change history Password expiration Restricting password The password policy is not applicable for federated users. The following password policy options are defined and must be fulfilled. Property Requirements Characters allowed A – Z, a - z, 0 – 9, (!, @, #, $, %, ^, &;, *) Number of characters between 8 - 128 characters Password Type The password must contain three of the following four character types: A lower-case letter An upper-case letter A number A special character (!, @, #, $, %, ^, &;, *) Password restrictions Setting passwords to common options like password, 123456, 12345678, 1234, qwerty, etc. Setting passwords that contain their personal data like name, username and nickname. The first part of the user's email will also be checked firstpart@example.com " }, 
{ "title" : "Password change history ", 
"url" : "password-policy.html#password-change-history", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Password policy \/ Password change history ", 
"snippet" : "This prevents users from recycling old passwords; the last five passwords can't be used again when the user changes a password. The password change history determines the number of unique new passwords associated with a user account before an old password can be reused....", 
"body" : "This prevents users from recycling old passwords; the last five passwords can't be used again when the user changes a password. The password change history determines the number of unique new passwords associated with a user account before an old password can be reused. " }, 
{ "title" : "Password expiration ", 
"url" : "password-policy.html#password-expiration", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Password policy \/ Password expiration ", 
"snippet" : "The password expiration policy determines the period of time (in days) that a password can be used before it requires the user to change it. The password will expire 180 days from the date when the password is updated. The password expiration date is updated to 180 days once the user changes the pas...", 
"body" : "The password expiration policy determines the period of time (in days) that a password can be used before it requires the user to change it. The password will expire 180 days from the date when the password is updated. The password expiration date is updated to 180 days once the user changes the password. Example: The password that is changed on 1 January 2022 will be set to expire on 30 June 2022. Users will receive two email notifications, the first one before seven days, and another one before three days to reset the password. This email includes a link to change the password. " }, 
{ "title" : "Restricting password ", 
"url" : "password-policy.html#restricting-password", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Password policy \/ Restricting password ", 
"snippet" : "The password policy does not allow users to use the most commonly used passwords. The following restrictions include: Commonly used password See the restricted list to view the list of passwords that are not allowed. Personal data It prohibits users from setting passwords that contain any of their p...", 
"body" : "The password policy does not allow users to use the most commonly used passwords. The following restrictions include: Commonly used password See the restricted list to view the list of passwords that are not allowed. Personal data It prohibits users from setting passwords that contain any of their personal data. For example: name, username, nickname, user_metadata.name, user_metadata.first, user_metadata.last, first part of the user's email (firstpart@example.com) If the user's name is John, the user would not be allowed to include John in their password. For example, John1234 will not be allowed. " }, 
{ "title" : "Using multi-factor authentication (MFA) ", 
"url" : "configuring-multi-factor-authentication--mfa-.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Using multi-factor authentication (MFA) ", 
"snippet" : "Multi-factor authentication (MFA) is a security mechanism that adds a layer of protection to the sign-in process to access the Lyve Cloud console. Choose the one-time password (OTP) option (using third-party authenticator apps such as Google, Microsoft, or Oracle Mobile Authenticator) or the SMS tex...", 
"body" : "Multi-factor authentication (MFA) is a security mechanism that adds a layer of protection to the sign-in process to access the Lyve Cloud console. Choose the one-time password (OTP) option (using third-party authenticator apps such as Google, Microsoft, or Oracle Mobile Authenticator) or the SMS text message as the second level of authentication. By default, MFA is configured for all Lyve Cloud users and users must go through an additional registration step to complete MFA setup. The registration occurs during their successful first sign-in attempt. Users are then prompted to use the second level of authentication for subsequent login attempts. Setting cannot be changed to disable MFA. Configuring Federated Login " }, 
{ "title" : "About MFA ", 
"url" : "configuring-multi-factor-authentication--mfa-.html#about-mfa", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Using multi-factor authentication (MFA) \/ About MFA ", 
"snippet" : "MFA configuration provides these 2nd-factor authentication options: Password + SMS text message: Sets password and SMS as a two-factor authentication policy for users to sign in to the Lyve Cloud console. Password + One-Time Password (OTP): Sets password and OTP as a two-factor authentication policy...", 
"body" : "MFA configuration provides these 2nd-factor authentication options: Password + SMS text message: Sets password and SMS as a two-factor authentication policy for users to sign in to the Lyve Cloud console. Password + One-Time Password (OTP): Sets password and OTP as a two-factor authentication policy for users to sign in to the Lyve Cloud console. To use OTP, users must download a third-party authenticator to their phone. MFA login workflow How does MFA work? If MFA is enrolled All IAM users are required to set up their 2nd-factor authentication separate from their password. Users can choose one of these MFA methods: Users must login to the Lyve Cloud console. Using the 2nd-factor authentication. For more information, see MFA login workflow. Using the authenticator app: The authenticator app must be installed on your phone. Use the authenticator app to scan the QR code during the MFA enrollment. This enables the one-time password (auto-generated every 30 seconds) to be entered whenever you log in to Lyve Cloud. Using an SMS service: Use any mobile device with a phone number able to receive SMS text messages. When an MFA code is needed, Lyve Cloud sends a six-digit verification code to the phone number configured by the IAM user. Text messaging charges from the mobile carrier apply when choosing SMS-based MFA. When a user attempts to login to Lyve Cloud, a code is sent via SMS, which the user has to enter to complete the transaction. If MFA is not enrolled If any user (new user or existing user) has set a password as the first method of authentication but does not enroll in MFA, in that case the user is prompted to enroll either using SMS service or the authenticator app after successful login using the password. Users are not allowed to login into the Lyve Cloud console if they do not enroll in MFA " }, 
{ "title" : "Users login workflow ", 
"url" : "configuring-multi-factor-authentication--mfa-.html#users-login-workflow", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Using multi-factor authentication (MFA) \/ Users login workflow ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Enrolling in MFA ", 
"url" : "configuring-multi-factor-authentication--mfa-.html#enrolling-in-mfa", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Using multi-factor authentication (MFA) \/ Enrolling in MFA ", 
"snippet" : "MFA is configured for all Lyve Cloud users in the account to include an additional verification method. You can enroll after you have set the password. For more information, see Registration workflow for password authentication type. To enroll in MFA: You must set up the required MFA enrollment to a...", 
"body" : "MFA is configured for all Lyve Cloud users in the account to include an additional verification method. You can enroll after you have set the password. For more information, see Registration workflow for password authentication type. To enroll in MFA: You must set up the required MFA enrollment to access Lyve Cloud. Every time you log in to Lyve Cloud, you must follow the two-step authentication process. Lyve Cloud requests users to enter the OTP generated from the authenticator application or SMS. Login to Lyve Cloud using your credentials. Select the authenticator app or SMS as your second authentication method: If you choose the authenticator app option, scan the QR code from the authenticator app on your phone. Enter the one-time passcode (OTP) displayed on the authenticator app and select Submit . Use any 3rd-party authenticator app such as Google, Microsoft or Oracle Mobile Authenticator. The authenticator app generates a random OTP and expires within a time limit. If you choose to use SMS , select the SMS link I'd rather use SMS located below the QR code field . Select the Country code. Enter the phone number to receive the SMS passcode and select Continue . Enter the code received on your phone as an SMS and then select Submit . Once the verification code is entered, save the recovery code. Save a copy of the secret key in a secure place. If you lose the MFA device, you can use the recovery code to log in. The recovery code allows one-time login to the Lyve Cloud console. Check the I have safely recorded this code checkbox and select Submit to complete MFA enrollment. " }, 
{ "title" : "Resetting MFA for an individual IAM user ", 
"url" : "configuring-multi-factor-authentication--mfa-.html#resetting-mfa-for-an-individual-iam-user", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Using multi-factor authentication (MFA) \/ Enrolling in MFA \/ Resetting MFA for an individual IAM user ", 
"snippet" : "The Reset MFA feature allows admins to reset the IAM users' MFA enrollment. The reset action removes the old MFA entry. The user will then be unable to sign in to the Lyve Cloud console until they reset the MFA. Make sure the users have an active phone number if you want to set Password + SMS Text M...", 
"body" : "The Reset MFA feature allows admins to reset the IAM users' MFA enrollment. The reset action removes the old MFA entry. The user will then be unable to sign in to the Lyve Cloud console until they reset the MFA. Make sure the users have an active phone number if you want to set Password + SMS Text Message , or an authenticator app installed on their phone if you want to set Password + One-Time Password (OTP) as your authentication type. To reset MFA On the left-hand menu, select Users . A list of users is displayed on the Users page. Select the ellipsis next to the user, then select Reset MFA . To reset that user’s MFA, select Yes . After MFA is reset, users must re-enroll in MFA, see Enrolling in MFA. " }, 
{ "title" : "Configuring Federated Login ", 
"url" : "configuring-federated-login.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login ", 
"snippet" : "Federated Login provides authentication without revealing user login credentials to the Lyve Cloud service. Federated Login enables your users to use a single authentication method with the help of your organization’s Identity Provider (hereafter referred to as IdP) for Lyve Cloud users. Once the Ly...", 
"body" : "Federated Login provides authentication without revealing user login credentials to the Lyve Cloud service. Federated Login enables your users to use a single authentication method with the help of your organization’s Identity Provider (hereafter referred to as IdP) for Lyve Cloud users. Once the Lyve Cloud user signs in and has access to your organization's domain, the user then has direct access to the Lyve Cloud console. Hence the user need not perform a separate login process. To use Federated Login feature, your organization must have an authentication system which uses the SAML 2.0 protocol. To configure Federated Login, contact your organization's IdP administrator to obtain the metadata file in XML format. Upload this file to configure Federated Login. Security Assertion Markup Language (SAML) Protocol The Security Assertion Markup Language (SAML) protocol is an open-standard, XML-based framework for authentication and authorization between two entities without a password: A Service Provider (SP) agrees to trust the identity provider to authenticate users. An Identity Provider (IdP) authenticates users and provides to service providers an authentication assertion that indicates a user has been authenticated. In this scenario, Lyve Cloud is a Service Provider that will connect with your organization's Identity Provider to establish a Single Sign-On (SSO) access to your users. Configure Lyve Cloud as a SAML Service Provider: To configure Lyve Cloud as a SAML service provider: Obtain metadata and certificate from your IdP administrator . Configure Lyve Cloud as a service provider . Add service provider metadata to the identity provider . Configure the identity provider to send email attribute . Troubleshooting SSO If your application doesn't work the first time, you should clear your browser history and cookies before you test again. The browser may otherwise not pick up the latest version of your HTML page or it may have outdated cookies that impact execution. While troubleshooting SSO: Capture an HTTP trace of the interaction: Use any of the available tools to capture the HTTP traffic from your browser for analysis. Search for HTTP Trace Capture the login sequence from start to finish and analyze the sequence of GETs to determine how far in the expected sequence is achieved. See a redirect from your original site to the Service Provider and then to the Identity Provider. A post of credentials if you had to log in. Then a redirect back to the callback URL or the Service Provider. Finally a redirect to the callback URL specified in your application. Ensure the cookies and javascript are enabled for your browser. Check to make sure that the callback URL specified by your application in its authentication request is listed in the Allowed Callback URLs field. The http:\/\/samltool.io tool can decode a SAML assertion and is a useful debugging tool. Updating the metadata file You need to update the metadata file before the certificate expires. Contact your IdP administrator to get the updated XML file. If you make any updates and regenerate metadata.xml, you must delete the old metadata file and reupload the updated file. If you just upload the updated file, it may not make changes to the old file. To update the metadata file: On the left-hand menu, click SAML Federation . On the SAML Federation page, click Update Metadata File . Select the XML file from the desired location and click Open . After the SAML Metadata File is uploaded successfully, click Apply . Deleting existing IdP configuration To delete the IdP: On the left-hand menu, click the SAML Federation menu. On the SAML Federation page, click Delete IdP . In the Delete IdP configuration confirmation box, click Yes . Using multi-factor authentication (MFA) " }, 
{ "title" : "Step 1: Obtaining metadata and certificate from your IdP Administrator ", 
"url" : "configuring-federated-login.html#step-1--obtaining-metadata-and-certificate-from-your-idp-administrator", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Step 1: Obtaining metadata and certificate from your IdP Administrator ", 
"snippet" : "Contact your organizations IdP administrator and obtain the metadata file in XML format to upload and configure Federated Login. For more information on generating a metadata file for Okta, Google Gsuite, and Microsoft Azure, see  Generating XML Metadata files for IdP ....", 
"body" : "Contact your organizations IdP administrator and obtain the metadata file in XML format to upload and configure Federated Login. For more information on generating a metadata file for Okta, Google Gsuite, and Microsoft Azure, see  Generating XML Metadata files for IdP . " }, 
{ "title" : "Step 2: Configuring Lyve Cloud as a service provider: ", 
"url" : "configuring-federated-login.html#step-2--configuring-lyve-cloud-as-a-service-provider-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Step 2: Configuring Lyve Cloud as a service provider: ", 
"snippet" : "On the left-hand menu, click the SAML Federation menu. If you have not configured Federated Login, the status is displayed as Not Configured . On the Federated Login page, click Configure . In the Configure Federated Login page, click Upload . Select the XML file from the desired location, and selec...", 
"body" : "On the left-hand menu, click the SAML Federation menu. If you have not configured Federated Login, the status is displayed as Not Configured . On the Federated Login page, click Configure . In the Configure Federated Login page, click Upload . Select the XML file from the desired location, and select  Open . After SAML Metadata File is uploaded successfully, click Apply . You need to re-upload the file in case it is an invalid file The following image displays a Federated Login set up. After configuration Federated Login page displays the status as Configured , status name of the IdP Provider, and metadata file's Expiry Date . In addition, the Identity Provider configuration details are provided. The following attributes are used to configure the IdP: Provider URL Entity ID The following image displays sample IdP configuration details. " }, 
{ "title" : "Step 3: Adding service provider metadata to the Identity Provider ", 
"url" : "configuring-federated-login.html#step-3--adding-service-provider-metadata-to-the-identity-provider", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Step 3: Adding service provider metadata to the Identity Provider ", 
"snippet" : "In this section, you will add some information to the IdP, so it knows how to receive and respond to SAML based authentication requests from the Lyve Cloud service provider. The instructions provided here are generic. You will need to find the appropriate screens and fields on the Identity Provider....", 
"body" : "In this section, you will add some information to the IdP, so it knows how to receive and respond to SAML based authentication requests from the Lyve Cloud service provider. The instructions provided here are generic. You will need to find the appropriate screens and fields on the Identity Provider. Locate the screens from the Identity Provider that allow you to configure SAML. The IdP must know where to send the SAML assertions after it has authenticated a user. This is the Provider URL in Lyve Cloud. The IdP might call this Assertion Consumer Service URL or Application Callback URL . https:\/\/YOUR_TENANT_URL\/login\/callback?connection=YOUR_CONNECTION_NAME The connection URL parameter is required for identity provider-initiated flow. If you have custom domains set up, use the custom domain-based URL rather than your Lyve Cloud domain in the following format: https:\/\/auth.lyve.seagate.com\/login\/callback?connection=YOUR_ACCOUNT_ID Enter Entity ID in the Audience or Entity ID field from Lyve Cloud: audience:urn:lyvecloud:YOUR_TENANT:YOUR_CONNECTION_NAME If IdP provides a choice for bindings, you should select HTTP-Redirect for Authentication Requests. The Single Logout Service URL , where SAML logout requests and\/or responses from the Identity Provider must be sent and should be configured as: https:\/\/YOUR_DOMAIN\/logout Signing Logout Requests : When configuring the IdP, make sure that SAML Logout Requests sent to the service provider are signed. " }, 
{ "title" : "Step 4: Configure the identity provider to send email attribute ", 
"url" : "configuring-federated-login.html#step-4--configure-theidentity-provider-to-send-emailattribute", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Step 4: Configure the identity provider to send email attribute ", 
"snippet" : "Lyvecloud reads “email” attribute from your identity profile. Some identity providers send “email” by default, while some require you to configure it to send “email”. Auth0 and Azure Sends email by default. No additional configuration is required. G Suite It requires a configuration to send an email...", 
"body" : "Lyvecloud reads “email” attribute from your identity profile. Some identity providers send “email” by default, while some require you to configure it to send “email”. Auth0 and Azure Sends email by default. No additional configuration is required. G Suite It requires a configuration to send an email attribute. To configure an email attribute for G Suite: Login to the  GSuite  app and click  Apps  in the left menu, and then click  Web and mobile apps . Select the SAML app to edit and update attribute mapping. In the SAML attribute mapping section, click  arrow  to edit. In the Attributes mapping section, add the following attributes and click  Save . Okta It requires a configuration to send an email attribute. To configure an email attribute for Okta: On the left-hand menu select  Applications  and then click  Applications . On the  Applications  page, click the application to edit, and select  General . Click  Edit  in  SAML settings . Click Next in General Settings without making any change. In the Attribute Statements (optional) section, click Add Another to add an attribute statement, and update the following attributes: Name  = email Value  = user.email " }, 
{ "title" : "Generating XML metadata files for IdP ", 
"url" : "configuring-federated-login.html#generating-xml-metadata-files-for-idp", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP ", 
"snippet" : "Different types of IdP products have their own way of generating XML metadata files.  This section describes how to generate XML metadata files for Okta, Google GSuite, and Microsoft Azure Active Directory (AD)....", 
"body" : "Different types of IdP products have their own way of generating XML metadata files.  This section describes how to generate XML metadata files for Okta, Google GSuite, and Microsoft Azure Active Directory (AD). " }, 
{ "title" : "Okta ", 
"url" : "configuring-federated-login.html#okta", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Okta ", 
"snippet" : "Pre-requisites Create an Okta account and add a user as an administrator for configuration. Lyve Cloud account name (Tenant name) and administrators account in the console. To generate an XML file for Okta: Create an application in Okta for Lyve Cloud and log in as administrator. On the left-hand me...", 
"body" : "Pre-requisites Create an Okta account and add a user as an administrator for configuration. Lyve Cloud account name (Tenant name) and administrators account in the console. To generate an XML file for Okta: Create an application in Okta for Lyve Cloud and log in as administrator. On the left-hand menu, select  Applications  and then click  Applications . Click  Create App Integration . In the  Create a new app integration  dialog, select  SAML 2.0 , and then click  Next . In the  Create SAML Integration  section,  enter the  App name  in the  General Settings . In the  Configure SAML  section, select the following  SAML Settings . Audience URI (SP Entity ID) : Enter the SP entity ID in the following format:  urn:lyvecloud:<TENANT>-saml Single sign on URL : Enter the URL in the following format: https:\/\/auth.lyve.seagate.com\/login\/callback?connection=<TENANT>-saml. For example, consider your Lyve Cloud account (tenant) is  mylctenant1  in this case: The  Single sign on URL  is: https:\/\/auth.lyve.seagate.com\/login\/callback?connection=mylctenant1-saml The  SP Entity ID  uri: lyvecloud: mylctenant1 –saml The Single sign on URL and SP Entity ID is generated in configuring Lyve Cloud federated login 2.2. If the values are different as mentioned in the above step, you must update the attributes in the application. In the  Attribute Statements  section set the following values and click  Next . Name : email Value : user.email In the Feedback section, provide feedback to help them understand why the Okta application was configured. Select the appropriate option if you are an OKTA customer or a partner, and click  Finish . After the application is generated, you must retrieve the XML metadata file. To retrieve XML metadata file: Click  Sign On  tab. In the  Settings  section, click  Save  and add  .xml  extension to the file. This is the XML file that is used to configure Lyve Cloud federation. " }, 
{ "title" : "How can an Okta user login to Lyve Cloud? ", 
"url" : "configuring-federated-login.html#how-can-an-okta-user-login-to-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Okta \/ How can an Okta user login to Lyve Cloud? ", 
"snippet" : "To login to Lyve Cloud user as an Okta user: Add a user to your Okta account and assign apps to the users. First assign the Okta application to the users. Create a user and assign the authentication type as Federated, ensure that the account is configured as Federated Login. For more information, se...", 
"body" : "To login to Lyve Cloud user as an Okta user: Add a user to your Okta account and assign apps to the users. First assign the Okta application to the users. Create a user and assign the authentication type as Federated, ensure that the account is configured as Federated Login. For more information, see,   Adding a user . An Okta user can log in to Lyve Cloud account by two ways: Select the tile on the Okta  Home page  to connect to Lyve Cloud. You are redirected to the Lyve Cloud console and are logged in automatically. Copy the  App Embed link  from the  General  tab of the Okta application, and paste into a browser. " }, 
{ "title" : "Google GSuite ", 
"url" : "configuring-federated-login.html#google-gsuite", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Google GSuite ", 
"snippet" : "To generate an XML file for Google GSuite: Create an application in  GSuite  for Lyve Cloud and log in as administrator. Click  Apps  in the left menu, and click  Web and mobile apps . In the Web and mobile apps section, click  Add App  and then select  Add custom SAML app . In the App details secti...", 
"body" : "To generate an XML file for Google GSuite: Create an application in  GSuite  for Lyve Cloud and log in as administrator. Click  Apps  in the left menu, and click  Web and mobile apps . In the Web and mobile apps section, click  Add App  and then select  Add custom SAML app . In the App details section, provide the  App name  and optionally upload the  App icon , then select  Continue Click  DOWNLOAD METADATA  and save the file as an .xml extension, and select  Next . This XML file is used to enable SSO Federation. In the  Service Provider  details section, enter the following info: ACS URL : Enter the URL in the following format: https:\/\/auth.lyve.seagate.com\/login\/callback?connection=<TENANT>-saml Entity ID : Enter the SP entity ID in the following format:  urn:lyvecloud:&lt;TENANT&gt;-saml For example, Consider your Lyve Cloud account (tenant) is  mylctenant1  in this case: The  Single sign on URL  is  https:\/\/auth.lyve.seagate.com\/login\/callback?connection= mylctenant1 -saml The SP Entity ID is urn:lyvecloud: mylctenant1 –saml Enter the following info in the NAME ID section: In  Name ID Format , select  EMAIL  from the drop-down list. Select  Basic Information >  Primary email  from the drop-down list. Select the Continue tab as shown below. In the Attributes mapping section, add the following attributes and click  Finish . Google Directory Attributes : Primary email App attributes : email Once the app is successfully created, the configuration details are displayed. In the  User access  section, enable the user access, select  ON for everyone  and then select  Save . Download Metadata  file from the account. " }, 
{ "title" : "How can a GSuite user log in to Lyve Cloud? ", 
"url" : "configuring-federated-login.html#how-can-a-gsuite-user-log-in-to-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Google GSuite \/ How can a GSuite user log in to Lyve Cloud? ", 
"snippet" : "To login to Lyve Cloud user as an GSuite user: Log in to your Google account at  www.google.com . In the right-up corner of the page, select the Lyve Cloud app icon:...", 
"body" : "To login to Lyve Cloud user as an GSuite user: Log in to your Google account at  www.google.com . In the right-up corner of the page, select the Lyve Cloud app icon: " }, 
{ "title" : "Microsoft Azure AD ", 
"url" : "configuring-federated-login.html#microsoft-azure-ad", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Microsoft Azure AD ", 
"snippet" : "To generate an XML file for Microsoft Azure AD: Create an application in  Azure portal  for Lyve Cloud and log in as administrator. In the navigation pane, click  Azure Active Directory  and then select  Enterprise applications . In the Enterprise application section, click  New application . In the...", 
"body" : "To generate an XML file for Microsoft Azure AD: Create an application in  Azure portal  for Lyve Cloud and log in as administrator. In the navigation pane, click  Azure Active Directory  and then select  Enterprise applications . In the Enterprise application section, click  New application . In the  Browse Azure AD Gallery  section, click  Create your own application . In  Create your own application , enter a name for the application and select the purpose of using the application as  Integrate any other application you don't find in the gallery (Non-gallery) , and then select  Create . In the navigation pane, select  Single sign-on , and then click the  SAML . In the  Basic SAML Configuration  section, click  Edit and enter the following: Identifier (Entity ID) : urn:lyvecloud:&lt;tenant_short_name&gt;-saml. For example: urn:lyvecloud:seagate-saml Reply URL (Assertion Consumer Service URL) :  https:\/\/auth.lyve.seagate.com\/login\/callback?connection=&lt;tenant_short_name&gt;-saml . For example: https:\/\/lyvecloud-sandbox.us.auth0.com\/login\/callback?connection=seagate3-saml In the  SAML Signing Certificate  section, click  Download  against  Federation Metadata XML . This XML metadata file is used to upload for Federation configuration on Lyve Cloud console. For more information, see Configure Lyve Cloud as a service provider . " }, 
{ "title" : "How can a Microsoft Azure user log in to Lyve Cloud? ", 
"url" : "configuring-federated-login.html#how-can-a-microsoft-azure-user-log-in-to-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Configuring Federated Login \/ Generating XML metadata files for IdP \/ Microsoft Azure AD \/ How can a Microsoft Azure user log in to Lyve Cloud? ", 
"snippet" : "Before you login, ensure the user must be an existing AD user, so users can utilize SSO via Azure AD. To login to Lyve Cloud user as an Azure user: Log in to the Azure account as the administrator. Select Users and groups and then select  Add user\/group  to add the existing Azure user. This user is ...", 
"body" : "Before you login, ensure the user must be an existing AD user, so users can utilize SSO via Azure AD. To login to Lyve Cloud user as an Azure user: Log in to the Azure account as the administrator. Select Users and groups and then select  Add user\/group  to add the existing Azure user. This user is granted permission to use Lyve Cloud application. Select Properties and copy the  User access URL . You must provide this URL to the user to login Lyve Cloud using Azure credentials. " }, 
{ "title" : "Managing notification recipients ", 
"url" : "managing-notification-recipients.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing notification recipients ", 
"snippet" : "The notification feature allows people to receive service notices and other important Lyve Cloud information via email. Add any number of notification recipients, even if they are not registered as Lyve Cloud users. Any user that is added as an Administrator, Storage Administrator, or Auditor is aut...", 
"body" : "The notification feature allows people to receive service notices and other important Lyve Cloud information via email. Add any number of notification recipients, even if they are not registered as Lyve Cloud users. Any user that is added as an Administrator, Storage Administrator, or Auditor is automatically added to the recipient list. Once you add someone to the notification list, they start receiving service announcements and product-related emails. Recipients can stop receiving these notifications in the following ways: An administrator removes the recipient from the notification list. For more information see Removing a recipient . The recipient selects the Unsubscribe link in any notification email. An administrator disables the registered Lyve Cloud user. For more information see Disabling or enabling a user. " }, 
{ "title" : "Adding a recipient ", 
"url" : "managing-notification-recipients.html#adding-a-recipient", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing notification recipients \/ Adding a recipient ", 
"snippet" : "To add a recipient to receive notifications: On the left-hand menu, select Notifications . On the Recipients page, select Add New Recipient . Enter the First Name , Last Name , and Email , and select Add ....", 
"body" : "To add a recipient to receive notifications: On the left-hand menu, select Notifications . On the Recipients page, select Add New Recipient . Enter the First Name , Last Name , and Email , and select Add . " }, 
{ "title" : "Editing a recipient ", 
"url" : "managing-notification-recipients.html#editing-a-recipient", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing notification recipients \/ Editing a recipient ", 
"snippet" : "You can only edit data for recipients who are not registered as users in Lyve Cloud. To edit a recipient: On the left-hand menu, select Notifications . On the Notifications page, select Edit . Edit the First Name , Last Name , or Email fields, and select Edit . The recipient's list on the Notificati...", 
"body" : "You can only edit data for recipients who are not registered as users in Lyve Cloud. To edit a recipient: On the left-hand menu, select Notifications . On the Notifications page, select Edit . Edit the First Name , Last Name , or Email fields, and select Edit . The recipient's list on the Notifications page displays the changes. " }, 
{ "title" : "Removing a recipient ", 
"url" : "managing-notification-recipients.html#removing-a-recipient", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing notification recipients \/ Removing a recipient ", 
"snippet" : "To remove a recipient: On the left-hand menu, select Notifications . On the Notifications page, select Remove next to the recipient's name. Select Yes , Remove in the confirmation. The recipient is now removed from the list. If the recipient selects the Unsubscribe link in the notification email and...", 
"body" : "To remove a recipient: On the left-hand menu, select Notifications . On the Notifications page, select Remove next to the recipient's name. Select Yes , Remove in the confirmation. The recipient is now removed from the list. If the recipient selects the Unsubscribe link in the notification email and accepts the confirmation, the recipient is removed from the list. " }, 
{ "title" : "Video: How to add a notification recipient ", 
"url" : "managing-notification-recipients.html#video--how-to-add-a-notification-recipient", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Identity and Access Management (IAM) \/ Managing notification recipients \/ Video: How to add a notification recipient ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Understanding billing ", 
"url" : "understanding-billing.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing ", 
"snippet" : "Lyve Cloud uses a single-tier price model so that storage costs are predictable and simple to forecast. Invoices are provided monthly and are based on the previous month's daily average storage per terabyte used. Customers are only charged for the storage used, with no additional S3 API charges or e...", 
"body" : "Lyve Cloud uses a single-tier price model so that storage costs are predictable and simple to forecast. Invoices are provided monthly and are based on the previous month's daily average storage per terabyte used. Customers are only charged for the storage used, with no additional S3 API charges or egress fees. Role-based access Actions Admin Customer Admin Auditor (Read only) View ✓ × ✓ Download ✓ × × " }, 
{ "title" : "Viewing your invoices ", 
"url" : "understanding-billing.html#viewing-your-invoices", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing \/ Viewing your invoices ", 
"snippet" : "Your invoices are easily accessed on the Lyve Cloud Billing page. You can view the invoice date, invoice month, data usage, and cost for each monthly invoice. You may also download a PDF version from the invoice history, which is arranged by year and month. Monthly invoices are sent over email, with...", 
"body" : "Your invoices are easily accessed on the Lyve Cloud Billing page. You can view the invoice date, invoice month, data usage, and cost for each monthly invoice. You may also download a PDF version from the invoice history, which is arranged by year and month. Monthly invoices are sent over email, within the first few days of the month for the previous month’s usage. Name Description Invoice Date The date that the invoice was generated, is in MM-DD-YYYY format. Invoice Month The calendar month that the invoice bills for. Invoice Number A unique number is assigned to each invoice. Cost (USD) The invoice amount. To view and download your invoices: You can view the invoice in a PDF. Install the free Acrobat Reader . On the Billing page, find the invoice you want and select Download from the rightmost column. To view, open the invoice in PDF format. " }, 
{ "title" : "Understanding invoices ", 
"url" : "understanding-billing.html#understanding-invoices", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing \/ Understanding invoices ", 
"snippet" : "With zero add-on S3 API charges or egress fees, interpreting invoices is meant to be simple and straightforward. On line 1 of the invoice, simply view the average monthly storage used (QTY. INVOICED), data storage unit of measurement (UOM), price per terabyte (NET UNIT PRICE LIST PRICE), net price a...", 
"body" : "With zero add-on S3 API charges or egress fees, interpreting invoices is meant to be simple and straightforward. On line 1 of the invoice, simply view the average monthly storage used (QTY. INVOICED), data storage unit of measurement (UOM), price per terabyte (NET UNIT PRICE LIST PRICE), net price after sales tax (NET EXTENSION TAX), and applied discounts (TOTAL DISCOUNT %). View additional promotions on line 1.1 included in the net monthly bill (TOTAL IN USD). The image shows a sample usage for a US-based customer. Name Description QUANTITY The average monthly storage used. UOM The unit of measurement UNIT PRICE The price per terabyte. The Unit Price (9) shown in the invoice is just a placeholder. AMOUNT The net price without adding tax. Amount = Quantity * Unit price Total in USD The total price after adding tax. Total = Amount + Tax " }, 
{ "title" : "Calculating usage ", 
"url" : "understanding-billing.html#calculating-usage", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing \/ Calculating usage ", 
"snippet" : "Lyve Cloud usage comprises daily average usage, month-to-date usage, and monthly average usage. Daily average usage  is calculated by averaging four capacity usage measurements taken in 6-hour intervals during the day. For example, there are four snapshots of actual usage: one at 2 am showing 10 GB ...", 
"body" : "Lyve Cloud usage comprises daily average usage, month-to-date usage, and monthly average usage. Daily average usage  is calculated by averaging four capacity usage measurements taken in 6-hour intervals during the day. For example, there are four snapshots of actual usage: one at 2 am showing 10 GB of usage; one at 8 am showing 11 GB of usage; another at 2 pm showing 13 GB of usage, and one at 8 pm showing 14 GB of usage. The total of four usage amounts (10 + 11 + 13 + 14 = 48) divided by the number of snapshots that day (4), results in an average daily usage amount of 12 GB. Month-to-date usage  is calculated by averaging the daily average usage from the beginning of the month to the current date. Month-to-date usage is updated once every six hours, corresponding with each new daily-use snapshot. It is displayed on the Lyve Cloud dashboard. For example, the daily average usage amounts are captured from February 1 to February 4 (10,12,14 and 16). They are added (10 + 12 + 14 + 16 = 52) and then divided by the total number of days in the calendar month to date to get a current month-to-date usage of 13 GB. Monthly average usage is calculated by averaging the daily average usage of all the days in that month. The monthly average usage is used to calculate the monthly cost at the end of the month. " }, 
{ "title" : "Viewing the previous month's usage and cost ", 
"url" : "understanding-billing.html#viewing-the-previous-month-s-usage-and-cost", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing \/ Viewing the previous month's usage and cost ", 
"snippet" : "You can view a graphical representation of your monthly use in the Lyve Cloud dashboard. Simply utilize the dropdown button to select and view the average terabytes of data stored. Monthly usage by bucket can also be downloaded from this page....", 
"body" : "You can view a graphical representation of your monthly use in the Lyve Cloud dashboard. Simply utilize the dropdown button to select and view the average terabytes of data stored. Monthly usage by bucket can also be downloaded from this page. " }, 
{ "title" : "Billing page ", 
"url" : "understanding-billing.html#billing-page", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding billing \/ Billing page ", 
"snippet" : "The billing tab on the left menu of the Lyve Cloud console is available only for administrator customers who are responsible for making the payment. The following images display the billing details for sales assisted customers: Billing page for sales assisted customers...", 
"body" : "The billing tab on the left menu of the Lyve Cloud console is available only for administrator customers who are responsible for making the payment. The following images display the billing details for sales assisted customers: Billing page for sales assisted customers " }, 
{ "title" : "Understanding Sub-accounts ", 
"url" : "understanding-sub-accounts.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Sub-accounts ", 
"url" : "understanding-sub-accounts.html#sub-accounts-54523", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Sub-accounts ", 
"snippet" : "The Sub-accounts feature allows you to maintain a multi-level account structure. Lyve Cloud provisions the master account, and the master account provisions Sub-accounts. These Sub-accounts are managed by and connected to the master account. Each sub-account can provision and manage its own storage,...", 
"body" : "The Sub-accounts feature allows you to maintain a multi-level account structure. Lyve Cloud provisions the master account, and the master account provisions Sub-accounts. These Sub-accounts are managed by and connected to the master account. Each sub-account can provision and manage its own storage, users, and account settings. Large organizations can use Sub-accounts to support different departments. It can be used by Service Providers to manage their independent implementations or by storage resellers to aggregate the usage of different Sub-customers. Some items may not be available to some users, depending on the permissions applied by your administrator. " }, 
{ "title" : "Master Account ", 
"url" : "understanding-sub-accounts.html#master-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Master Account ", 
"snippet" : "Lyve Cloud provisions the master account for an organization. A master account manages the entire lifecycle of its Sub-accounts, including creating, disabling, and deleting a Sub-account. The master account has the details for each Sub-accounts usage and is responsible for billing all its Sub-accoun...", 
"body" : "Lyve Cloud provisions the master account for an organization. A master account manages the entire lifecycle of its Sub-accounts, including creating, disabling, and deleting a Sub-account. The master account has the details for each Sub-accounts usage and is responsible for billing all its Sub-accounts. The master account can create a limited number of Sub-accounts. The administrator can request to increase the limit by contacting support at support.lyvecloud@seagate.com . For more information, see Creating a ticket. The master account can view all Sub-account statistics on the dashboard, and this helps the organization identify its own and its customer's sub-accounts. The following image displays the dashboard of the master account. The master account can view a list of all Sub-accounts. When the master account administrator selects a Sub-account name, it displays the Sub-account dashboard. For more information, see Understanding sub-account dashboard . " }, 
{ "title" : "Using Support ", 
"url" : "understanding-sub-accounts.html#using-support", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Using Support ", 
"snippet" : "Sub-account and storage administrators can create a support ticket from the Lyve Cloud console. Create a support ticket, including the organization name, phone number to contact and details concerning the issue. To create a support ticket, see Creating a ticket. If you do not have a support option a...", 
"body" : "Sub-account and storage administrators can create a support ticket from the Lyve Cloud console. Create a support ticket, including the organization name, phone number to contact and details concerning the issue. To create a support ticket, see Creating a ticket. If you do not have a support option available, please contact your administrator with the required details of the issue. " }, 
{ "title" : "Managing Sub-accounts ", 
"url" : "managing-sub-accounts.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Role-based access to manage Sub-account ", 
"url" : "managing-sub-accounts.html#role-based-access-to-manage-sub-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Role-based access to manage Sub-account ", 
"snippet" : "The following table describes access to manage Sub-account features based on the user's role: Field Administrator Storage Admin Auditor (Read-only) Sub-account menu ✓ × ✓ Create Sub-account ✓ × × Disable Sub-account ✓ × × Delete Sub-account ✓ × × View Sub-account details ✓ × ✓ View all storage usage...", 
"body" : "The following table describes access to manage Sub-account features based on the user's role: Field Administrator Storage Admin Auditor (Read-only) Sub-account menu ✓ × ✓ Create Sub-account ✓ × × Disable Sub-account ✓ × × Delete Sub-account ✓ × × View Sub-account details ✓ × ✓ View all storage usage ✓ ✓ ✓ " }, 
{ "title" : "Creating a Sub-account ", 
"url" : "managing-sub-accounts.html#creating-a-sub-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Creating a Sub-account ", 
"snippet" : "Based on the user type, the fields are available for the administrator to create a Sub-account. For more information, see Role-based access to manage Sub-account. The number of Sub-accounts per master account is limited. If you need additional Sub-accounts beyond the limit, please create a support t...", 
"body" : "Based on the user type, the fields are available for the administrator to create a Sub-account. For more information, see Role-based access to manage Sub-account. The number of Sub-accounts per master account is limited. If you need additional Sub-accounts beyond the limit, please create a support ticket to request an increase to the limit. Some users may not see all the attributes for the Sub-account based on the attributes set for master account. To create a Sub-account: On the left-hand menu, select  Sub-accounts . On the  Sub-accounts management page, select  Create Sub-account . Enter the following and select Create . Organization Information Organization Name : Specify the name of the organization. The name is chosen when your first create a sub-account and is provisioned for your organization. Enable Lyve Cloud Support : This option displays the support menu option in the left navigation pane for the Sub-account. Street Address : Specify the street address of the organization. City : Specify the city name of the organization. State : Specify the state of the organization. Zip Code : Specify the zip code. Country\/Region : Specify the country or region of the organization. Phone Number : Specify the contact number. Account Information Account Name : Specify the name of the account. You specify an account name when you create a new account, while an account name uniquely identifies an account within your organization. Only alphabet letters, numbers, space, dash \"-\", and periods are allowed. The account name can only start and end with alphanumeric characters. Account ID : Account ID is unique across all the Lyve Cloud accounts. A single URL is used to access Lyve Cloud console authenticated by the account ID. The account id is used in the URL, and the URL is account specific in the following format: https:\/\/<account_ID>.console.lyvecloud.seagate.com . You cannot change the account ID once it is created. Account ID is generated immediately and is validated based on the following: Account ID length must be between 3 and 63. Account ID has lower case characters, numbers, and hyphens (-). Account ID starts and ends with alphanumeric characters. Admin Details : Specify the administrator's details managing the Sub-account. Once the account is created, the administrator will receive an email with a link to create a password for the new Lyve Cloud account. If this user exists in the master account, the administrator will receive an email with a link to the new account. First Name : Specify the name of the administrator. Last Name : Specify the last name of the administrator. Email : Specify the email address of the administrator. After registering, the administrator will receive all notifications on the registered email. Confirm Email : Re-enter the email to verify the email mentioned in the Email field. " }, 
{ "title" : "Listing Sub-accounts ", 
"url" : "managing-sub-accounts.html#listing-sub-accounts", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Listing Sub-accounts ", 
"snippet" : "Sub-accounts are listed under the Sub-accounts management page in the master account. You can search the Sub-accounts by name. Column Name Description Account Name Displays the name of the account. Account ID Displays the account ID of the Sub-account. This is a unique ID across Lyve Cloud. Organiza...", 
"body" : "Sub-accounts are listed under the Sub-accounts management page in the master account. You can search the Sub-accounts by name. Column Name Description Account Name Displays the name of the account. Account ID Displays the account ID of the Sub-account. This is a unique ID across Lyve Cloud. Organization Name Displays the name of the organization. Created On Displays the date and time when the Sub-account is created. Status Displays the status of the Sub-account. The status can be Enabled or Disabled . Trial Displays the remaining number of days for the trial to expire. Exporting the Sub-account list By selecting Download , the administrator of the master account can export the Sub-account list in the CSV file. " }, 
{ "title" : "Disabling\/Enabling a Sub-account ", 
"url" : "managing-sub-accounts.html#disabling-enabling-a-sub-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Disabling\/Enabling a Sub-account ", 
"snippet" : "Disabling a Sub-account restricts all account users from accessing the console and all its service accounts from accessing storage. After the account is disabled, the master account is not billed for that period. You must contact the administrator if the option to disable an account is unavailable. ...", 
"body" : "Disabling a Sub-account restricts all account users from accessing the console and all its service accounts from accessing storage. After the account is disabled, the master account is not billed for that period. You must contact the administrator if the option to disable an account is unavailable. You can enable a Sub-account if it is disabled. You can enable the account with all data access and its users without restrictions. Enabling a disabled account permits all active users to access the console, and the service accounts are re-enabled. To disable a Sub-account On the left-hand menu, select  Sub-accounts . Select the ellipsis for the account to disable, and then select Disable . Select Yes , in the Disable Sub-account confirmation dialog. Verify the Sub-account name to disable, and select Confirm . A confirmation message is displayed on the Sub-account list page. Alternatively, you can disable a Sub-account from the Sub-accounts dashboard from the Settings tab. " }, 
{ "title" : "Deleting Sub-account ", 
"url" : "managing-sub-accounts.html#deleting-sub-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Disabling\/Enabling a Sub-account \/ Deleting Sub-account ", 
"snippet" : "You can delete a Sub-account if it is disabled for at least 30 days. You can confirm the status of the disabled account on the Sub-account detail page. Once the Sub-account is deleted, all the data, including buckets, objects, service accounts, users, etc., are also deleted and cannot be restored. T...", 
"body" : "You can delete a Sub-account if it is disabled for at least 30 days. You can confirm the status of the disabled account on the Sub-account detail page. Once the Sub-account is deleted, all the data, including buckets, objects, service accounts, users, etc., are also deleted and cannot be restored. To delete a Sub-account On the left-hand menu, select  Sub-accounts . Select the Sub-account to delete. Select the Settings tab on the Sub-account detail page and then select Delete . " }, 
{ "title" : "Video: How to use the Sub-Account feature of Lyve Cloud? ", 
"url" : "managing-sub-accounts.html#video--how-to-use-the-sub-account-feature-of-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Managing Sub-accounts \/ Video: How to use the Sub-Account feature of Lyve Cloud? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Understanding Sub-account usage dashboard ", 
"url" : "understanding-sub-account-usage-dashboard.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Understanding Sub-account usage dashboard ", 
"snippet" : "Sub-account usage is monitored from the Sub-account usage dashboard. The Sub-account dashboard displays statistics of the storage system, including usage, the number of buckets, daily average usage etc. After selecting a Sub-account on the left navigation pane, you can view the dashboard. Initially,...", 
"body" : "Sub-account usage is monitored from the Sub-account usage dashboard. The Sub-account dashboard displays statistics of the storage system, including usage, the number of buckets, daily average usage etc. After selecting a Sub-account on the left navigation pane, you can view the dashboard. Initially, there is no data on the dashboard. Once you create Sub-accounts and the accounts begin storing data in the buckets, the dashboard displays essential details in different sections. A graphical view of daily average usage with the date range filter displays the storage usage trend. There are two tabs on the dashboard displaying the Usage information and Settings for the Sub-account. Month-to-date Usage : Displays the average usage of the account from the beginning of the month until the current date. Buckets : Displays the total number of buckets created in the Sub-account. Created on : Displays the date and time of Sub-account creation. Created by : Displays the administrator's name who created the Sub-account. Usage : The Usage tab is selected by default. Reports Daily Average Usage : Displays the daily average from a series of four usage snapshots within 24 hours of data stored in all the buckets. Date range selection : Select a current month, last six months, or custom time range to view usage trends. This month  is a default selection that displays the daily average usage trend for the current month to date. Selecting the  Last 6 months  shows the usage trend of the previous six months. Each data point displays the monthly average for that month. Selecting a  Custom time range  allows you to choose the monthly time range, and the data points display the monthly average usage. Use the Date range selection to choose the length of time of the report. Download the usage data in CSV format by selecting Download . This report shows the Date, Region Name, Bucket Name, Usage(byte), and Usage (GB) in the excel sheet. Settings : Settings tab displays the details of the Sub-account. Organization Information : Displays the organization name and address of the Sub-account. Admin Details : Displays the administrator details of the Sub-account. Danger Zone : This section allows you to disable and delete the Sub-account. You must be very careful before you perform any of these actions. For more information, see Disabling\/Enabling a Sub-account and Deleting Sub-account. " }, 
{ "title" : "Understanding Sub-accounts billing ", 
"url" : "understanding-sub-accounts-billing.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Understanding Sub-accounts billing ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Master account billing ", 
"url" : "understanding-sub-accounts-billing.html#master-account-billing", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Understanding Sub-accounts billing \/ Master account billing ", 
"snippet" : "The master account administrator can view the usage total, the number of buckets, and the storage trends of the buckets in that account. The Home page displays the details of the master account that is combined with Sub-accounts. It shows the number of buckets, total usage from the beginning of the ...", 
"body" : "The master account administrator can view the usage total, the number of buckets, and the storage trends of the buckets in that account. The Home page displays the details of the master account that is combined with Sub-accounts. It shows the number of buckets, total usage from the beginning of the month to date, and a graphical representation of unlimited usage. The Sub-account usage graph displays the usage of each Sub-account on the same chart. The graph shows different colour lines for separate Sub-accounts. " }, 
{ "title" : "Invoicing master account ", 
"url" : "understanding-sub-accounts-billing.html#invoicing-master-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Understanding Sub-accounts billing \/ Invoicing master account ", 
"snippet" : "The master account is billed for all the Sub-accounts created under the master account. The administrator can view all the monthly invoices. You can download the Invoices for each month by selecting Download . The image shows a sample usage for a US-based customer. The Product Number\/Product Descrip...", 
"body" : "The master account is billed for all the Sub-accounts created under the master account. The administrator can view all the monthly invoices. You can download the Invoices for each month by selecting Download . The image shows a sample usage for a US-based customer. The Product Number\/Product Description displays the following information: Service Period Actual Usage For more information, see Understanding billing. The Unit Price (9.00) shown in the invoice is just a placeholder. " }, 
{ "title" : "Billing for disabled Sub-account ", 
"url" : "understanding-sub-accounts-billing.html#billing-for-disabled-sub-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Understanding Sub-accounts \/ Understanding Sub-accounts billing \/ Invoicing master account \/ Billing for disabled Sub-account ", 
"snippet" : "The master account is not charged for the usage of the disabled Sub-account. If the account is disabled during the period of usage calculation, then the usage is calculated till the date the account is disabled. For example, If the Sub-account is disabled on June 15, and the usage is calculated for ...", 
"body" : "The master account is not charged for the usage of the disabled Sub-account. If the account is disabled during the period of usage calculation, then the usage is calculated till the date the account is disabled. For example, If the Sub-account is disabled on June 15, and the usage is calculated for June, the Sub-account is billed from June 1 to June 15. If you re-enable the Sub-account, the account is billed on a pro-rata basis. For example, If the account is enabled on August 10, then the account is billed on a pro-rata basis from August 10 till the end of August. " }, 
{ "title" : "Managing support tickets ", 
"url" : "managing-support-tickets.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets ", 
"snippet" : "If you experience a problem with Lyve Cloud, use the Support page to create a support ticket. Please provide detailed information in the Subject and Description fields, and attach any relevant references for the support team. Detailed information helps us provide a more efficient and effective resol...", 
"body" : "If you experience a problem with Lyve Cloud, use the Support page to create a support ticket. Please provide detailed information in the Subject and Description fields, and attach any relevant references for the support team. Detailed information helps us provide a more efficient and effective resolution, as the ticket response time is based on its severity level which is determined from the details provided. Each support ticket is assigned a unique number. Use this ticket number to track the progress of the reported issue, and update the support ticket by adding a comment. Comments and resolutions are recorded in each ticket. You can also send an email to support.lyvecloud@seagate.com to report an issue. A support ticket is opened based on the issue reported in the email. The support team reviews ticket details and updates the ticket status. New: This status is assigned immediately when a ticket is created, and work is not yet started. In Progress: This status indicates that the ticket is under review, and a support engineer is investigating the issue. After the ticket is updated, you will receive an email notification containing the ticket number, subject, and changes made. You will receive an email notification when a new ticket is opened, a ticket is updated, or an issue is resolved and the ticket is closed. The Support page lists the number of New and In Progress tickets. Customers of a partner must report Lyve Cloud related issues to its partner.  If you purchase Lyve Cloud through a reseller or partner, you will not have direct access to support. Please contact your reseller with all support queries. Video: How to contact Lyve Cloud support " }, 
{ "title" : "Role-based access for the support page ", 
"url" : "managing-support-tickets.html#role-based-access-for-the-support-page", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Role-based access for the support page ", 
"snippet" : "The following table describes access to the Support page features based on the admin role. Actions Admin Storage Admin Auditor (Read only) Create ticket ✓ ✓ × Edit ticket ✓ ✓ × View ticket ✓ ✓ ✓ Add Comments ✓ ✓ × View Comments ✓ ✓ ✓...", 
"body" : "The following table describes access to the Support page features based on the admin role. Actions Admin Storage Admin Auditor (Read only) Create ticket ✓ ✓ × Edit ticket ✓ ✓ × View ticket ✓ ✓ ✓ Add Comments ✓ ✓ × View Comments ✓ ✓ ✓ " }, 
{ "title" : "Creating a ticket ", 
"url" : "creating-a-support-ticket.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Creating a ticket ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "creating-a-support-ticket.html#-41532", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Creating a ticket \/ ", 
"snippet" : "To create a ticket: On the left-hand menu, select Support . On the Support page, select Create New Ticket. In the Create New Ticket dialog, enter the following: Subject: Enter a subject for the support ticket. This is a mandatory field. Description: Enter the ticket details. This is an optional fiel...", 
"body" : "To create a ticket: On the left-hand menu, select Support . On the Support page, select Create New Ticket. In the Create New Ticket dialog, enter the following: Subject: Enter a subject for the support ticket. This is a mandatory field. Description: Enter the ticket details. This is an optional field that allows you to describe the problem summary. Attachments: Add documents that provide more details about the issue. The file size must not exceed 4 MB. Select Upload and choose the file from the desired location to upload an attachment, then select Open . After the file is uploaded, it is listed under Attachments. To remove the attachment, select the x to the right of the file name. Select Create . The new ticket displays in the ticket listing table. Once a ticket is saved, you cannot delete the attachments. " }, 
{ "title" : "Editing a ticket ", 
"url" : "creating-a-support-ticket.html#editing-a-ticket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Creating a ticket \/ Editing a ticket ", 
"snippet" : "You can edit New and In Progress tickets. Editing a ticket allows you to edit or add to the problem summary, description, customer name, and attachments. To edit a ticket: On the left-hand menu, select Support . In the ticket listing table, select a Ticket number to edit that ticket. On the Details ...", 
"body" : "You can edit New and In Progress tickets. Editing a ticket allows you to edit or add to the problem summary, description, customer name, and attachments. To edit a ticket: On the left-hand menu, select Support . In the ticket listing table, select a Ticket number to edit that ticket. On the Details pane, select Edit . Edit any of the following fields: Subject Description Attachments : You can add new attachments, but you cannot delete attachments that were previously added. Add New Comment To add comments, select Add New Comment . Enter a comment and select Add . Select Save . " }, 
{ "title" : "Viewing ticket details ", 
"url" : "creating-a-support-ticket.html#viewing-ticket-details", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Creating a ticket \/ Viewing ticket details ", 
"snippet" : "To view a ticket: On the left-hand menu, select Support . On the Support page, select the ticket number to view its details....", 
"body" : "To view a ticket: On the left-hand menu, select Support . On the Support page, select the ticket number to view its details. " }, 
{ "title" : "Availability and Service Status ", 
"url" : "availability-and-service-status.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Availability and Service Status ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Availability ", 
"url" : "availability-and-service-status.html#availability", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Availability and Service Status \/ Availability ", 
"snippet" : "The Lyve Cloud availability in the following image shows the calculated service availability for the month. Lyve Cloud service availability is calculated by subtracting the error rate from 100% within a five-minute interval. If a customer does not make any requests in a given 5-minute interval, that...", 
"body" : "The Lyve Cloud availability in the following image shows the calculated service availability for the month. Lyve Cloud service availability is calculated by subtracting the error rate from 100% within a five-minute interval. If a customer does not make any requests in a given 5-minute interval, that interval is assumed to have an error rate of 0%. The error rate is the total number of errors returned, divided by the total number of requests during that 5-minute interval. Error rate = number of errors ÷ number of request Availability = 100% - error rate " }, 
{ "title" : "Service Status ", 
"url" : "availability-and-service-status.html#service-status", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Administrator's Guide \/ Managing support tickets \/ Availability and Service Status \/ Service Status ", 
"snippet" : "The status of the Lyve Cloud service is available on the Support page of the Lyve Cloud console. Selecting the service status will show information about any recent service incidents, scheduled maintenance, and the current status of Lyve Cloud.  It displays the Lyve Cloud's storage services by the d...", 
"body" : "The status of the Lyve Cloud service is available on the Support page of the Lyve Cloud console. Selecting the service status will show information about any recent service incidents, scheduled maintenance, and the current status of Lyve Cloud.  It displays the Lyve Cloud's storage services by the data center. The following image displays the service status page " }, 
{ "title" : "API Compatibility Guide ", 
"url" : "api-compatibility-guide.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide ", 
"snippet" : "This guide provides details of the supported, and unsupported Lyve Cloud API calls relative to Amazon Web Services’ Simple Storage Service (AWS S3). This guide is intended for S3 aware application developers and architects responsible for storing data....", 
"body" : "This guide provides details of the supported, and unsupported Lyve Cloud API calls relative to Amazon Web Services’ Simple Storage Service (AWS S3). This guide is intended for S3 aware application developers and architects responsible for storing data. " }, 
{ "title" : "Public bucket access ", 
"url" : "api-compatibility-guide.html#public-bucket-access", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Public bucket access ", 
"snippet" : "Lyve Cloud does not support: Any anonymous access to buckets, as all API calls to Lyve Cloud must be authenticated. Generating a pre-signed URL with a custom domain name to access s3 buckets. Cross-Origin Resource Sharing (CORS) for websites, nor does it support hosting static websites using custom ...", 
"body" : "Lyve Cloud does not support: Any anonymous access to buckets, as all API calls to Lyve Cloud must be authenticated. Generating a pre-signed URL with a custom domain name to access s3 buckets. Cross-Origin Resource Sharing (CORS) for websites, nor does it support hosting static websites using custom domains. Creating a prefix with an empty name. Creating an object with the same prefix name at the same level. Unlisting of prefixes when using the ls command, even if it contains only delete markers when versioning is enabled. " }, 
{ "title" : "Signature Versions ", 
"url" : "api-compatibility-guide.html#signature-versions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Public bucket access \/ Signature Versions ", 
"snippet" : "Lyve Cloud supports Signature Version 4 (SigV4)....", 
"body" : "Lyve Cloud supports Signature Version 4 (SigV4). " }, 
{ "title" : "Supported S3 API calls ", 
"url" : "supported-s3-api-calls.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Supported S3 API calls ", 
"snippet" : "Lyve Cloud supports the most commonly used standard S3 API calls as listed in the following table. Some API calls are rejected if the service account does not have appropriate permissions. Lyve Cloud supports Signature Version 4 (SigV4). The following list specifies API calls allowed for each Lyve C...", 
"body" : "Lyve Cloud supports the most commonly used standard S3 API calls as listed in the following table. Some API calls are rejected if the service account does not have appropriate permissions. Lyve Cloud supports Signature Version 4 (SigV4). The following list specifies API calls allowed for each Lyve Cloud permission type. Permissions General API All Operations Read Only Write Only Specific buckets All buckets Buckets CreateBucket × ✓ × × DeleteBucket ✓ ✓ × × GetBucketLocation ✓ ✓ ✓ ✓ GetBucketReplication ✓ ✓ ✓ × GetBucketVersioning ✓ ✓ ✓ × HeadBucket ✓ ✓ ✓ × ListBucket ✓ ✓ ✓ × GetObjectLockConfiguration ✓ ✓ ✓ × PutObjectLockConfiguration ✓ ✓ × × Object CopyObject ✓ ✓ × ✓ DeleteObject ✓ ✓ × × DeleteObjects ✓ ✓ × × GetObject ✓ ✓ ✓ × GetObjectRetention ✓ ✓ ✓ × HeadObject ✓ ✓ ✓ × ListObjects ✓ ✓ ✓ × ListObjectsV2 ✓ ✓ ✓ × ListObjectVersions ✓ ✓ ✓ × PutObject ✓ ✓ × ✓ PutObjectRetention ✓ ✓ × × SelectObjectContent ✓ ✓ ✓ × Upload AbortMultipartUpload ✓ ✓ × ✓ CreateMultipartUpload ✓ ✓ × ✓ CompleteMultipartUpload ✓ ✓ × ✓ ListMultipartUploads ✓ ✓ × ✓ ListParts ✓ ✓ × ✓ UploadPart ✓ ✓ × ✓ UploadPartCopy Y ✓ × ✓ " }, 
{ "title" : "Unsupported S3 API calls ", 
"url" : "unsupported-s3-api-calls.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Unsupported S3 API calls ", 
"snippet" : "The following S3 calls are not supported in Lyve Cloud. ACL – GetBucketAcl, GetObjectAcl, PutBucketAcl, and PutObjectAcl Analytics – DeleteBucketAnalyticsConfiguration, GetBucketAnalyticsConfiguration, ListBucketAnalyticsConfigurations, and PutBucketAnalyticsConfiguration Bucket Inventory – DeleteBu...", 
"body" : "The following S3 calls are not supported in Lyve Cloud. ACL – GetBucketAcl, GetObjectAcl, PutBucketAcl, and PutObjectAcl Analytics – DeleteBucketAnalyticsConfiguration, GetBucketAnalyticsConfiguration, ListBucketAnalyticsConfigurations, and PutBucketAnalyticsConfiguration Bucket Inventory – DeleteBucketInventoryConfiguration, GetBucketInventoryConfiguration, ListBucketInventoryConfigurations, PutBucketInventoryConfiguration Bucket Ownership – DeleteBucketOwnershipControls, GetBucketOwnershipControls, PutBucketOwnershipControls Bucket Website – DeleteBucketWebsite, GetBucketWebsite, PutBucketWebsite Bucket Notification – GetBucketNotification, GetBucketNotificationConfiguration, PutBucketNotification, PutBucketNotificationConfiguration Bucket Encryption – DeleteBucketEncryption, GetBucketEncryption, PutBucketEncryption Bucket Lifecycle – DeleteBucketLifecycle, GetBucketLifecycle, GetBucketLifecycleConfiguration, PutBucketLifecycle, PutBucketLifecycleConfiguration Bucket Policy – DeleteBucketPolicy, GetBucketPolicy, GetBucketPolicyStatus, PutBucketPolicy CORS - DeleteBucketCors, GetBucketCors, PutBucketCors Logging - GetBucketLogging, PutBucketLogging Metrics - DeleteBucketMetricsConfiguration, GetBucketMetricsConfiguration, ListBucketMetricsConfigurations, PutBucketMetricsConfiguration Object Locking - GetObjectLegalHold, PutObjectLegalHold Public Access - DeletePublicAccessBlock, GetPublicAccessBlock, PutPublicAccessBlock Replication - DeleteBucketReplication, PutBucketReplication, GetBucketReplication Tagging – DeleteBucketTagging, DeleteObjectTagging, GetBucketTagging, GetObjectTagging, PutBucketTagging, PutObjectTagging Tiering - DeleteBucketIntelligentTieringConfiguration, GetBucketIntelligentTieringConfiguration, ListBucketIntelligentTieringConfigurations, PutBucketIntelligentTieringConfiguration, RestoreObject Transfer acceleration - GetBucketAccelerateConfiguration, PutBucketAccelerateConfiguration Torrent - GetObjectTorrent Payment – PutBucketRequestPayment, GetBucketRequestPayment Versioning – PutBucketVersioning " }, 
{ "title" : "Limitations of S3 API calls ", 
"url" : "limitations-of-s3-api-calls.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Limitations of S3 API calls ", 
"snippet" : "Bucket and object limitations The table lists the limitations of buckets and objects and their associated actions. Item Specification Maximum number of buckets 100 buckets per account. To create additional buckets (up to 1000 buckets), raise a Support Ticket. For more information, see Creating a sup...", 
"body" : "Bucket and object limitations The table lists the limitations of buckets and objects and their associated actions. Item Specification Maximum number of buckets 100 buckets per account. To create additional buckets (up to 1000 buckets), raise a Support Ticket. For more information, see Creating a support ticket . Maximum number of objects per bucket no-limit Maximum object size 5 TiB Minimum object size 0 B Maximum object size per PUT operation 5 TiB Maximum length for bucket names 63 Maximum length for object names 255 " }, 
{ "title" : "Multipart upload limitation ", 
"url" : "limitations-of-s3-api-calls.html#multipart-upload-limitation", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Limitations of S3 API calls \/ Multipart upload limitation ", 
"snippet" : "The table lists the multipart upload limitations. Item Specification Maximum number of parts per upload 10,000 Part size range 5MiB to 5GiB. The last part can be 0 B to 5 GiB Maximum number of parts returned per list parts request 10,000 Maximum number of objects returned per list objects request 10...", 
"body" : "The table lists the multipart upload limitations. Item Specification Maximum number of parts per upload 10,000 Part size range 5MiB to 5GiB. The last part can be 0 B to 5 GiB Maximum number of parts returned per list parts request 10,000 Maximum number of objects returned per list objects request 1000 Maximum number of multipart uploads returned per list multipart uploads request 1,000 " }, 
{ "title" : "Incompatible S3 API calls ", 
"url" : "limitations-of-s3-api-calls.html#incompatible-s3-api-calls", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Limitations of S3 API calls \/ Incompatible S3 API calls ", 
"snippet" : "The table lists S3 API calls that are incompatible. S3 API Description ListMultipartUploads This command does not list active multipart uploads using a bucket name. Add a prefix and the prefix value must be an object name to list the object multipart in the bucket. GetBucketLocation This command ret...", 
"body" : "The table lists S3 API calls that are incompatible. S3 API Description ListMultipartUploads This command does not list active multipart uploads using a bucket name. Add a prefix and the prefix value must be an object name to list the object multipart in the bucket. GetBucketLocation This command returns the bucket location, regardless of a Signature v4 region different from the S3 endpoint region. ListBuckets This command lists all the buckets, regardless of a  Signature v4 region different from the S3 endpoint region. This command does not list the bucket Owner DisplayName. ListObjectsV2 It returns owner information even if you do not pass the fetch owner parameter. It returns Key Count as 0 instead of 1 when the prefix does not have an object. Using two consecutive slashes (\/\/) in a prefix path results in an incorrect sequence of objects in a bucket. Returns Key Count as 0 instead of 1 when the prefix does not have an object. PutObjectRetention It returns an HTTP 204 message on successful execution instead of HTTP 200. HeadObject Returns incorrect HTTP code if an object does not exist. CompleteMultipartUpload The Entity tag (ETag) value of an object received in response does not match with AWS S3. Putobject Uploading an object where the object key contains a new line character (0x0A) returns 503 as an incorrect error code. Returns a different error message, whereas AWS returns a 400 error code when you specify the following parameters: -content-length : Greater than the actual content length, Lyve Cloud does not return any error code. --sse-customer-key : Less than or greater than 32 bytes, LyveCloud returns a 403 error code. -object-lock-legal-hold-status : A random string is passed, Lyve Cloud allows uploading objects despite incorrect  --object-lock-legal-hold-status . --tagging : For multiple tagset with invalid separators, Lyve Cloud allows multiple tagset with invalid separators, but it splits the value from first = . --metadata : LyveCloud returns capitalized keys when you pass lowercase letters. UploadPartCopy Using \"\/\" (slash) as a prefix to the source bucket returns a 403 error. For example, to copy the object logs\/january.txt from bucket bucket1, use the header x-amz-copy-source path: bucket1\/logs\/january.txt instead of the x-amz-copy-source path: \/bucket1\/logs\/january.txt . GetObject This command returns a content range of the first part number irrespective of the part number specified of the object. In Lyve Cloud, the custom metadata is stored in a different sequence than AWS S3. An object name same as the prefix is not supported. For example, Object with the name \/A\/B, where A is a prefix and B is the object name and another object with the name: \/A is not supported. " }, 
{ "title" : "Overview of S3 Select API ", 
"url" : "overview-of-s3-select-api.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API ", 
"snippet" : "Welcome to the Lyve Cloud S3 Select API reference documentation. This section provides the S3 API commands that are required to successfully apply to a bucket and perform object-level actions in Lyve Cloud.   Data stored in the cloud can become very large and difficult to manage. The S3 Select API w...", 
"body" : "Welcome to the Lyve Cloud S3 Select API reference documentation. This section provides the S3 API commands that are required to successfully apply to a bucket and perform object-level actions in Lyve Cloud.   Data stored in the cloud can become very large and difficult to manage. The S3 Select API with Lyve Cloud can be utilized in order to pull out the necessary elements that users need. This will fundamentally improve data management and retrieval for improved functionality and latency.  S3 Select can run simple SQL expressions. For example, you can query S3 object data (through the Lyve Cloud Console or Analytics package) and retrieve a subset of S3 object data instead of retrieving the entire S3 object. Data can be as large as a terabyte (TB) and is available in CSV, JSON and Apache Parquet formats. You can run SQL clauses, such as SELECT and WHERE to fetch data from objects stored in the mentioned formats. The feature also supports objects that are compressed with GZIP or BZIP2 (only for CSV and JSON objects) and server-side encrypted objects.    Query pushdown with S3 Select is supported with Spark, Hive, and Presto. Pushdown optimizes mapping performance because the source database can process transformation logic faster than the Data Integration Service. Use this feature to push down the computational work of filtering large data sets for processing from the Spark\/Presto cluster to Lyve Cloud S3, which improves performance and reduces the amount of data transferred between Analytical applications and Lyve Cloud S3. " }, 
{ "title" : "Filtering and retrieving data with Lyve Cloud S3 Select ", 
"url" : "filtering-and-retrieving-data-with-lyve-cloud-s3-select.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Filtering and retrieving data with Lyve Cloud S3 Select ", 
"snippet" : "With Lyve Cloud S3 Select, you can use simple structured query language (SQL) statements to filter the contents of a Lyve Cloud S3 object and retrieve just the subset of data that you need. With this feature, you can reduce the amount of data that is transferred, resulting in lower cost and improved...", 
"body" : "With Lyve Cloud S3 Select, you can use simple structured query language (SQL) statements to filter the contents of a Lyve Cloud S3 object and retrieve just the subset of data that you need. With this feature, you can reduce the amount of data that is transferred, resulting in lower cost and improved latency when you retrieve this data. Lyve Cloud S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only) and server-side encrypted objects. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited. Pass the SQL expressions to Lyve Cloud S3 in the request. Lyve Cloud S3 Select supports a subset of SQL. For more information about supported SQL elements from Lyve Cloud S3 Select, see  SQL reference for Lyve Cloud. " }, 
{ "title" : "Requirements and limitations ", 
"url" : "filtering-and-retrieving-data-with-lyve-cloud-s3-select.html#requirements-and-limitations", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Filtering and retrieving data with Lyve Cloud S3 Select \/ Requirements and limitations ", 
"snippet" : "The following are the requirements for using Lyve Cloud S3 Select: You must have the s3:GetObject permission to use S3 Select command. Use HTTPS and include the customer-provided encryption key (SSE-C) if the object you query is encrypted. The following limits apply when using Lyve Cloud S3 Select: ...", 
"body" : "The following are the requirements for using Lyve Cloud S3 Select: You must have the s3:GetObject permission to use S3 Select command. Use HTTPS and include the customer-provided encryption key (SSE-C) if the object you query is encrypted. The following limits apply when using Lyve Cloud S3 Select: Lyve Cloud S3 Select can only emit nested data using the JSON output format. The maximum length of a SQL expression is 256 KB. The maximum length of a record in the input or result is 1 MB. Additional limitations apply when using Lyve Cloud S3 Select with Parquet objects: Lyve Cloud S3 Select does not support Parquet output. You must specify the output format as CSV or JSON. The maximum uncompressed row group size is 256 MB. Lyve Cloud S3 Select supports only columnar compression using GZIP or Snappy. Lyve Cloud S3 Select does not support whole-object compression for Parquet objects. You must use the data types specified in the object's schema. Selecting on a repeated field returns only the last value. " }, 
{ "title" : "Prerequisites ", 
"url" : "filtering-and-retrieving-data-with-lyve-cloud-s3-select.html#prerequisites", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Filtering and retrieving data with Lyve Cloud S3 Select \/ Prerequisites ", 
"snippet" : "Install AWS CLI  for Windows, MacOS, or Linux to execute all the queries using AWS CLI. Configure AWS CLI v2 with Lyve Cloud. See  Using AWS CLI. Create a bucket and assign the required bucket permissions using either the Lyve Cloud console or the API. Configure the Access key and Secret Key....", 
"body" : "Install AWS CLI  for Windows, MacOS, or Linux to execute all the queries using AWS CLI. Configure AWS CLI v2 with Lyve Cloud. See  Using AWS CLI. Create a bucket and assign the required bucket permissions using either the Lyve Cloud console or the API. Configure the Access key and Secret Key. " }, 
{ "title" : "Synopsis ", 
"url" : "filtering-and-retrieving-data-with-lyve-cloud-s3-select.html#synopsis", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Filtering and retrieving data with Lyve Cloud S3 Select \/ Synopsis ", 
"snippet" : "select-object-content--bucket <value>--key <value>--expression <value>--expression-type <value>--input-serialization <value>--output-serialization <value><outfile>...", 
"body" : "select-object-content--bucket <value>--key <value>--expression <value>--expression-type <value>--input-serialization <value>--output-serialization <value><outfile> " }, 
{ "title" : "Option ", 
"url" : "filtering-and-retrieving-data-with-lyve-cloud-s3-select.html#option", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Filtering and retrieving data with Lyve Cloud S3 Select \/ Option ", 
"snippet" : "Options Description Bucket (String) The S3 bucket. key (string) The object key. expression (string) The expression that is used to query the object. expression-type (string) The type of the provided expression (for example, SQL).Possible value: SQL. input-serialization (structure) Describes the form...", 
"body" : "Options Description Bucket (String) The S3 bucket. key (string) The object key. expression (string) The expression that is used to query the object. expression-type (string) The type of the provided expression (for example, SQL).Possible value: SQL. input-serialization (structure) Describes the format of the data in the object that is being queried. CSV: (structure): Describes the serialization of a CSV-encoded object. FileHeaderInfo: (string): Describes the first line of input. Valid values are: NONE: The first line is not a header. IGNORE: The first line is a header, but you can’t use the header values to indicate the column in an expression. You can use column position (such as _1, _2, …) to indicate the column (SELECT s._1 FROM OBJECT s ). Use: The first line is a header, and you can use the header value to identify a column in an expression (SELECT \"name\" FROM OBJECT). Comments: (string): A single character is used to indicate that a row should be ignored when the character is present at the start of that row. You can specify any character to indicate a comment line. QuoteEscapeCharacter: (string): A single character used for escaping the quotation mark character inside an already escaped value. For example, the value “”” a, b “”” is parsed as ” a, b “. RecordDelimiter: (string): A single character used to separate individual records in the input. Instead of the default value, you can specify an arbitrary delimiter. FieldDelimiter: (string): A single character used to separate individual fields in a record. You can specify an arbitrary delimiter. QuoteCharacter: (string): A single character is used for escaping when the field delimiter is part of the value. For example, if the value is a, b, Lyve Cloud S3 wraps this field value in quotation marks, as follows: \" a , b \" . Type: String Default: \" Ancestors: CSV AllowQuotedRecordDelimiter: (boolean): Specifies that CSV field values may contain quoted record delimiters and such records should be allowed. Default value is FALSE. Setting this value to TRUE may lower performance. CompressionType: (string): Specifies object’s compression format. Valid values: NONE, GZIP, BZIP2. Default Value: NONE. JSON (structure): Specifies JSON as object’s input serialization format. Type: (string) The type of JSON. Valid values: Document, Lines. Parquet (structure): Specifies Parquet as object’s input serialization format. output-serialization (structure) Describes the format of the data you want Lyve Cloud S3 to return in the response. CSV: (structure): Describes the serialization of CSV-encoded Select results. QuoteFields: (string): Indicates whether to use quotation marks around output fields. ALWAYS: Always use quotation marks for output fields. ASNEEDED: Use quotation marks for output fields when needed. QuoteEscapeCharacter (string): The single character used for escaping the quote character inside an already escaped value. RecordDelimiter: (string): A single character used to separate individual records in the output. Instead of the default value, you can specify an arbitrary delimiter. FieldDelimiter: (string): The value used to separate individual fields in a record. You can specify an arbitrary delimiter. QuoteCharacter: (string): A single character is used for escaping when the field delimiter is part of the value. For example, if the value is a, b, Lyve Cloud S3 wraps this field value in quotation marks, as follows: \" a, b \". JSON (structure): Specifies JSON as the request’s output serialization format. RecordDelimiter (string): The value used to separate individual records in the output. If no value is specified, Lyve Cloud S3 uses a newline character (‘n’). outfile (string) Filename where the records will be saved. " }, 
{ "title" : "Examples of using Lyve Cloud S3 Select on objects ", 
"url" : "examples-of-using-lyve-cloud-s3-select-on-objects.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Examples of using Lyve Cloud S3 Select on objects ", 
"snippet" : "Run the commands using AWS CLI....", 
"body" : "Run the commands using AWS CLI. " }, 
{ "title" : "Read data from CSV ", 
"url" : "examples-of-using-lyve-cloud-s3-select-on-objects.html#read-data-from-csv", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Examples of using Lyve Cloud S3 Select on objects \/ Read data from CSV ", 
"snippet" : "Example 1 : The following example reads the data from CSV. The following table consists of sample data: Name Team Position Height (Inches) Weight (lbs) Age AdamDonachie BAL Catcher 74 180 22.99 PaulBako BAL Catcher 74 215 34.69 RamonHernandez BAL Catcher 72 210 30.78 KevinMillar BAL FisherBaseman 72...", 
"body" : "Example 1 : The following example reads the data from CSV. The following table consists of sample data: Name Team Position Height (Inches) Weight (lbs) Age AdamDonachie BAL Catcher 74 180 22.99 PaulBako BAL Catcher 74 215 34.69 RamonHernandez BAL Catcher 72 210 30.78 KevinMillar BAL FisherBaseman 72 210 35.43 ChrisGomez BAL FisherBaseman 73 188 35.71 BrianRoberts BAL SecondBaseman 69 176 29.39 MiguelTejada BAL Shortstop 69 209 30.77 MelvinMora BAL ThirdBaseman 71 200 35.07 Copy CSV to Lyve Cloud S3 using the following command:  aws s3 cp input.csv s3a:\/\/lyve-cloud-bucket\/path_to_s3_object\/input.csv Query the CSV using s3api  with SQL dialect. Query : select * from s3object limit 10 Command : aws s3api\n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com \\\nselect-object-content \\\n--bucket lyve-cloud-bucket \\\n--key path_to_s3_object\/ input.csv \\\n--expression \"select * from s3object \" \\\n--expression-type 'SQL' \\\n--input-serialization '{\"CSV\": {}, \"CompressionType\": \"NONE\"}' \\\n--output-serialization '{\"CSV\": {}}' \"output.csv\" Result : Run the cat output.csv command to see the output. Add filter in select query. Query : select * from s3object where Position = 'Catcher' limit 10 Command : aws s3api \n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com \\\nselect-object-content \\\n--bucket lyve-cloud-bucket \\\n--key path_to_s3_object\/ input.csv \\\n--expression \" select * from s3object  where Position ='Catcher' limit 10\" \\\n--expression-type 'SQL' \\\n--input-serialization '{ \"CSV\": {\"FileHeaderInfo\": \"Use\" }, \"CompressionType\": \"NONE\"}' \\\n--output-serialization '{\"CSV\": {}}' \"output.csv\" Result : Run the cat output.csv command to see the output. Select a specific column from the CSV file. Query : select Name, Team, Position from s3object limit 10 Command : aws s3api \n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com \\\nselect-object-content \\\n--bucket lyve-cloud-bucket \\\n--key path_to_s3_object\/ input.csv \\\n--expression \"select Name, Team, Position from s3object  where Position = 'Catcher' limit 10\" \\\n--expression-type 'SQL' \\\n--input-serialization '{ \"CSV\": {\"FileHeaderInfo\": \"Use\" }, \"CompressionType\": \"NONE\"}' \\\n--output-serialization '{\"CSV\": {}}' \"output.csv\" Result : Run the cat output.csv command to see the output. " }, 
{ "title" : "Reads data from JSON ", 
"url" : "examples-of-using-lyve-cloud-s3-select-on-objects.html#reads-data-from-json", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Examples of using Lyve Cloud S3 Select on objects \/ Reads data from JSON ", 
"snippet" : "Example 2 : The following example reads the data from JSON. The following image is sample data: Run the following command to copy JSON to Lyve Cloud S3. aws s3 cp input.json s3a:\/\/lyve-cloud-bucket\/path_to_s3_object\/input.json Query JSON using s3api with sql dialect. Query : select * from s3object l...", 
"body" : "Example 2 : The following example reads the data from JSON. The following image is sample data: Run the following command to copy JSON to Lyve Cloud S3. aws s3 cp input.json s3a:\/\/lyve-cloud-bucket\/path_to_s3_object\/input.json Query JSON using s3api with sql dialect. Query : select * from s3object limit 10 Command : aws s3api\n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com \nselect-object-content\n--bucket lyve-cloud-bucket\n--key path_to_s3_object\/input.json \n--expression \" select * from s3object limit 10\" \n--expression-type 'SQL'\n--input-serialization '{\"JSON\": {\"Type\": \"LINES\"}, \"CompressionType\": \"NONE\"}'\n--output-serialization '{\"JSON\": {}}' \"output.json\" Result : Add filter in select query. Query : select * from s3object where Position = 'Catcher' limit 10 Command : aws s3api\n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com \nselect-object-content\n--bucket lyve-cloud-bucket\n--key path_to_s3_object\/input.json \n--expression \" select * from s3object  where Position = 'Catcher' limit 10\" \n--expression-type 'SQL'\n--input-serialization '{\"JSON\": {\"Type\": \"LINES\"}, \"CompressionType\": \"NONE\"}'\n--output-serialization '{\"JSON\": {}}' \"output.json\" Result : Run the cat output.json command to see the output. Select a specific column from the JSON file. Query : select Name, Team, Position from s3object limit 10 Command : aws s3api\n--endpoint=https:\/\/s3.us-east-1.Lyve Cloud.seagate.com  \nselect-object-content\n--bucket lyve-cloud-bucket\n--key path_to_s3_object\/input.json \n--expression \"select Name, Team, Position from s3object limit 10\" \n--expression-type 'SQL'\n--input-serialization '{\"JSON\": {\"Type\": \"LINES\"}, \"CompressionType\": \"NONE\"}'\n--output-serialization '{\"JSON\": {}}' \"output.json\" Result: Run the cat output.json command to see the output. " }, 
{ "title" : "Read from PARQUET ", 
"url" : "examples-of-using-lyve-cloud-s3-select-on-objects.html#read-from-parquet", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ Examples of using Lyve Cloud S3 Select on objects \/ Read from PARQUET ", 
"snippet" : "Parquet option is not enabled....", 
"body" : "Parquet option is not enabled. " }, 
{ "title" : "SQL reference for Lyve Cloud ", 
"url" : "sql-reference-for-lyve-cloud.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud ", 
"snippet" : "This reference contains a description of the structured query language (SQL) elements that are supported by Lyve Cloud S3 Select....", 
"body" : "This reference contains a description of the structured query language (SQL) elements that are supported by Lyve Cloud S3 Select. " }, 
{ "title" : "SELECT Command ", 
"url" : "sql-reference-for-lyve-cloud.html#select-command", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SELECT Command ", 
"snippet" : "Lyve Cloud S3 supports only the SELECT: SQL command. The following ANSI standard clauses are supported for  SELECT : SELECT list : The SELECT list names the columns, functions, and expressions that you want the query to return. The list represents the output of the query. SELECT * SELECT projection ...", 
"body" : "Lyve Cloud S3 supports only the SELECT: SQL command. The following ANSI standard clauses are supported for  SELECT : SELECT list : The SELECT list names the columns, functions, and expressions that you want the query to return. The list represents the output of the query. SELECT *\nSELECT projection [ AS column_alias | column_alias ] [, ...] The first form with * (asterisk) returns every row that passed the WHERE clause, as-is. The second form creates a row with user-defined output scalar expressions projection for each column. The FROM clause : Lyve Cloud S3 Select supports the following forms of the FROM clause: FROM table_name\nFROM table_name alias\nFROM table_name AS alias Where table_name is referring to the S3Object being queried over. Users using traditional relational databases can use this as a database schema that contains multiple views over a table. Following standard SQL, the FROM clause creates rows that are filtered in the WHERE clause and projected in the SELECT list. For JSON objects that are stored in Lyve Cloud S3 Select, you can also use the following forms of the FROM clause: FROM S3Object[*].path\nFROM S3Object[*].path alias\nFROM S3Object[*].path AS alias Using this form of the FROM clause, you can select from arrays or objects within a JSON object. You can specify path using one of the following forms: By name (in an object): .name or ['name'] By index (in an array): [index] By wildcard (in an object): .* By wildcard (in an array): [*] This example shows results using the following dataset and queries: { \"created\": \"936864000\", \"dir_name\": \n\"important_docs\", \"files\": [ {\"name\": \".\"}, {\"name\": \"..\"}, {\"name\": \n\".aws\"}, {\"name\": \"downloads\"} ], \"owner\": \"Lyve Cloud S3\" }\n{ \"created\": \"936864000\", \"dir_name\": \"other_docs\", \"files\": [ {\"name\": \n\".\"}, {\"name\": \"..\"}, {\"name\": \"my stuff\"}, {\"name\": \"backup\"} ], \n\"owner\": \"User\" } SELECT d.dir_name, d.files FROM S3Object[*] d {\"dir_name\":\"important_docs\",\"files\":[{\"name\":\".\"},{\"name\":\"..\"},{\"name\":\".Lyve Cloud\"},{\"name\":\"downloads\"}]}\n{\"dir_name\":\"other_docs\",\"files\":[{\"name\":\".\"},{\"name\":\"..\"},{\"name\":\"my stuff\"},{\"name\":\"backup\"}]} {\"dir_name\":\"important_docs\",\"owner\":\"LYVE CLOUD S3\"}\n{\"dir_name\":\"other_docs\",\"owner\":\"User\"} WHERE clause : The WHERE clause filters rows based on the condition. A condition is an expression that has a Boolean result. Only rows for which the condition evaluates to TRUE are returned in the result. WHERE condition LIMIT clause : The LIMIT clause limits the number of records that you want the query to return based on the number. Limit number Attribute Access : The SELECT and WHERE clauses can refer to record data using one of the methods in the following sections, depending on whether the file that is being queried is in CSV or JSON format. CSV Column Numbers: You can refer to the Nth column of a row with the column name _N, where N is the column position. The position count starts at 1. For example, the first column is named _1 and the second column is named _2. You can refer to a column as _N or alias._N. For example, _2 and myAlias._2 are both valid ways to refer to a column in the SELECT list and WHERE clause. Column Headers: For objects in CSV format that have a header row, the headers are available in the SELECT list and WHERE clause. In particular, as in traditional SQL, within SELECT and WHERE clause expressions, you can refer to the columns by alias.column_name or column_name. JSON Document: You can access JSON document fields as alias.name. Nested fields can also be accessed; for example, alias.name1.name2.name3. List: You can access elements in a JSON list using zero-based indexes with the [] operator. For example, you can access the second element of a list as alias[1]. Accessing list elements can be combined with fields as alias.name1.name2[1].name3. Examples: Consider this JSON object as a sample dataset: {\"name\": \"Susan Smith\",\n\"org\": \"engineering\",\n\"projects\":\n    [\n         {\"project_name\":\"project1\", \"completed\":false},\n         {\"project_name\":\"project2\", \"completed\":true}\n    ]\n} The following query returns these results: Select s.name from S3Object s {\"name\":\"Susan Smith\"} The following query returns these results: Select s.projects[0].project_name from S3Object s     {\"project_name\":\"project1\"} Case Sensitivity of Header\/Attribute Names With Lyve Cloud S3 Select, you can use double quotation marks to indicate that column headers (for CSV objects) and attributes (for JSON objects) are case sensitive. Without double quotation marks, object headers\/attributes are case insensitive. An error is displayed in case of ambiguity. The following examples are: Lyve Cloud S3 objects in CSV format with the specified column header(s), and with FileHeaderInfo set to \"Use\" for the query request. or Lyve Cloud S3 object in JSON format with the specified attributes. The object being queried has header\/attribute \"NAME\". The following expression successfully returns values from the object (no quotation marks: case insensitive): SELECT s.name from S3Object s The following expression results in a 400 error MissingHeaderName (quotation marks: case sensitive): SELECT s.\"name\" from S3Object s The Lyve Cloud S3 object being queried has one header\/attribute with \"NAME\" and another header\/attribute with \"name\". The following expression results in a 400 error AmbiguousFieldName (no quotation marks: case insensitive, but there are two matches): SELECT s.name from S3Object s The following expression successfully returns values from the object (quotation marks: case sensitive, so it resolves the ambiguity). SELECT s.\"NAME\" from S3Object s Using Reserved Keywords as User-Defined Terms Lyve Cloud S3 Select has a set of reserved keywords that are required to run the SQL expressions used to query object content. Reserved keywords include function names, data types, operators, and so on. In some cases, user-defined terms like the column headers (for CSV files) or attributes (for JSON objects) may clash with a reserved keyword. When this happens, you must use double quotation marks to indicate that you are intentionally using a user-defined term that clashes with a reserved keyword. Otherwise, a 400 parse error will result. Scalar Expressions Within the WHERE clause and the SELECT list, you can have SQL scalar expressions, which are expressions that return scalar values. They have the following form: Literal: An SQL literal. column_reference: A reference to a column in the form column_name or alias.column_name. unary_op expression: Where unary_op unary is an SQL unary operator. expression binary_op expression: Where binary_op is an SQL binary operator. func_name: Where func_name is the name of a scalar function to invoke. expression [ NOT ] BETWEEN expression AND expression expression LIKE expression [ ESCAPE expression ] " }, 
{ "title" : "Data Types ", 
"url" : "sql-reference-for-lyve-cloud.html#data-types", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ Data Types ", 
"snippet" : "Lyve Cloud S3 Select support several primitive data types. Data Type Conversions The general rule is to follow the CAST function if defined. If CAST is not defined, then all input data is treated as a string. It must be cast into the relevant data types when necessary. Name Description Examples bool...", 
"body" : "Lyve Cloud S3 Select support several primitive data types. Data Type Conversions The general rule is to follow the CAST function if defined. If CAST is not defined, then all input data is treated as a string. It must be cast into the relevant data types when necessary. Name Description Examples bool True or False False int, integer 8-byte signed integer in the range -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. 100000 string UTF8-encoded variable-length string. The default limit is one character. The maximum character limit is 2,147,483,647. xyz float 8-byte floating-point number. CAST(0.456 AS FLOAT) decimal, numeric Base-10 number, with a maximum precision of 38 (that is, the maximum number of significant digits), and with a scale within the range of -231 to 231-1 (that is, the base-10 exponent). Lyve Cloud S3 Select ignores scale and precision when both are provided at the same time. 123.456 timestamp Time stamps represent a specific moment in time, always include a local offset, and are capable of arbitrary precision. In the text format, time stamps follow the W3C note on date and time formats , but they must end with the literal \"T\" if not at least whole-day precision. Fractional seconds are allowed, with at least one digit of precision, and an unlimited maximum. Local-time offsets can be represented as either hour: minute offsets from UTC, or as the literal \"Z\" to denote a local time of UTC. They are required on timestamps with time and are not allowed on date values. CAST('2007-04-05T14:30Z' AS TIMESTAMP) " }, 
{ "title" : "Operators ", 
"url" : "sql-reference-for-lyve-cloud.html#operators", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ Operators ", 
"snippet" : "Lyve Cloud S3 support the following operators. Operator Elements Logical Operators AND, NOT, OR Comparison Operators &lt;, &gt;, &lt;=, &gt;=, =, &lt;&gt;, !=, BETWEEN, IN (For example: IN ('a', 'b', 'c')) Pattern Matching Operators LIKE, _ (Matches any character), % (Matches any sequence of charact...", 
"body" : "Lyve Cloud S3 support the following operators. Operator Elements Logical Operators AND, NOT, OR Comparison Operators &lt;, &gt;, &lt;=, &gt;=, =, &lt;&gt;, !=, BETWEEN, IN (For example: IN ('a', 'b', 'c')) Pattern Matching Operators LIKE, _ (Matches any character), % (Matches any sequence of characters) Unitary Operators IS NULL, IS NOT NULL Math Operators Addition, subtraction, multiplication, division, and modulo are supported. (+, -, *, \/, %, ) Operator Precedence The following table shows the operators' precedence in decreasing order. Operator\/Element Associativity Required - unary minus *, \/, % left multiplication, division, modulo +, - left addition, subtraction IN set membership BETWEEN range containment LIKE string pattern matching <> less than, greater than = right equality, assignment NOT right logical negation AND left logical conjunction OR left logical disjunction " }, 
{ "title" : "Reserved Keyword ", 
"url" : "sql-reference-for-lyve-cloud.html#reserved-keyword", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ Reserved Keyword ", 
"snippet" : "Below is the list of reserved keywords for Lyve Cloud S3 Select. These include function names, data types, operators, etc., that is required to run the SQL expressions used to query object content. absolute, action, add, all, allocate, alter, and, any, are, as, asc, assertion, at, authorization, avg...", 
"body" : "Below is the list of reserved keywords for Lyve Cloud S3 Select. These include function names, data types, operators, etc., that is required to run the SQL expressions used to query object content. absolute, action, add, all, allocate, alter, and, any, are, as, asc, assertion, at, authorization, avg, bag, begin, between, bit, bit_length, blob, bool, boolean, both, by, cascade, cascaded, case, cast, catalog, char, char_length, character, character_length, check, clob, close, coalesce, collate, collation, column, commit, connect, connection, constraint, constraints, continue, convert, corresponding, count, create, cross, current, current_date, current_time, current_timestamp, current_user, cursor, date, day, deallocate, dec, decimal, declare, default, deferrable, deferred, delete, desc, describe, descriptor, diagnostics, disconnect, distinct, domain, double, drop, else, end, end-exec, escape, except, exception, exec, execute, exists, external, extract, false, fetch, first, float, for, foreign, found, from, full, get, global, go, goto, grant, group, having, hour, identity, immediate, in, indicator, initially, inner, input, insensitive, insert, int, integer, intersect, interval, into, is, isolation, join, key, language, last, leading, left, level, like, limit, list, local, lower, match, max, min, minute, missing, module, month, names, national, natural, nchar, next, no, not, null, nullif, numeric, octet_length, of, on, only, open, option, or, order, outer, output, overlaps, pad, partial, pivot, position, precision, prepare, preserve, primary, prior, privileges, procedure, public, read, real, references, relative, restrict, revoke, right, rollback, rows, schema, scroll, second, section, select, session, session_user, set, sexp, size, smallint, some, space, sql, sqlcode, sqlerror, sqlstate, string, struct, substring, sum, symbol, system_user, table, temporary, then, time, timestamp, timezone_hour, timezone_minute, to, trailing, transaction, translate, translation, trim, true, tuple, union, unique, unknown, unpivot, update, upper, usage, user, using, value, values, varchar, varying, view, when, whenever, where, with, work, write, year, zone " }, 
{ "title" : "SQL Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#sql-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions ", 
"snippet" : "Lyve Cloud S3 Select supports several SQL functions....", 
"body" : "Lyve Cloud S3 Select supports several SQL functions. " }, 
{ "title" : "Aggregate Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#aggregate-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions \/ Aggregate Functions ", 
"snippet" : "Lyve Cloud S3 Select supports the following aggregate functions. Function Argument Type Return Type AVG(expression) INT, FLOAT, DECIMAL DECIMAL for an INT argument, FLOAT for a floating-point argument; otherwise the same as the argument data type. COUNT - INT MAX(expression) INT, DECIMAL Same as the...", 
"body" : "Lyve Cloud S3 Select supports the following aggregate functions. Function Argument Type Return Type AVG(expression) INT, FLOAT, DECIMAL DECIMAL for an INT argument, FLOAT for a floating-point argument; otherwise the same as the argument data type. COUNT - INT MAX(expression) INT, DECIMAL Same as the argument type. MIN(expression) INT, DECIMAL Same as the argument type. SUM(expression) INT, FLOAT, DOUBLE, DECIMAL INT for INT argument, FLOAT for a floating-point argument; otherwise, the same as the argument data type. " }, 
{ "title" : "Conditional Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#conditional-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions \/ Conditional Functions ", 
"snippet" : "Lyve Cloud S3 Select supports the following conditional functions. CASE The CASE expression is a conditional expression, similar to if\/then\/else statements found in other languages. CASE is used to specify a result when there are multiple conditions. There are two types of CASE expressions: simple a...", 
"body" : "Lyve Cloud S3 Select supports the following conditional functions. CASE The CASE expression is a conditional expression, similar to if\/then\/else statements found in other languages. CASE is used to specify a result when there are multiple conditions. There are two types of CASE expressions: simple and searched. In simple CASE expressions, an expression is compared with a value. When a match is found, the specified action in the THEN clause is applied. If no match is found, the action in the ELSE clause is applied. In searched CASE expressions, each CASE is evaluated based on a Boolean expression, and the CASE statement returns the first matching CASE. If no matching CASEs are found among the WHEN clauses, the action in the ELSE clause is returned. Syntax Simple CASE statement used to match conditions: CASE expression\nWHEN value THEN result\n[WHEN...]\n[ELSE result]\nEND Searched CASE statement used to evaluate each condition: CASE\nWHEN boolean condition THEN result\n[WHEN ...]\n[ELSE result]\nEND Use a simple CASE expression to replace New York City with Big Apple in a query. Replace all other city names with other. select venuecity,\ncase venuecity\nwhen 'New York City'\nthen 'Big Apple' else 'other'\nend from venue\norder by venueid desc;\nvenuecity        |   case\n-----------------+-----------\nLos Angeles      | other\nNew York City    | Big Apple\nSan Francisco    | other\nBaltimore        | other\n... COALESCE Evaluate the arguments in order and returns the first non-unknown, that is, the first non-null or non-missing. This function does not propagate null and missing. Syntax COALESCE ( expression, expression, ... ) Parameters Expression : The target expression that the function operates on. COALESCE(1)                -- 1\nCOALESCE(null)             -- null\nCOALESCE(null, null)       -- null\nCOALESCE(missing)          -- null\nCOALESCE(missing, missing) -- null\nCOALESCE(1, null)          -- 1\nCOALESCE(null, null, 1)    -- 1\nCOALESCE(null, 'string')   -- 'string'\nCOALESCE(missing, 1)       -- 1 NULLIF Given two expressions, it returns NULL if the two expressions evaluate to the same value; otherwise, it returns the result of evaluating the first expression. Syntax NULLIF ( expression1, expression2 ) Parameters expression1, expression2: The target expressions that the function operates on. NULLIF(1, 1)             -- null\nNULLIF(1, 2)             -- 1\nNULLIF(1.0, 1)           -- null\nNULLIF(1, '1')           -- 1\nNULLIF([1], [1])         -- null\nNULLIF(1, NULL)          -- 1\nNULLIF(NULL, 1)          -- null\nNULLIF(null, null)       -- null\nNULLIF(missing, null)    -- null\nNULLIF(missing, missing) -- null " }, 
{ "title" : "Conversion Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#conversion-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions \/ Conversion Functions ", 
"snippet" : "CAST The CAST function converts an entity, such as an expression that evaluates to a single value, from one type to another. Syntax CAST ( expression AS data_type ) Parameters Expression : A combination of one or more values, operators, and SQL functions that evaluate to a value. data_type : The tar...", 
"body" : "CAST The CAST function converts an entity, such as an expression that evaluates to a single value, from one type to another. Syntax CAST ( expression AS data_type ) Parameters Expression : A combination of one or more values, operators, and SQL functions that evaluate to a value. data_type : The target data type, such as INT, to cast the expression to. For a list of supported data types, see Data Types. CAST('2007-04-05T14:30Z' AS TIMESTAMP)\nCAST(0.456 AS FLOAT) " }, 
{ "title" : "Date Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#date-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions \/ Date Functions ", 
"snippet" : "DATE_ADD Given a date part, a quantity, and a timestamp, returns an updated timestamp by altering the date part by the quantity. Syntax DATE_ADD( date_part, quantity, timestamp ) Parameters The following table describes the parameters used to update the timestamp. Parameter Description date_part Spe...", 
"body" : "DATE_ADD Given a date part, a quantity, and a timestamp, returns an updated timestamp by altering the date part by the quantity. Syntax DATE_ADD( date_part, quantity, timestamp ) Parameters The following table describes the parameters used to update the timestamp. Parameter Description date_part Specifies which part of the date to modify. This can be one of the following: year month day hour miniute second quantity The value to apply to the updated timestamp. Positive values for quantity add to the time stamp's date_part, and negative values subtract. Timestamp The target timestamp that the function operates on. DATE_ADD(year, 5, `2010-01-01T`)                -- 2015-01-01 (equivalent to 2015-01-01T)\nDATE_ADD(month, 1, `2010T`)                     -- 2010-02T (result will add precision as necessary)\nDATE_ADD(month, 13, `2010T`)                    -- 2011-02T\nDATE_ADD(day, -1, `2017-01-10T`)                -- 2017-01-09 (equivalent to 2017-01-09T)\nDATE_ADD(hour, 1, `2017T`)                      -- 2017-01-01T01:00-00:00\nDATE_ADD(hour, 1, `2017-01-02T03:04Z`)          -- 2017-01-02T04:04Z\nDATE_ADD(minute, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:05:05.006Z\nDATE_ADD(second, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:04:06.006Z DATE_DIFF Given a date part and two valid timestamps returns the difference in date parts. The return value is a negative integer when the date_part value of timestamp1 is greater than the date_part value of timestamp2. The return value is a positive integer when the date_part value of timestamp1 is less than the date_part value of timestamp2. Syntax   DATE_DIFF( date_part, timestamp1, timestamp2 )   Parameters The following table describes the parameters used to find the difference in dates. Parameter Description date_part Specifies which part of the timestamps to compare. For the definition of date_part, see DATE_ADD. timestamp1 The first timestamp to compare. timestamp2 The second timestamp to compare. DATE_DIFF(year, `2010-01-01T`, `2011-01-01T`)            -- 1\nDATE_DIFF(year, `2010T`, `2010-05T`)                     -- 4 (2010T is equivalent to 2010-01-01T00:00:00.000Z)\nDATE_DIFF(month, `2010T`, `2011T`)                       -- 12\nDATE_DIFF(month, `2011T`, `2010T`)                       -- -12\nDATE_DIFF(day, `2010-01-01T23:00`, `2010-01-02T01:00`) -- 0 (need to be at least 24h apart to be 1 day apart) EXTRACT Given a date part and a timestamp returns the timestamp's date part value.   Syntax EXTRACT( date_part FROM timestamp )   Parameters The following table describes the parameters used to extract the timestamp. Parameter Description date_part Specifies which part of the date to modify. This can be one of the following: hour year month day hour minute second timezone_hour timezone_minute Timestamp The target timestamp that the function operates on. EXTRACT(YEAR FROM `2010-01-01T`)                           -- 2010\nEXTRACT(MONTH FROM `2010T`)                                -- 1 (equivalent to 2010-01-01T00:00:00.000Z)\nEXTRACT(MONTH FROM `2010-10T`)                             -- 10\nEXTRACT(HOUR FROM `2017-01-02T03:04:05+07:08`)             -- 3\nEXTRACT(MINUTE FROM `2017-01-02T03:04:05+07:08`)           -- 4\nEXTRACT(TIMEZONE_HOUR FROM `2017-01-02T03:04:05+07:08`)    -- 7\nEXTRACT(TIMEZONE_MINUTE FROM `2017-01-02T03:04:05+07:08`)  -- 8 TO_STRING Given a timestamp and a format pattern returns a string representation of the time stamp in the given format.   Syntax TO_STRING ( timestamp time_format_pattern )   Parameters The following table describes the parameters used to represent the timestamp in a given format Parameter Description timestamp The target timestamp that the function operates on. time_format_pattern A string that has the following special character interpretations. TO_STRING(`1969-07-20T20:18Z`,  'MMMM d, y')                    -- \"July 20, 1969\"\nTO_STRING(`1969-07-20T20:18Z`, 'MMM d, yyyy')                   -- \"Jul 20, 1969\"\nTO_STRING(`1969-07-20T20:18Z`, 'M-d-yy')                        -- \"7-20-69\"\nTO_STRING(`1969-07-20T20:18Z`, 'MM-d-y')                        -- \"07-20-1969\"\nTO_STRING(`1969-07-20T20:18Z`, 'MMMM d, y h:m a')               -- \"July 20, 1969 8:18 PM\"\nTO_STRING(`1969-07-20T20:18Z`, 'y-MM-dd''T''H:m:ssX')           -- \"1969-07-20T20:18:00Z\"\nTO_STRING(`1969-07-20T20:18+08:00Z`, 'y-MM-dd''T''H:m:ssX')     -- \"1969-07-20T20:18:00Z\"\nTO_STRING(`1969-07-20T20:18+08:00`, 'y-MM-dd''T''H:m:ssXXXX')   -- \"1969-07-20T20:18:00+0800\"\nTO_STRING(`1969-07-20T20:18+08:00`, 'y-MM-dd''T''H:m:ssXXXXX')  -- \"1969-07-20T20:18:00+08:00\" TO_TIMESTAMP   Given a string, convert it into a timestamp. This is the inverse operation of TO_STRING.   Syntax TO_TIMESTAMP ( string )   Parameters The following table describes the parameters used to convert a string to a timestamp. Parameter Description string The target string that the function operates on. TO_TIMESTAMP('2007T')                         -- `2007T`\nTO_TIMESTAMP('2007-02-23T12:14:33.079-08:00') -- `2007-02-23T12:14:33.079-08:00` UTCNOW It returns the current time in UTC as a time stamp.   Syntax UTCNOW()   Parameters None UTCNOW() -- 2017-10-13T16:02:11.123Z " }, 
{ "title" : "String Functions ", 
"url" : "sql-reference-for-lyve-cloud.html#string-functions", 
"breadcrumbs" : "Lyve Cloud Documentation \/ API Compatibility Guide \/ Overview of S3 Select API \/ SQL reference for Lyve Cloud \/ SQL Functions \/ String Functions ", 
"snippet" : "CHAR_LENGTH, CHARACTER_LENGTH Count the number of characters in the specified string.   Syntax CHAR_LENGTH ( string ) Parameters The following table describes the parameters used to count the characters in a string. Parameter Description string The target string that the function operates on. CHAR_L...", 
"body" : "CHAR_LENGTH, CHARACTER_LENGTH Count the number of characters in the specified string.   Syntax CHAR_LENGTH ( string ) Parameters The following table describes the parameters used to count the characters in a string. Parameter Description string The target string that the function operates on. CHAR_LENGTH('')          -- 0\nCHAR_LENGTH('abcdefg')   -- 7 LOWER Given a string, it converts all uppercase characters to lowercase characters. Any non-uppercased characters remain unchanged.   Syntax LOWER ( string ) Parameters The following table describes the parameters used to convert all upper case characters to lower case. Parameter Description string The target string that the function operates on. LOWER('AbCdEfG!@#$') -- 'abcdefg!@#$' SUBSTRING   Given a string, a start index, and optionally a length, returns the substring from the start index up to the end of the string, or up to the length provided. The first character of the input string has index 1. If the start is <1, it is set to 1. Syntax SUBSTRING( string FROM start [ FOR length ] )   Parameters The following table describes the parameters that return the substring Parameter Description string The target string that the function operates on. start The start position of the string. length The length of the substring to return. If not present, proceed to the end of the string. SUBSTRING(\"123456789\", 0)      -- \"123456789\"\nSUBSTRING(\"123456789\", 1)      -- \"123456789\"\nSUBSTRING(\"123456789\", 2)      -- \"23456789\"\nSUBSTRING(\"123456789\", -4)     -- \"123456789\"\nSUBSTRING(\"123456789\", 0, 999) -- \"123456789\" \nSUBSTRING(\"123456789\", 1, 5)   -- \"12345\" TRIM Trims leading or trailing characters from a string. The default character to remove is ' '.   Syntax TRIM ( [[LEADING | TRAILING | BOTH remove_chars] FROM] string ) Parameters The following table describes the parameters used to trim characters from a string. Parameter Description string The target string that the function operates on. LEADING | TRAILING | BOTH Whether to trim leading or trailing characters or both leading and trailing characters. remove_chars The set of characters to remove. Note that remove_chars can be a string with length >1. This function returns the string with any character from remove_chars found at the beginning or end of the string that was removed. TRIM('       foobar         ')               -- 'foobar'\nTRIM('      \\tfoobar\\t         ')            -- '\\tfoobar\\t'\nTRIM(LEADING FROM '       foobar         ')  -- 'foobar         '\nTRIM(TRAILING FROM '       foobar         ') -- '       foobar'\nTRIM(BOTH FROM '       foobar         ')     -- 'foobar'\nTRIM(BOTH '12' FROM '1112211foobar22211122') -- 'foobar' UPPER Given a string, it converts all lowercase characters to uppercase characters. Any non-lowercased characters remain unchanged. Syntax UPPER ( string ) Parameters The following table describes the parameters used to convert the lower case character to upper case Parameter Description string The target string that the function operates on. UPPER('AbCdEfG!@#$') -- 'ABCDEFG!@#$' " }, 
{ "title" : "Lyve Cloud Compute ", 
"url" : "lyve-cloud-compute.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute ", 
"snippet" : "The Lyve Cloud Compute by Zadara allows you to subscribe to virtual servers and run their applications adjacent to Lyve Cloud S3-compatible storage. Compute provides a web service through which you can create a virtual machine image and boot a virtual machine. It allows you to develop, deploy, virtu...", 
"body" : "The Lyve Cloud Compute by Zadara allows you to subscribe to virtual servers and run their applications adjacent to Lyve Cloud S3-compatible storage. Compute provides a web service through which you can create a virtual machine image and boot a virtual machine. It allows you to develop, deploy, virtualize and terminate instances as needed and pay for active servers. Compute also controls autoscaling groups, virtual private networks, block storage and other related resources. You can manage Compute by UI, CLI and EC2-compatible API. Benefits of using Lyve Cloud Compute Flexible: Infrastructure-as-a-service solution provides a user-friendly interface for all computing operations Predictable pricing: Pay-as-you-go and 100% Opex Pricing Elastic scalability: Autoscaling allocates resources as needed Made for multi-cloud High-quality service and support Integration with Lyve Cloud Storage service Secure High availability " }, 
{ "title" : "Getting started with Compute ", 
"url" : "getting-started-with-compute.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Getting started with Compute ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Onboarding Lyve Cloud Compute by Zadara ", 
"url" : "getting-started-with-compute.html#onboarding-lyve-cloud-compute-by-zadara", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Getting started with Compute \/ Onboarding Lyve Cloud Compute by Zadara ", 
"snippet" : "Initially, the onboarding process creates an evaluation account. After 30 days, you may switch to the production environment as you cannot run the production workloads on the evaluation account. To sign up for Compute: On the left-hand menu, select Compute . On the Compute page, select Sign Up . You...", 
"body" : "Initially, the onboarding process creates an evaluation account. After 30 days, you may switch to the production environment as you cannot run the production workloads on the evaluation account. To sign up for Compute: On the left-hand menu, select Compute . On the Compute page, select Sign Up . You are signed up for the evaluation period of 30 days. Enter the details in the Signup form and select Create Free Account . You must specify the desired Compute capacity location, where you may prefer a region adjacent to your Lyve Cloud storage capacity. Once the registration is successful, an email is sent with login credentials. The email includes a temporary password for the initial login. Select Login , and you are directed to the Compute account login page. Create a new password after your first login. Once the password is updated, you are logged into the Compute tenant. Repeat the steps to create an additional account with another Compute region. You can start using Lyve Cloud Compute once you visit the Home Page. For more information, see zCompute User Guide . " }, 
{ "title" : "Using Lyve Cloud Compute ", 
"url" : "using-lyve-cloud-compute.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Using Lyve Cloud Compute ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Compute console overview ", 
"url" : "using-lyve-cloud-compute.html#compute-console-overview", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Using Lyve Cloud Compute \/ Compute console overview ", 
"snippet" : "The Zadara console enables you to create, configure, run, access and manage Virtual Machines (VMs). Select zCompute to open the Compute Project Overview page. The following menu options of the left navigation pane include: Overview : This page summarizes key resource usage statics of VM instances an...", 
"body" : "The Zadara console enables you to create, configure, run, access and manage Virtual Machines (VMs). Select zCompute to open the Compute Project Overview page. The following menu options of the left navigation pane include: Overview : This page summarizes key resource usage statics of VM instances and has various widgets and graphs. Instances : An instance is a virtual machine that is hosted on Zadara. Snapshots : You can create a snapshot of a specific volume or an instance that includes a boot volume and multiple data volumes. Key Pairs : A key pair consists of a public and private key. These are a set of security credentials used to prove your identity when connecting to a zCompute instance. Auto Scaling Groups : An Auto Scaling group contains a collection of zCompute instances treated as a logical grouping for automatic scaling and management. An Auto Scaling group lets you use zCompute Auto Scaling features such as health check replacements and scaling policies. Launch Config : A launch configuration is an instance configuration template that an Auto Scaling group uses to launch zCompute instances. Rules : Determine the VM instances' placement on physical nodes. You can perform various actions using the Compute console from the menu options. For more information, see zCompute Project Overview . " }, 
{ "title" : "How to use the Compute platform? ", 
"url" : "using-lyve-cloud-compute.html#how-to-use-the-compute-platform-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Using Lyve Cloud Compute \/ Compute console overview \/ How to use the Compute platform? ", 
"snippet" : "This video explains how to use the Compute console....", 
"body" : "[video] This video explains how to use the Compute console. " }, 
{ "title" : "Operating Compute ", 
"url" : "using-lyve-cloud-compute.html#operating-compute", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Using Lyve Cloud Compute \/ Operating Compute ", 
"snippet" : "To operate Compute: Create a Virtual Private Cloud (VPC). For more information, see Create Your Own Virtual Private Cloud . To associate a Subnet with VPC, configure Sub-nets within the VPC. For more information, see Creating Subnet . Create a firewall by creating a security group so that whitelists...", 
"body" : "To operate Compute: Create a Virtual Private Cloud (VPC). For more information, see Create Your Own Virtual Private Cloud . To associate a Subnet with VPC, configure Sub-nets within the VPC. For more information, see Creating Subnet . Create a firewall by creating a security group so that whitelists are applied to the virtual network interfaces to control the inbound and outbound traffic. For more information, see Security Group . Add an internet gateway to the routing table. For more information, see Internet Gateways . First, create a routing table that controls the IP forwarding of all traffic in the subnet. For more information, see the Routing tables . Create a key pair for remote connection. For more information, see Key Pairs . Create a VM instance to launch, and attach an elastic IP address. For more information, see Creating VM Instances . " }, 
{ "title" : "How to create an instance? ", 
"url" : "using-lyve-cloud-compute.html#how-to-create-an-instance--74240", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Compute \/ Using Lyve Cloud Compute \/ How to create an instance? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud Analytics ", 
"url" : "lyve-cloud-analytics.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics ", 
"snippet" : "Lyve Cloud Analytics by Iguazio is an end-to-end managed platform that enables you to unlock the full potential of your data while reducing your storage, computing, and analytics operations TCO by 40%. It is a highly scalable and secure platform that provides you with open-source industry standard t...", 
"body" : "Lyve Cloud Analytics by Iguazio is an end-to-end managed platform that enables you to unlock the full potential of your data while reducing your storage, computing, and analytics operations TCO by 40%. It is a highly scalable and secure platform that provides you with open-source industry standard tools and resources necessary to compete in the complex and dynamic analytics environment we find ourselves in today. Lyve Cloud Analytics is a production-centric data analytics solution that accelerates data engineering, data science, and data governance, all while encouraging data ops and machine learning operations best practices. This architecture is built on a foundation of data storage, computing, monitoring, and security. The platform's tight integration provides users with a unified approach that reduces the technical debt associated with modern analytic and machine learning systems. " }, 
{ "title" : "Features of Lyve Cloud Analytics ", 
"url" : "lyve-cloud-analytics.html#features-of-lyve-cloud-analytics", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Features of Lyve Cloud Analytics ", 
"snippet" : "Data Storage and Compute: Cost-effective data storage and state-of-the-art computing resources. Data Operations: Solutions for batch or real-time data flow and management using centralized or distributed processing. Machine Learning Operations: Machine learning experiment tracking and governance wit...", 
"body" : "Data Storage and Compute: Cost-effective data storage and state-of-the-art computing resources. Data Operations: Solutions for batch or real-time data flow and management using centralized or distributed processing. Machine Learning Operations: Machine learning experiment tracking and governance with simplified deployment and monitoring. Analytics Accelerators: Pre-built models and templates emphasizing deployment to expedite the machine learning lifecycle. The following diagram helps to visualize the different features of the platform as well as the relevant open-sourced technologies that it is comprised of. " }, 
{ "title" : "Getting started with Analytics ", 
"url" : "getting-started-with-analytics.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Getting started with Analytics ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "How does Lyve Cloud Analytics work? ", 
"url" : "getting-started-with-analytics.html#how-does-lyve-cloud-analytics-work-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Getting started with Analytics \/ How does Lyve Cloud Analytics work? ", 
"snippet" : "The Lyve Cloud Analytics platform allows you to analyze and process massive amounts of data on the Lyve Cloud using big data frameworks such as Apache Spark, Trino, and ML. You can process data for analytics and business intelligence workloads by using these frameworks and related open-source projec...", 
"body" : "The Lyve Cloud Analytics platform allows you to analyze and process massive amounts of data on the Lyve Cloud using big data frameworks such as Apache Spark, Trino, and ML. You can process data for analytics and business intelligence workloads by using these frameworks and related open-source projects. The Lyve Cloud Analytics platform also allows you to transform and move large amounts of data between data stores and databases on Lyve Cloud. Data is collected and stored from databases, IoT devices, sensors, media sources, files and logs. Platform capabilities include using higher-level languages to create processing workloads to schedule and monitor and leverage machine learning algorithms, build stream processing applications, and build data warehouses. The ML library enables the quick creation of predictive models, ad-hoc query analysis and reports with dashboards for end users (consumers). " }, 
{ "title" : "Onboarding Analytics ", 
"url" : "getting-started-with-analytics.html#onboarding-analytics", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Getting started with Analytics \/ Onboarding Analytics ", 
"snippet" : "You can sign up for Lyve Cloud Analytics (powered by Iguazio) products from the Lyve Cloud console. You must have a zCompute account before you sign up for Analytics. To sign up for zCompute, see Onboarding Lyve Cloud Compute by Zadara. To sign up for Analytics: On the left-hand menu, select Analyti...", 
"body" : "You can sign up for Lyve Cloud Analytics (powered by Iguazio) products from the Lyve Cloud console. You must have a zCompute account before you sign up for Analytics. To sign up for zCompute, see Onboarding Lyve Cloud Compute by Zadara. To sign up for Analytics: On the left-hand menu, select Analytics . On the Analytics page, select Sign Up . In the Analytics Onboarding Form , fill all the required details. After all the details are filled in, and the form is submitted successfully: You will receive an email confirming the submission of the form. The Lyve Cloud team will set up the analytics platform for the account. Note that initially, a trial account is set up to use the analytics platform, which later can be converted to a paying account. You can choose to close the analytics platform after the trial period. Analytics capabilities are available to Lyve Cloud users after enabling Analytics. It enables users to manage resources between different projects on a single platform. After connecting to Lyve Cloud Analytics, you must create the required service. For more information, see Using Analytics services . " }, 
{ "title" : "Using Analytics services ", 
"url" : "using-analytics-services.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services ", 
"snippet" : "Using a self-service model, you can view and manage the application services from the analytics dashboard. You can also install the additional software on the platform services for real-time data analytics and visualization tools. The application services include default and user-defined services. L...", 
"body" : "Using a self-service model, you can view and manage the application services from the analytics dashboard. You can also install the additional software on the platform services for real-time data analytics and visualization tools. The application services include default and user-defined services. Listed below are a few applications which can help you with data science workflow, data collection to production and more. The platform allows you to automate the deployment, scale and manage the application services. For more information on services, see Working with Services . Jupyter Notebook Service Spark Service Trino Service Grafana Dashboards You can use the following services as default services or as part of the Lyve Cloud integration. You can use either for data analytics: Trino Service How to launch Trino with Hive on Lyve Cloud Using TrinoSpark services How to launch Spark service on Lyve Cloud Submitting a Spark job" }, 
{ "title" : "Setting Labels on App Nodes ", 
"url" : "using-analytics-services.html#setting-labels-on-app-nodes", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Setting Labels on App Nodes ", 
"snippet" : "Lyve cloud Analytics platform K8S Node Label automates the provisioning and lifecycle management of nodes for Kubernetes clusters. You can provision optimized groups of nodes with labels for these clusters. When you configure nuclio and mlrun services in the  Custom parameters  tab, the node selecto...", 
"body" : "Lyve cloud Analytics platform K8S Node Label automates the provisioning and lifecycle management of nodes for Kubernetes clusters. You can provision optimized groups of nodes with labels for these clusters. When you configure nuclio and mlrun services in the  Custom parameters  tab, the node selector users can enter a label that identifies the servers on which the pod can run. To set the labels on App Nodes: On the Clusters page, select the App Nodes tab and Edit Label for one or more nodes. Select + and type in a  key: value  pair, and select Apply . You can add more key: value pairs. The label name (or names) is displayed in the node row " }, 
{ "title" : "Using Trino ", 
"url" : "using-analytics-services.html#using-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino ", 
"snippet" : "The analytics platform provides Trino as a service for data analysis. Trino is a distributed query engine that accesses data stored on object storage through ANSI SQL. This is also used for interactive query and analysis. Trino is integrated with enterprise authentication and authorization automatio...", 
"body" : "The analytics platform provides Trino as a service for data analysis. Trino is a distributed query engine that accesses data stored on object storage through ANSI SQL. This is also used for interactive query and analysis. Trino is integrated with enterprise authentication and authorization automation to ensure seamless access provisioning with access ownership at the dataset level residing with the business unit owning the data. With Trino resource management and tuning, we ensure 95% of the queries are completed in less than 10 seconds to allow interactive UI and dashboard fetching data directly from Trino. This avoids the data duplication that can happen when creating multi-purpose data cubes. DBeaver is a universal database administration tool to manage relational and NoSQL databases. Users can connect to Trino from DBeaver to perform the SQL operations on the Trino tables. " }, 
{ "title" : "Configuring Trino service ", 
"url" : "using-analytics-services.html#configuring-trino-service", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Configuring Trino service ", 
"snippet" : "To configure Trino with Lyve Cloud: On the left-hand menu of the Platform Dashboard, select Services and then select New Services . In the Create a new service dialogue, complete the following: Basic Settings : Configure your service by entering the following details: Service type : Select Trino fro...", 
"body" : "To configure Trino with Lyve Cloud: On the left-hand menu of the Platform Dashboard, select Services and then select New Services . In the Create a new service dialogue, complete the following: Basic Settings : Configure your service by entering the following details: Service type : Select Trino from the list. Service name : Enter a unique service name. This name is listed on the Services page. Description : Enter the description of the service. Enabled : The check box is selected by default. Selecting the option allows you to configure the Common and Custom parameters for the service. Common Parameters : Configure the memory and CPU resources for the service. The platform uses the default system values if you do not enter any values. When setting the resource limits, consider that an insufficient limit might fail to execute the queries. Memory : Provide a minimum and maximum memory based on requirements by analyzing the cluster size, resources and available memory on nodes. Trino uses memory only within the specified limit. CPU : Provide a minimum and maximum number of CPUs based on the requirement by analyzing cluster size, resources and availability on nodes. Trino uses CPU only the specified limit. Priority Class : By default, the priority is selected as Medium. You can change it to High or Low. Running User : Specifies the logged-in user ID. Shared : Select the checkbox to share the service with other users. Custom Parameters : Configure the additional custom parameters for the Trino service. Replicas : Configure the number of replicas or workers for the Trino service. Enable Hive : Select the check box to enable Hive. Once enabled, You must enter the following: Username : Enter the username of the platform (Lyve Cloud Compute) user creating and accessing Hive Metastore. Container : Select big data from the list. This is the name of the container which contains Hive Metastore. Hive Metastore path : Specify the relative path to the Hive Metastore in the configured container. Select Create Servive . " }, 
{ "title" : "Configuring Trino with Lyve Cloud ", 
"url" : "using-analytics-services.html#configuring-trino-with-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Configuring Trino service \/ Configuring Trino with Lyve Cloud ", 
"snippet" : "To configure Trino with Lyve Cloud On the Services page, select the Trino services to edit. Select ellipses and then select Edit . On the Edit service dialog, select the Custom Parameters tab. Expand Advanced , in the Predefined section, and select the pencil icon to edit Hive. Specify the following...", 
"body" : "To configure Trino with Lyve Cloud On the Services page, select the Trino services to edit. Select ellipses and then select Edit . On the Edit service dialog, select the Custom Parameters tab. Expand Advanced , in the Predefined section, and select the pencil icon to edit Hive. Specify the following in the properties file: Property name Description hive.s3.aws-access-key Lyve cloud S3 access key is a private key used to authenticate for connecting a bucket created in Lyve Cloud. The access key is displayed when you create a new service account in Lyve Cloud. A service account contains bucket credentials for Lyve Cloud to access a bucket. For more information, see Creating a service account. hive.s3.aws-secret-key Lyve cloud S3 secret key is private key password used to authenticate for connecting a bucket created in Lyve Cloud. The secret key displays when you create a new service account in Lyve Cloud. For more information, see Creating a service account. hive.s3.endpoint Enter Lyve Cloud S3 endpoint of the bucket to connect to a bucket created in Lyve Cloud. For more information, see the S3 API endpoints. hive.s3.ssl.enabled Use the HTTPS to communicate with Lyve Cloud API. By default, it is set to true. hive.s3.path-style-access Use path-style access for all requests to access buckets created in Lyve Cloud. This is for S3-compatible storage that doesn’t support virtual-hosted-style access. By default it is set to false. For more information about other properties, see S3 configuration properties. " }, 
{ "title" : "Advanced configuration for Trino service ", 
"url" : "using-analytics-services.html#advanced-configuration-for-trino-service", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Configuring Trino service \/ Advanced configuration for Trino service ", 
"snippet" : "To configure advanced settings for Trino service: On the Services page, select the Trino services to edit. Select ellipses and then select Edit . On the Edit service dialog, select the Custom Parameters tab. Expand Advanced , to edit the Configuration File for Coordinator and Worker . You can edit t...", 
"body" : "To configure advanced settings for Trino service: On the Services page, select the Trino services to edit. Select ellipses and then select Edit . On the Edit service dialog, select the Custom Parameters tab. Expand Advanced , to edit the Configuration File for Coordinator and Worker . You can edit the properties file for Coordinators and Workers. Select the Coordinator and Worker tab, and select the pencil icon to edit the predefined properties file. The following are the predefined properties file: log properties : You can set the log level. For more information, see Log Levels . JVM Config : It contains the command line options to launch the Java Virtual Machine. For more information, see JVM Config . Config Properties : You can edit the advanced configuration for the Trino server. For more information, see Config properties . Catalog Properties : You can edit the catalog configuration for connectors, which are available in the catalog properties file. For more information, see Catalog Properties . " }, 
{ "title" : "Assigning node label to Trino ", 
"url" : "using-analytics-services.html#assigning-node-label-to-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Configuring Trino service \/ Assigning node label to Trino ", 
"snippet" : "Assign a label to a node and configure Trino to use a node with the same label and make Trino use the intended nodes running the SQL queries on the Trino cluster. During the Trino service configuration, node labels are provided, you can edit these labels later. To assign node label to Trino: On the ...", 
"body" : "Assign a label to a node and configure Trino to use a node with the same label and make Trino use the intended nodes running the SQL queries on the Trino cluster. During the Trino service configuration, node labels are provided, you can edit these labels later. To assign node label to Trino: On the Services menu, select the Trino service and select Edit . In the Edit service dialogue, verify the Basic Settings and Common Parameters and select Next Step . In the Node Selection section under Custom Parameters , select Create a new entry . Specify the Key and Value of nodes, and select Save Service . The values in the image are for reference. " }, 
{ "title" : "Connecting Trino in DBeaver ", 
"url" : "using-analytics-services.html#connecting-trino-in-dbeaver", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Connecting Trino in DBeaver ", 
"snippet" : "Prerequisite before you connect Trino with DBeaver. Download and Install DBeaver from https:\/\/dbeaver.io\/download\/ . To connect Trino in DBeaver Start DBeaver. In the Database Navigator panel and select New Database Connection . In the Connect to a database dialog, select All and type Trino in the s...", 
"body" : "Prerequisite before you connect Trino with DBeaver. Download and Install DBeaver from https:\/\/dbeaver.io\/download\/ . To connect Trino in DBeaver Start DBeaver. In the Database Navigator panel and select New Database Connection . In the Connect to a database dialog, select All and type Trino in the search field. Select Trino logo and select Next . Select the Main tab and enter the following details: Host : Enter the hostname or IP address of your Trino cluster coordinator. Port : Enter the port number where the Trino server listens for a connection. Database\/Schema : Enter the database\/schema name to connect. Username : Enter the username of Lyve Cloud Analytics by Iguazio console. Password : Enter the valid password to authenticate the connection to Lyve Cloud Analytics by Iguazio. Select Driver properties and add the following properties: SSL: Set SSL to True . SSL Verification: Set SSL verification to None . Select Test Connection . If the JDBC driver is not already installed, it opens the  Download driver files  dialog showing the latest available JDBC driver. You must select and download the driver. Select Finish once the testing is completed successfully. " }, 
{ "title" : "Scaling Trino ", 
"url" : "using-analytics-services.html#scaling-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Scaling Trino ", 
"snippet" : "When you create a new Trino cluster, it can be challenging to predict the number of worker nodes needed in future. The number of worker nodes ideally should be sized to both ensure efficient performance and avoid excess costs. Scaling can help achieve this balance by adjusting the number of worker n...", 
"body" : "When you create a new Trino cluster, it can be challenging to predict the number of worker nodes needed in future. The number of worker nodes ideally should be sized to both ensure efficient performance and avoid excess costs. Scaling can help achieve this balance by adjusting the number of worker nodes, as these loads can change over time. The Lyve Cloud analytics platform supports static scaling, meaning the number of worker nodes is held constant while the cluster is used. To scale Trino services On the left-hand menu of the Platform Dashboard , select Services . Select the ellipses against the Trino services and select Edit . Skip Basic Settings and Common Parameters and proceed to configure Custom Parameters . In the Custom Parameters section, enter the Replicas and select Save Service . Trino scaling is complete once you save the changes. " }, 
{ "title" : "Securing Trino ", 
"url" : "using-analytics-services.html#securing-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Securing Trino ", 
"snippet" : "After you install Trino the default configuration has no security features enabled. You can enable the security feature in different aspects of your Trino cluster....", 
"body" : "After you install Trino the default configuration has no security features enabled. You can enable the security feature in different aspects of your Trino cluster. " }, 
{ "title" : "Authentication in Trino ", 
"url" : "using-analytics-services.html#authentication-in-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Securing Trino \/ Authentication in Trino ", 
"snippet" : "You can configure a preferred authentication provider, such as LDAP. You can secure Trino access by integrating with LDAP. After completing the integration, you can establish the Trino coordinator UI and JDBC connectivity by providing LDAP user credentials. To enable LDAP authentication for Trino, L...", 
"body" : "You can configure a preferred authentication provider, such as LDAP. You can secure Trino access by integrating with LDAP. After completing the integration, you can establish the Trino coordinator UI and JDBC connectivity by providing LDAP user credentials. To enable LDAP authentication for Trino, LDAP-related configuration changes need to make on the Trino coordinator. On the left-hand menu of the  Platform Dashboard , select  Services . Select the ellipses against the Trino services and select Edit. Skip Basic Settings and Common Parameters and proceed to configure  Custom Parameters . In the Advanced section, add the ldap.properties file for Coordinator in the Custom section Configure the password authentication to use LDAP in ldap.properties as below. password-authenticator.name=ldap\nldap.url=ldaps:\/\/<ldap-server>:636\nldap.user-bind-pattern=<Refer below for\nusage>\nldap.user-base-dn=OU=Sites,DC=ad,DC=com Property name Description ldap.url The URL to the LDAP server. The URL scheme must be  ldap:\/\/  or  ldaps:\/\/ . It connects to the LDAP server without TLS enabled requires ldap.allow-insecure=true. ldap.user-bind-pattern This property can be used to specify the LDAP user bind string for password authentication. This property must contain the pattern  ${USER} , which is replaced by the actual username during password authentication. The property can contain multiple patterns separated by a colon. Each pattern is checked in order until a login succeeds or all logins fail. For example: ${USER}@corp.example.com:${USER}@corp.example.co.uk ldap.user-base-dn The base LDAP distinguished name for the user trying to connect to the server. For example: OU=America,DC=corp,DC=example,DC=com " }, 
{ "title" : "Authorization based on LDAP group membership ", 
"url" : "using-analytics-services.html#authorization-based-on-ldap-group-membership", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Securing Trino \/ Authentication in Trino \/ Authorization based on LDAP group membership ", 
"snippet" : "You can restrict the set of users to connect to the Trino coordinator in following ways: based on their group membership by setting the optional ldap.group-auth-pattern  property In addition to the basic LDAP authentication properties. Add below properties in ldap.properties file. Property name Desc...", 
"body" : "You can restrict the set of users to connect to the Trino coordinator in following ways: based on their group membership by setting the optional ldap.group-auth-pattern  property In addition to the basic LDAP authentication properties. Add below properties in ldap.properties file. Property name Description ldap.group-auth-pattern This property is used to specify the LDAP query for the LDAP group membership authorization. This query is executed against the LDAP server and if successful, a user distinguished name is extracted from a query result. Trino validates user password by creating LDAP context with user distinguished name and user password. For more information about authorization properties, see Authorization based on LDAP group membership . Add the ldap.properties file details in config.properties file of Cordinator using the password-authenticator.config-files=\/presto\/etc\/ldap.properties property: Save changes to complete LDAP integration. You must configure one step at a time and always apply changes on dashboard after each change and verify the results before you proceed. " }, 
{ "title" : "Querying to Trino ", 
"url" : "using-analytics-services.html#querying-to-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Querying to Trino ", 
"snippet" : "Once the Trino service is launched, create a web-based shell service to use Trino from the shell and run queries. On the left-hand menu of the Platform Dashboard, select  Services  and then select  New Services . In the Create a new service dialogue, complete the following: Basic Settings : Configur...", 
"body" : "Once the Trino service is launched, create a web-based shell service to use Trino from the shell and run queries. On the left-hand menu of the Platform Dashboard, select  Services  and then select  New Services . In the Create a new service dialogue, complete the following: Basic Settings : Configure your service by entering the following details: Service type: Select Web-based shell from the list. Service name: Enter a unique service name. This name is listed on the Services page. Description: Enter the description of the service. Enabled: The check box is selected by default. Selecting the option allows you to configure the Common and Custom parameters for the service. Common Parameters : Configure the memory and CPU resources for the service. The platform uses the default system values if you do not enter any values. When setting the resource limits, consider that an insufficient limit might fail to execute the queries. Memory: Provide a minimum and maximum memory based on requirements by analyzing the cluster size, resources and available memory on nodes. Web-based shell uses memory only within the specified limit. CPU: Provide a minimum and maximum number of CPUs based on the requirement by analyzing cluster size, resources and availability on nodes. Web-based shell uses CPU only the specified limit. Priority Class: By default, the priority is selected as Medium. You can change it to High or Low. Running User: Specifies the logged-in user ID. Shared: Select the checkbox to share the service with other users. Custom Parameters : Configure the additional custom parameters for the Web-based shell service. Spark: Assign Spark service from drop-down for which you want a web-based shell. Trino: Assign Trino service from drop-down for which you want a web-based shell. Service Account: A Kubernetes service account which determines the permissions for using the kubectl CLI to run commands against the platform's application clusters. Select CREATE SERVICE " }, 
{ "title" : "Sample querying to Trino ", 
"url" : "using-analytics-services.html#sample-querying-to-trino", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Trino \/ Querying to Trino \/ Sample querying to Trino ", 
"snippet" : "After you create a Web based shell with Trino service, start the service which opens web-based shell terminal to execute shell commands. Select the web-based shell with Trino service to launch web based shell. The Web based shell launches in new tab. Enter the Trino command to run the queries and in...", 
"body" : "After you create a Web based shell with Trino service, start the service which opens web-based shell terminal to execute shell commands. Select the web-based shell with Trino service to launch web based shell. The Web based shell launches in new tab. Enter the Trino command to run the queries and inspect catalog structures. Creating a schema Create a Schema with a simple query CREATE SCHEMA hive.test_123 . After the schema is created, execute SHOW create schema hive.test_123 to verify the schema. trino> create schema hive.test_123;\nCREATE SCHEMA\n\ntrino> show create schema hive.test_123;\nCreate Schema\n\n------------------------------------------------------------------\nCREATE SCHEMA hive.test_123\n\nAUTHORIZATION USER edpadmin\n WITH (\n    location ='v3io:\/\/projects\/user\/hive\/warehouse\/test_123.db'\n  )\n Creating a sample table and with the table name as Employee Create a sample table assuming you need to create a table named employee using CREATE TABLE statement. trino> CREATE TABLE IF NOT EXISTS hive.test_123.employee (eid\nvarchar, name varchar,\n    -> salary varchar, destination varchar);\n\nCREATE TABLE\n\ntrino> SHOW CREATE TABLE hive.test_123.employee;\n             Create Table\n\n---------------------------------------\nCREATE TABLE hive.test_123.employee (\n    eid varchar,\n    name varchar,\n    salary varchar,\n    destination varchar\n )\n WITH (\n    format = 'ORC'\n )\n Inserting data into the table Insert sample data into the employee table with an insert statement. trino> INSERT INTO hive.test_123.employee VALUES ('1201', 'Mark','45000','Technical manager');\n    ->\n    -> INSERT INTO hive.test_123.employee VALUES ('1202', 'Paul','45000','Technical writer');\n    ->\n    -> INSERT INTO hive.test_123.employee VALUES ('1203', 'Allen','40000','Hr Admin');\n    ->\n    -> INSERT INTO hive.test_123.employee VALUES ('1204', 'John','30000','Op Admin');\n Viewing data in the table View data in a table with select statement. \n\ntrino> select * from hive.test_123.employee;\n\neid  | name  | salary |  destination\n------+-------+--------+-------------------\n1203 | Allen | 40000   | Hr Admin\n\n1201 | Mark  | 45000  | Technical manager\n\n1202 | Paul | 45000  | Technical writer\n\n1204 | John | 30000  | Op Admin\n\n " }, 
{ "title" : "Submitting a Spark job ", 
"url" : "using-analytics-services.html#submitting-a-spark-job", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job ", 
"snippet" : "The platform uses MLRun to submit spark jobs and Spark Operator for running Spark jobs over k8s. When sending a request with MLRun to the Spark operator, the request contains your full application configuration, including the code and dependencies to run (packaged as a docker image or specified via ...", 
"body" : "The platform uses MLRun to submit spark jobs and Spark Operator for running Spark jobs over k8s. When sending a request with MLRun to the Spark operator, the request contains your full application configuration, including the code and dependencies to run (packaged as a docker image or specified via URIs), the infrastructure parameters (e.g. the memory, CPU, and storage volume specs to allocate to each Spark executor), and the Spark configuration. Kubernetes takes this request and starts the Spark driver in a Kubernetes pod (a k8s abstraction, just a docker container in this case). The Spark driver then communicates directly with the Kubernetes master to request executor pods, scaling them up and down at runtime according to the load if the dynamic allocation is enabled. Kubernetes takes care of the bin-packing of the pods onto Kubernetes nodes (the physical VMs) and dynamically scales the various node pools to meet the requirements. You can submit spark jobs on K8s via Jupyter Notebook. For more information, see Spark operator : " }, 
{ "title" : "How to submit a spark job? ", 
"url" : "using-analytics-services.html#how-to-submit-a-spark-job-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ How to submit a spark job? ", 
"snippet" : "You can submit a spark job using any of the following:...", 
"body" : "You can submit a spark job using any of the following: " }, 
{ "title" : "Using Python ", 
"url" : "using-analytics-services.html#using-python", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ How to submit a spark job? \/ Using Python ", 
"snippet" : "To submit a spark job using python: Install the mlrun package using the following command. pip install mlrun Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses...", 
"body" : "To submit a spark job using python: Install the mlrun package using the following command. pip install mlrun\n Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses the spark code and locate it in the file system. mlrun.run import new_function Set the spark driver and executor config sj = new_function(kind='spark', command='pi.py', name='my-pi-python') Add the fuse, daemon and Jar support. sj.with_driver_requests(cpu=\"1\",mem=\"512m\")\nsj.with_executor_requests(cpu=\"1\",mem=\"512m\")\nsj.with_driver_limits(cpu=\"1\")\nsj.with_executor_limits(cpu=\"1\")\n\n Adds fuse, daemon & Iguazio's jars support. sj.with_igz_spark()\n Set the artifact path to save the artifact. sr = sj.run(artifact_path=\"local:\/\/\/tmp\/\") " }, 
{ "title" : "Using Java ", 
"url" : "using-analytics-services.html#using-java", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ How to submit a spark job? \/ Using Java ", 
"snippet" : "To submit a spark job using Java: Install the mlrun package using the following command. pip install mlrun Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses t...", 
"body" : "To submit a spark job using Java: Install the mlrun package using the following command. pip install mlrun\n Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses the spark code and locate it in the file system. mlrun.run import new_function Set the spark driver and executor config sj = new_function(kind='spark', command='\/User\/spark-examples_2.12-3.0.1.jar', name='my-pi-java') Add the fuse, daemon and Jar support. sj.with_driver_requests(cpu=\"1\",mem=\"512m\")\nsj.with_executor_requests(cpu=\"1\",mem=\"512m\")\nsj.with_driver_limits(cpu=\"1\")\nsj.with_executor_limits(cpu=\"1\")\n\n Add the Java class name to execute. sj.with_igz_spark()\nsj.spec.job_type = \"Java\"\n sj.spec.main_class= \"org.apache.spark.examples.JavaSparkPi\"\n Set the artifact path to save the artifact. sr = sj.run(artifact_path=\"local:\/\/\/tmp\/\") " }, 
{ "title" : "Using Scala ", 
"url" : "using-analytics-services.html#using-scala", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ How to submit a spark job? \/ Using Scala ", 
"snippet" : "To submit a spark job using scala: Install the mlrun package using the following command. pip install mlrun Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses ...", 
"body" : "To submit a spark job using scala: Install the mlrun package using the following command. pip install mlrun\n Run the following code with Python or Jupyther notebook Run the following command to import a new function. This will set up a new spark function with the spark operator, and the command uses the spark code and locates it in the file system. mlrun.run import new_function Set the spark driver and executor config. sj = new_function(kind='spark', command='\/User\/spark-examples_2.12-3.0.1.jar', name='my-pi-scala') sj.with_driver_requests(cpu=\"1\",mem=\"512m\")\nsj.with_executor_requests(cpu=\"1\",mem=\"512m\")\nsj.with_driver_limits(cpu=\"1\")\nsj.with_executor_limits(cpu=\"1\")\n\n Add the fuse, daemon and Jar support. \nsj.with_igz_spark()\nsj.spec.job_type = \"Scala\"\n Add the scala class name to execute sj.spec.main_class= \"org.apache.spark.examples.SparkPi\"\n Set the artifact path to save the artifact. sr = sj.run(artifact_path=\"local:\/\/\/tmp\/\") " }, 
{ "title" : "Assigning node label to Spark ", 
"url" : "using-analytics-services.html#assigning-node-label-to-spark", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ Assigning node label to Spark ", 
"snippet" : "Assign a driver or executor label to the node with the same label while submitting the jobs. It will use the driver and executor node from the provided list. For more information about node label, see Setting Labels on App Nodes . To assign a node label to the spark job: Submitting job via Jupyter n...", 
"body" : "Assign a driver or executor label to the node with the same label while submitting the jobs. It will use the driver and executor node from the provided list. For more information about node label, see Setting Labels on App Nodes . To assign a node label to the spark job: Submitting job via Jupyter notebook or Python . Provide the driver node label and executor node label when you submit a job using Jupyter notebook. The following code snippet is about driver and executor node section with node label key and node label. sj.with_driver_node_selection(node_selector={node_label_key:driver_node_label})\nsj.with_executor_node_selection(node_selector={node_label_key:executor_node_label}) " }, 
{ "title" : "Best practices to optimize Spark jobs ", 
"url" : "using-analytics-services.html#best-practices-to-optimize-spark-jobs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Submitting a Spark job \/ Best practices to optimize Spark jobs ", 
"snippet" : "Optimizing your Spark jobs is critical to process large workloads, especially when resources are limited. With Spark, you can influence many configurations before using cluster elasticity. Below are recommendations for improving your Spark jobs Use the correct file format Most Spark jobs run as a pi...", 
"body" : "Optimizing your Spark jobs is critical to process large workloads, especially when resources are limited. With Spark, you can influence many configurations before using cluster elasticity. Below are recommendations for improving your Spark jobs Use the correct file format Most Spark jobs run as a pipeline where one Spark job writes data to a File, and another Spark job reads the data, processes it, and writes to another file to pick up. Writing intermediate files in serialized and optimized formats like Avro, Kryo, Parquet, etc., performs better than text, CSV, or JSON formats. Spark’s default file format is Apache Parquet which follows column-oriented storage. Data is stored contiguously within the same column, which is well-suited to perform queries (transformations) on a subset of columns and on large dataframe . Only the data associated with the required columns are loaded into memory. Trino is the faster SQL with its custom ORC reader implementation. ORC uses a two step system to decode data. The first step is a traditional compression algorithm like gzip that generically reduces data size. The second step has data type specific compression algorithms that convert the raw bytes into values. Maximize parallelism in Spark Spark is efficient because it can process several tasks in parallel at scale. Creating a clear division of tasks and allowing Spark to read and process data in parallel will optimize performance. Users should split a dataset into several partitions that can be read and processed independently and in parallel. Create partitions in the following manner: When reading the data, create a partition using the Spark parameter spark.sql.files.maxPartitionBytes (default value of 128 MB) . Data stored in several partitions on a disk is the best-case scenario. The following example shows a dataset in parquet format with a directory containing data partition files between 100 and 150 MB in size. df.repartition(100) \ndf.coalesce(100) Increase the number of partitions by lowering the value of the spark.sql.files.maxPartitionBytes parameter. Note: This choice can lead to issues for files with small file sizes So, parameters should be defined empirically according to the available resources. Directly in the Spark application code using the Dataframe API. Coalesce decreases the number of partitions while avoiding a shuffle in the network. Increase the number of partitions by lowering the value of the spark.sql.files.maxPartitionBytes parameter. Beware of shuffle operations Shuffle partitions are created during the stages of a job involving a shuffle, i.e. when a wide transformation (For example, groupBy(), join() ) is performed. The setting of these partitions impacts both the network and the read\/write disk resources. The number of partitions may be adjusted by changing the value of spark.sql.shuffle.partitions . This parameter should be adjusted according to the size of the data. The default number of partitions is 200, which may be too high for some processes and result in too many partitions being exchanged between executing nodes. Configurations Default Value, recommendation and description spark.driver.memory The default value is 1GB. This represents the memory allocated to the Spark driver to collect data from the executor nodes. For example, collect()operation spark.shuffle.file.buffer The default value is 32 KB. We recommend increasing this to 1 MB. This allows the spark to buffer before writing results to disk. Use Broadcast Hash Join A join between several dataframes is a common operation. In a distributed context, a large amount of data is exchanged in the network between the executing nodes to perform the join. Depending on the size of the tables, this exchange causes network latency, which slows down processing. Spark offers several join strategies to optimize this operation, such as Broadcast Hash Join (BHJ). This technique is suitable when one of the merged dataframes is sufficiently small to be duplicated in memory on all the executing nodes. By duplicating the smallest table, the join no longer requires any significant data exchange in the cluster apart from the broadcast of this table beforehand, which significantly improves the speed of the join. The Spark configuration parameter to modify is spark.sql.autoBroadcastHashJoi n. The default value is 10 MB, i.e., this method is chosen if one of the two tables is smaller than this size. If sufficient memory is available, it may be very useful to increase this value or set it to -1 to force Spark to use it. Cache intermediate results Spark uses lazy evaluation and a DAG to describe a job to optimize computations and manage memory resources. This offers the possibility for a quick recalculation of the steps before action and thus executing only part of the DAG. To take full advantage of this functionality, it is encouraged to store expensive intermediate results if several operations use them downstream of the DAG. If an action is run, its computation can be based on these intermediate results and thus only replay a sub-part of the DAG before this action. Users may decide to cache immediate results to speed up the execution. dataframe = spark.range(1 *\n1000000).toDF(\"id\").withColumn(\"square\",col(\"id\")**2)\ndataframe.persist()\ndataframe.count() The above code took 2.5 Seconds to execute. dataframe.count()\n The subsequent execution took 147 ms, requiring lesser time as data is cached. The same also can be cached using the below. dataframe.persist(StorageLevel=\"MEMORY_ONLY\") The full list of options is available here . Manage the memory of the executor nodes The memory of a Spark executor is broken down to execution memory, storage memory and reserved memory. Execution memory = spark.memory.fraction * (spark.executor.memory - Reserved memory)\n By default, the spark.memory.fraction parameter is set to 0.6. This means that 60% of the memory is allocated for execution and 40% for storage. Once the default reserved memory of 300 MB is removed, it prevents out-of-memory (OOM) errors. We can modify the following two properties: spark.executor.memory\nspark.memory.fraction Don’t use collect (). use take() instead When a user calls the collect action, the result is returned to the driver node. This might seem innocuous initially, but if you are working with huge amounts of data, the driver node can easily run out of memory. The take() action is simple—it scans and returns the first partition it finds. For example, if you just want to get a feel of the data, then take(1) row of data. df = spark.read.csv(\"\/FileStore\/tables\/train.csv\", header=True)\ndf.collect()\ndf.take(1) Avoid using UDF UDFs are a black box to Spark hence it can’t apply optimization, and it loses all the optimization that Spark does on Dataframe\/Dataset (Tungsten and catalyst optimizer). Whenever possible, we should use Spark SQL built-in functions as these functions are designed to provide optimization. Use Broadcast and Accumulators variables Accumulators and Broadcast variables are both Spark-shared variables. Accumulators is a write \/update shared variable whereas Broadcast is a read shared variable. In a distributed computing engine like Spark, it’s necessary to know the scope and life cycle of variables and methods while executing. This is important to understand because data is divided and executed in parallel on different machines in a cluster. Broadcast Variable : It is a read only variable that is cached on all the executors to avoid shuffling of data between executors. Broadcast variables are used as lookups without any shuffle, as each executor will keep a local copy. Accumulator variable : Accumulators are also known as counters in map reduce, which is an update variable. It is used when you want Spark workers to update some value. Partition and Bucketing columns Shuffling is the main reason that affects Spark Job performance. In order to reduce shuffling of data, one of the more important techniques is to use bucketing and repartitioning of data instead of broadcasting, caching\/persisting dataframes. These are the 2 optimization techniques which we used for hive tables. Both are used to organize large filesystem data that is leveraged in subsequent queries to efficiently distribute data into partitions. partitionBy() is a method of grouping the same type of data into partitions which will be stored in a directory. Bucketing is also a similar kind of grouping, but it is based on hashing technique which will be stored as a file. Garbage Collection Tuning JVM garbage collection can be a problem when you have a large collection of unused objects. The first step in GC tuning is to collect statistics by choosing verbose while submitting spark jobs. In an ideal situation, we try to keep GC overheads < 10% of heap memory. " }, 
{ "title" : "Using Nuclio ", 
"url" : "using-analytics-services.html#using-nuclio", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Nuclio ", 
"snippet" : "Nuclio is an open-source and managed serverless platform used to minimize development and maintenance overhead and automate the deployment of data-science-based applications. The framework focused on data, I\/O, and compute-intensive workloads. It is well integrated with popular data science tools, s...", 
"body" : "Nuclio is an open-source and managed serverless platform used to minimize development and maintenance overhead and automate the deployment of data-science-based applications. The framework focused on data, I\/O, and compute-intensive workloads. It is well integrated with popular data science tools, such as Jupyter and Kubeflow, supports a variety of data and streaming sources, and supports execution over CPUs and GPUs. You can use Nuclio through a fully managed application service, the Lyve cloud analytics platform. MLRun serving utilizes serverless Nuclio functions to create multi-stage real-time pipelines. Nuclio addresses the desired capabilities of a serverless framework: Real-time processing with minimal CPU\/GPU and I\/O overhead and maximum parallelism Native integration with a large variety of data sources, triggers, processing models, and ML frameworks Stateful functions with data-path acceleration Following is the sample code snippet about trigger rabbit-mq from mlrun exchangeName : The exchange that contains the queue topics and queueName : They are mutually exclusive. The trigger can either create an existing queue specified by queueName or create its queue, subscribing it to topics. topics : If you Specify the trigger, create a queue with a unique name and subscribe it to these topics. url : It is the actual host URL and port details where queue address details are available. import mlrun\nimport json\nclass Echo:\n def __init__(self, context, name=None, **kw):\n self.context = context\n self.name = name\n self.kw = kw\n\n def do(self,x):\n y = type(x)\n print(\"Echo:\", \"done consuming\", y)\n return x\n\nfunction = mlrun.code_to_function(\"rabbit2\",kind=\"serving\", \n image=\"mlrun\/mlrun\")\ntrigger_spec={\"kind\":\"rabbit-mq\",\n \"maxWorkers\": 1,\n \"url\": \"amqp:\/\/athena-spr-kpiv-prod-admin:46f08c5d998e@spr-athena-rabbitmq.stni.seagate.com:5672\/athena-spr-kpiv-prod-01\",\n \"attributes\":{\n \"exchangeName\": \"athena.spr.topic.inference\",\n \"queueName\": \"athena-spr-sampling-shadow-validate\",\n \"topic\": \"#.sampling.#\"},\n }\nfunction.add_trigger(\"rabbitmq\",trigger_spec)\n\ngraph = function.set_topology(\"flow\", engine=\"async\") \ngraph.to(class_name=\"Echo\", name=\"testingRabbit\") \nfunction.deploy() " }, 
{ "title" : "Using Grafana dashboard ", 
"url" : "using-analytics-services.html#using-grafana-dashboard", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Grafana dashboard ", 
"snippet" : "Grafana is an open-source solution for running data analytics, reporting metrics of massive data sets and providing customizable dashboards. The tool helps to visualize metrics, time series data and application analytics. The dashboard helps to: Explore the data Track user behaviour Track applicatio...", 
"body" : "Grafana is an open-source solution for running data analytics, reporting metrics of massive data sets and providing customizable dashboards. The tool helps to visualize metrics, time series data and application analytics. The dashboard helps to: Explore the data Track user behaviour Track application behaviour Identify the frequency and type of errors in production or staging environments. Grafana with Lyve Cloud analytics platform natively integrates with other services so you can securely add, query, visualize, and analyze your data across multiple accounts and regions. " }, 
{ "title" : "Configuring Grafana service ", 
"url" : "using-analytics-services.html#configuring-grafana-service", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Grafana dashboard \/ Configuring Grafana service ", 
"snippet" : "To configure Grafana service: On the left-hand menu of the Platform Dashboard , select Services and then select New Services . In the Create a new service dialogue, complete the following configuration: Basic Settings : Configure your service by entering the following details: Service type : Select ...", 
"body" : "To configure Grafana service: On the left-hand menu of the Platform Dashboard , select Services and then select New Services . In the Create a new service dialogue, complete the following configuration: Basic Settings : Configure your service by entering the following details: Service type : Select Grafana from the list. Service name : Enter the service name. This name is listed on the Services page. Description : Enter the description of the service. Enabled : The check box is selected by default. Selecting the option allows you to configure the Common and Custom parameters for the service. Common Parameters : Configure the memory and CPU resources for the service. If you do not enter any values, the platform uses the default system values. When setting the resource limits, consider that an insufficient limit might fail to execute the queries. Memory : Provide a minimum and maximum memory based on requirements by analyzing the cluster size, resources and available memory on nodes. Trino uses memory only within the specified limit. CPU : Provide a minimum and maximum number of CPUs based on the requirement by analyzing cluster size, resources and availability on nodes. Trino uses CPU only the specified limit. Priority Class : By default, the priority is selected as Medium. You can change it to High or Low. Running User : Specifies the logged-in user ID. Custom Parameters : Configure the user and the node label key and value. Platform data-access user : Enter the username\/First Name\/Last Name\/Email of the user who has data access to the platform's data containers. Node Selection : Assign a node for the services to run. The Grafana service will run only on the nodes that are defined with the labels. Select Create a new entry . Key : Enter the key for the node label. Value : Enter the value of the key. If there are conflicting key values, where the same key is assigned for multiple servers but it has different values, the system prompts you to delete the duplicate keys. Select Save Service . " }, 
{ "title" : "Understanding pre-built panels and dashboard ", 
"url" : "using-analytics-services.html#understanding-pre-built-panels-and-dashboard", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Grafana dashboard \/ Understanding pre-built panels and dashboard ", 
"snippet" : "Grafana makes it easy to construct the right queries and customize the display properties to create your required dashboard. With multiple pre-built dashboards for various data sources, you can instantly start visualizing and analyzing your application data without having to build dashboards from sc...", 
"body" : "Grafana makes it easy to construct the right queries and customize the display properties to create your required dashboard. With multiple pre-built dashboards for various data sources, you can instantly start visualizing and analyzing your application data without having to build dashboards from scratch. The following image shows a pre-built dashboard visualizing data for a Kubernetes cluster. Grafana provides pre-built dashboards to help you get started quickly. " }, 
{ "title" : "Viewing Grafana dashboards ", 
"url" : "using-analytics-services.html#viewing-grafana-dashboards", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Using Grafana dashboard \/ Viewing Grafana dashboards ", 
"snippet" : "You can view and create a new dashboard for a cluster. To view the Grafana dashboard. On the left-hand menu of the Platform Dashboard , select Clusters . On the Clusters page select the Applications tab, and then select Status Dashboard . Selecting Status Dashboard directs you to the Grafana dashboa...", 
"body" : "You can view and create a new dashboard for a cluster. To view the Grafana dashboard. On the left-hand menu of the Platform Dashboard , select Clusters . On the Clusters page select the Applications tab, and then select Status Dashboard . Selecting Status Dashboard directs you to the Grafana dashboard of the corresponding cluster. Monitor the cluster resource usage. In this case, Kubernetes Resource Usage Analysis is used as an example as a cluster resource usage. However you can search for other cluster resources such as Kubernetes Custer Health, Kubernetes Cluster Status, etc. On the Status Dashboard, select Dashboard , and then select Manage . On the Dashboard page, select Private folder, and search Kubernetes Resource Usage Analysis . The Kubernetes Resource Usage Analysis displays the Overall usage, Total usage, Usage by Node, Usage by Pod, etc. You can expand the resources to view the graphical representation of the usage dashboard. Usage Overview Usage Overview Pending\/ Failed Pods Pod Count Usage - Total Usage – By Nodes Usage – By Pods " }, 
{ "title" : "Managing Jupyter service ", 
"url" : "using-analytics-services.html#managing-jupyter-service", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Lyve Cloud Analytics \/ Using Analytics services \/ Managing Jupyter service ", 
"snippet" : "Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Its uses include data cleaning and transformation, numerical simulation, statistical modelling, data visualization, machine learning, and muc...", 
"body" : "Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. Its uses include data cleaning and transformation, numerical simulation, statistical modelling, data visualization, machine learning, and much more. Refer to Jupyter Notebook Service for details about Jupyter's service and Jupyter flavours on the Lyve cloud analytics platform. To create a Jupyter service Configure Jupyter from the cluster dashboard On the Services menu, select Add new service . In the Create a new service dialogue, complete the following: In the Basic Settings , enter the following and select Next . Service Type : Select the type Jupyter Notebook from the list. Service name : Enter the service name. Description : Enter the description. In the Common Parameters section, enter the following and select Next : Memory : Set the maximum memory limit. CPU : Set the maximum CPU limit. Priority : Set the priority. In the Custom Parameters section, enter the following: Flavor : Specify Flavor to use for the resources. Spark : Enter the spark job if required. Trino : Select Trino from the list. Environment Variable : Enter the Key and Key Value . Node Selection : Enter the node label Key and Value of nodes details. " }, 
{ "title" : "Connecting S3 clients ", 
"url" : "connecting-s3-clients-to-lyve-cloud.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients ", 
"snippet" : "This guide explains how to configure third-party clients including AWS CLI, Cyberduck, Mountain Duck, or S3 Browser to manage data in Lyve Cloud. Lyve Cloud is an S3-compatible storage service for data-intensive applications such as data backup and analytic workloads, that leverage multi-petabyte da...", 
"body" : "This guide explains how to configure third-party clients including AWS CLI, Cyberduck, Mountain Duck, or S3 Browser to manage data in Lyve Cloud. Lyve Cloud is an S3-compatible storage service for data-intensive applications such as data backup and analytic workloads, that leverage multi-petabyte data lakes. You can also use any other compatible third-party client to copy and move files, manage files and folders, and synchronize folders between Lyve Cloud and your local storage, once you've established a connection. This document and its subtopics provide instructions about how to: Configure Cyberduck, Mountain Duck, S3 Browser, or rclone to connect to Lyve Cloud. Upload, download, and delete files, and how to best manage files and folder. Disconnect from the server. " }, 
{ "title" : "Using Cyberduck ", 
"url" : "using-cyberduck.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck ", 
"snippet" : "Use Cyberduck to connect with Lyve Cloud and transfer your data. For more information, review the Cyberduck Tutorial and Cyberduck Quick Reference Guide . Prerequisites Download and Install Cyberduck. Register the S3 (HTTPS) profile  for preconfigured settings. For more information, see  Generic S3 ...", 
"body" : "Use Cyberduck to connect with Lyve Cloud and transfer your data. For more information, review the Cyberduck Tutorial and Cyberduck Quick Reference Guide . Prerequisites Download and Install Cyberduck. Register the S3 (HTTPS) profile  for preconfigured settings. For more information, see  Generic S3 profiles . To enable and register the S3 (HTTPS) profile: Select Edit and then Preferences . For Mac users, select Cyberduck and then Preferences . In the Profiles tab, choose S3 (HTTPS) from the connection profiles list. Alternatively, to register the S3 (HTTPS) profile: Open  S3 (HTTPS) profile connection profile file. Copy the file contents into a notepad\/any text editor. Save the notepad file name with .cyberduckprofile extension and change the Save as type to  All Files . You need both the access key and secret key for each account you plan to connect with Cyberduck. For more information, see Creating a service account. Consult your organization's policies and the EULA policies of the software before downloading 3rd party applications. Bookmark : Bookmarks store the details of the connection to easily re-connect to the server. To connect Cyberduck to Lyve Cloud In Cyberduck, select  Bookmark|New Bookmark. For Mac users, select + in the bottom left to add a New Bookmark . Select the S3 (HTTPS) protocol from the list. Enter the following mandatory details to add your connection to Lyve Cloud: Field Name Description Nickname Enter a name for the bookmark. URL Displays the URL once you enter the server and access key in the following format:http:\/\/ <Access Key ID><Server> Server Enter Lyve Cloud S3 endpoint. For more information see  S3 API endpoints. Lyve Cloud supports only region-specific S3 endpoints. To access buckets created in different regions in the S3 client, add an endpoint connection for each of the regions. Copy the URL without https:\/\/ Port Enter 443 as the port number to access the server. Access Key ID Enter your access key, a private key for authentication to connect a bucket created in Lyve Cloud. The access key is displayed when you create a new service account in Lyve Cloud. A service account contains bucket credentials for Lyve Cloud to access a bucket. For more information, see Creating a service account. Secret Access Key Enter your secret key, a private key password for authentication to connect a bucket created in Lyve Cloud. The secret key displays when you create a new service account in Lyve Cloud. For more information, see Creating a service account. The bookmark is displayed once you close the window. Right-click the bookmark and select  Connect to Server . Select  Continue  to establish the connection. View the buckets available in the created bookmark once the connection is established. On the Cyberduck client, the  Disconnect  button is displayed in the top right corner. A green dot appears to the right of active bookmarks, signifying an established connection. If no connection is established, the  Disconnect  icon is greyed out. Right-click the bucket or select Actions to perform various operations or actions. For more information, see  Managing Data. " }, 
{ "title" : "Video: How to connect Cyberduck to Lyve Cloud ", 
"url" : "using-cyberduck.html#video--how-to-connect-cyberduck-to-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Video: How to connect Cyberduck to Lyve Cloud ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Managing Data ", 
"url" : "using-cyberduck.html#managing-data-37585", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Managing Data ", 
"snippet" : "Perform various actions once a connection is established between Cyberduck and Lyve Cloud. For more information, see  Cyberduck Help ....", 
"body" : "Perform various actions once a connection is established between Cyberduck and Lyve Cloud. For more information, see  Cyberduck Help . " }, 
{ "title" : "Uploading data to a bucket ", 
"url" : "using-cyberduck.html#uploading-data-to-a-bucket-37585", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Uploading data to a bucket ", 
"snippet" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK To uplo...", 
"body" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK To upload data to a bucket: Select and right-click the bucket and select Upload and choose the file to upload. After the file transfer progress status is complete, you can view the file. Accept the certification installation and select  Continue . Certificate installation is prompted only when the certificate is installed for the first time. " }, 
{ "title" : "Downloading data to local storage ", 
"url" : "using-cyberduck.html#downloading-data-to-local-storage-37585", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Downloading data to local storage ", 
"snippet" : "To download data to local storage: Expand the bucket where files are available. Right-click the file to download and select one of the following options: Download: Download a file to the predefined path.  The Transfers dialog displays the connection status. Select  Continue  in the Download dialog. ...", 
"body" : "To download data to local storage: Expand the bucket where files are available. Right-click the file to download and select one of the following options: Download: Download a file to the predefined path.  The Transfers dialog displays the connection status. Select  Continue  in the Download dialog. You can view the remote file location and the local file location, but you cannot change the download path. Once the download is complete, the  Transfers  dialog displays the status. Download As:  Download a file in the required format.  Select  Save as Type  from the list, and select  Save . Download To : Download the file to a specific location.  Select Download To, then select the download folder in the  Browse to Folder  dialog, or create a new folder. Select Save . " }, 
{ "title" : "Deleting data from the bucket ", 
"url" : "using-cyberduck.html#deleting-data-from-the-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Downloading data to local storage \/ Deleting data from the bucket ", 
"snippet" : "To delete data from the bucket: Expand the bucket from which to delete data. Right-click the data file, and select  Delete. Select Delete in the Confirmation prompt....", 
"body" : "To delete data from the bucket: Expand the bucket from which to delete data. Right-click the data file, and select  Delete. Select Delete in the Confirmation prompt. " }, 
{ "title" : "Creating a new folder ", 
"url" : "using-cyberduck.html#creating-a-new-folder-37585", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Downloading data to local storage \/ Creating a new folder ", 
"snippet" : "To create a new folder: In Cyberduck, select Select and open a bucket. Right-click inside the bucket and select  New Folder. Enter the folder name in the  Create New Folder  screen....", 
"body" : "To create a new folder: In Cyberduck, select Select and open a bucket. Right-click inside the bucket and select  New Folder. Enter the folder name in the  Create New Folder  screen. " }, 
{ "title" : "Deleting a folder ", 
"url" : "using-cyberduck.html#deleting-a-folder-37585", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Downloading data to local storage \/ Deleting a folder ", 
"snippet" : "To delete a folder: Navigate into the bucket from which to delete the folder. Right click the folder and select Delete . Select Delete in the confirmation prompt....", 
"body" : "To delete a folder: Navigate into the bucket from which to delete the folder. Right click the folder and select Delete . Select Delete in the confirmation prompt. " }, 
{ "title" : "Disconnecting Cyberduck from Lyve Cloud ", 
"url" : "using-cyberduck.html#disconnecting-cyberduck-from-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Cyberduck \/ Downloading data to local storage \/ Disconnecting Cyberduck from Lyve Cloud ", 
"snippet" : "To disconnect Cyberduck from Lyve Cloud: Open Cyberduck to view all the available bookmarks or connections. Select  Disconnect in the top-right corner on the Cyberduck client. A green dot indicates an active bookmark and an established connection. However, if the connection is not established, the  ...", 
"body" : "To disconnect Cyberduck from Lyve Cloud: Open Cyberduck to view all the available bookmarks or connections. Select  Disconnect in the top-right corner on the Cyberduck client. A green dot indicates an active bookmark and an established connection. However, if the connection is not established, the  Disconnect  icon is greyed out. " }, 
{ "title" : "Using S3 Browser ", 
"url" : "using-s3-browser.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser ", 
"snippet" : "Use  S3 Browser  to connect with Lyve Cloud and manage your data transfer. For more information on S3 Browser see,  S3 Browser Help . Prerequisites You will need the access key and secret key for each account you’ll be using to connect with S3 browser. For more information, see  Creating a service a...", 
"body" : "Use  S3 Browser  to connect with Lyve Cloud and manage your data transfer. For more information on S3 Browser see,  S3 Browser Help . Prerequisites You will need the access key and secret key for each account you’ll be using to connect with S3 browser. For more information, see  Creating a service account. Consult your organization's policies and the EULA policies of the software before downloading 3rd party applications. " }, 
{ "title" : "", 
"url" : "using-s3-browser.html#-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ ", 
"snippet" : "To connect to Lyve Cloud Open S3 Browser and select  Accounts , then select  Add New Account . Enter the following mandatory details and Select Close . Field Name Description Account name Enter an account name. Account type Enter the account type.  Select  S3 Compatible Storage  from the list. REST ...", 
"body" : "To connect to Lyve Cloud Open S3 Browser and select  Accounts , then select  Add New Account . Enter the following mandatory details and Select Close . Field Name Description Account name Enter an account name. Account type Enter the account type.  Select  S3 Compatible Storage  from the list. REST Endpoint Enter Lyve Cloud S3 endpoint. For more information see  S3 API endpoints. Currently, Lyve Cloud supports only region-specific S3 endpoints. To access buckets created in different regions in the S3 client, add an endpoint connection for each of the regions. Access Key Enter your access key. The access key displays when you create a new service account in Lyve Cloud. A service account contains the bucket credentials for the Lyve Cloud bucket. For more information, see  Creating a service accountSecret Key Enter your secret key. The secret key displays when you create a new service account in Lyve Cloud. For more information, see  Creating a service accountUse secure transfer (SSL\/TLS) Select this option to ensure all communication with the storage passes through encrypted SSL\/TLS. Advanced S3 Compatible storage settings Select the signature version and addressing model in the advanced settings. For more information, see  Advance S3 compatible storage settings, below. " }, 
{ "title" : "Advance S3 compatible storage settings ", 
"url" : "using-s3-browser.html#advance-s3-compatible-storage-settings", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Advance S3 compatible storage settings ", 
"snippet" : "Field Name Description Signature Version Select  Signature V4. For more information, see S3 Browser's help documentation . Addressing Model Path Style  is selected by default and is the recommended setting. For more information, see S3 Browser's help documentation . Once the connection is establishe...", 
"body" : "Field Name Description Signature Version Select  Signature V4. For more information, see S3 Browser's help documentation . Addressing Model Path Style  is selected by default and is the recommended setting. For more information, see S3 Browser's help documentation . Once the connection is established, the bucket displays in the left pane. If there is no connection, an error displays. " }, 
{ "title" : "Video: How to Use S3 Browser with Lyve Cloud ", 
"url" : "using-s3-browser.html#video--how-to-use-s3-browser-with-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Video: How to Use S3 Browser with Lyve Cloud ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Managing data ", 
"url" : "using-s3-browser.html#managing-data-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data ", 
"snippet" : "Perform various actions once the connection between S3 Browser and Lyve Cloud is established. For more information, see  S3 Browser's help documentation ....", 
"body" : "Perform various actions once the connection between S3 Browser and Lyve Cloud is established. For more information, see  S3 Browser's help documentation . " }, 
{ "title" : "Uploading data to a bucket ", 
"url" : "using-s3-browser.html#uploading-data-to-a-bucket-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Uploading data to a bucket ", 
"snippet" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK To uplo...", 
"body" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK To upload data to a bucket: Select the bucket, then select  Upload , then select  Upload file(s) ; or select  Upload folder(s) . Select the file, then select  Open . " }, 
{ "title" : "Downloading data to local storage ", 
"url" : "using-s3-browser.html#downloading-data-to-local-storage-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Downloading data to local storage ", 
"snippet" : "To download data to your local machine: Select the bucket where the data file is available. Select the folder or file(s) to download, and select  Download . The  Tasks  tab displays the upload or download progress....", 
"body" : "To download data to your local machine: Select the bucket where the data file is available. Select the folder or file(s) to download, and select  Download . The  Tasks  tab displays the upload or download progress. " }, 
{ "title" : "Deleting data from a bucket ", 
"url" : "using-s3-browser.html#deleting-data-from-a-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Deleting data from a bucket ", 
"snippet" : "To delete data from a bucket: Navigate into the bucket, select the file from the right pane, and select  Delete . Select  Yes  in the  Confirm File Delete  confirmation....", 
"body" : "To delete data from a bucket: Navigate into the bucket, select the file from the right pane, and select  Delete . Select  Yes  in the  Confirm File Delete  confirmation. " }, 
{ "title" : "Creating a new folder ", 
"url" : "using-s3-browser.html#creating-a-new-folder-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Creating a new folder ", 
"snippet" : "To create a new folder: Navigate into the bucket in which to create a folder. Select  New Folder , enter the folder name, then select  Create New folder ....", 
"body" : "To create a new folder: Navigate into the bucket in which to create a folder. Select  New Folder , enter the folder name, then select  Create New folder . " }, 
{ "title" : "Deleting a folder ", 
"url" : "using-s3-browser.html#deleting-a-folder-38451", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Deleting a folder ", 
"snippet" : "To delete a folder: Open the bucket, right-click the folder to delete, then select  Delete . Select  Yes  in the  Confirm File Delete  confirmation....", 
"body" : "To delete a folder: Open the bucket, right-click the folder to delete, then select  Delete . Select  Yes  in the  Confirm File Delete  confirmation. " }, 
{ "title" : "Disconnecting S3 Browser from Lyve Cloud ", 
"url" : "using-s3-browser.html#disconnecting-s3-browser-from-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using S3 Browser \/ Managing data \/ Disconnecting S3 Browser from Lyve Cloud ", 
"snippet" : "To disconnect S3 browser Open the  S3 Browser  to view all available connections. Select  Accounts  from the menu. Select  Manage Accounts , select the account name, and select  Delete . Select  Save Changes ....", 
"body" : "To disconnect S3 browser Open the  S3 Browser  to view all available connections. Select  Accounts  from the menu. Select  Manage Accounts , select the account name, and select  Delete . Select  Save Changes . " }, 
{ "title" : "Using Mountain Duck ", 
"url" : "using-mountain-duck.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck ", 
"snippet" : "Use Mountain Duck to mount your Lyve Cloud storage as a disk in the Windows File Explorer or Mac OS Finder, and manage your files through a familiar interface. For more information on Mountain Duck see,  Mountain Duck Help . Prerequisites You will need the access key and secret key for each account ...", 
"body" : "Use Mountain Duck to mount your Lyve Cloud storage as a disk in the Windows File Explorer or Mac OS Finder, and manage your files through a familiar interface. For more information on Mountain Duck see,  Mountain Duck Help . Prerequisites You will need the access key and secret key for each account you’ll be using to connect with Mountain Duck. For more information, see  Creating a service account. Consult your organization's policies and the EULA policies of the software before downloading 3rd-party applications. The difference between Windows and macOS is mentioned specifically, else it is similar for both. To connect to Lyve Cloud: Download Mountain Duck’s S3 (HTTPS) profile  for preconfigured settings. For more information, see their  Generic S3 profiles  documentation. Open the downloaded file with Mountain Duck. The  New Connection  dialog appears. The dialog for macOS does not have a Drive Letter field. Enter the following information: Field Name Description Nickname Enter a unique name. This will be the name of your connection bookmark. Server Enter the endpoint based on the region. To select endpoint see,  S3 API endpoints. Currently, Lyve Cloud supports only region-specific S3 endpoints. To access buckets created in different regions in the S3 client, add an endpoint connection for each of the regions. Port This should populate as 443. If not, enter that port number. Access Key ID Enter your access key ID. For more information, see  Creating a service account. Secret Access Key Enter your secret key.For more information, see  Creating a service account. Drive Letter(Windows only) Enter a drive letter so that Mountain Duck always uses that same letter for the mounted drive. Once the fields have been filled, select  OK . This creates your connection bookmark. Select the Mountain Duck icon in the Windows system tray or the macOS' menu bar. Select your connection bookmark, then select  Connect . When the connection is created, a notification appears. Select the new drive and right-click or Ctrl+click in the File Explorer or Finder to bring up the context menu. Right-click in the folder to get the context menu for macOS. Select  Mountain Duck  and select  Keep offline on local disk  to sync all the data to local drive. When connected, the drive and folder contents display their sync status. Look for a circle in the lower-left corner of the folder or file icon. Once mounted, all the files are stored on your local drive. icon Meaning In Progress . Synchronization is in progress for this item. In sync. This item is selected to be synced, and the content will always be available offline. Sync error. This item cannot be synchronized. Up to date. This item is synced and up to date. Ignored. The file is available in its temporary location and never synced to cloud or remote storage. Paused. The sync on that item is paused. Online only. This item is available in the cloud but can be opened and edited when you have an active connection to the server. To learn more about various Mountain Duck options and sync modes, see Mountain Duck’s  Help documentation. " }, 
{ "title" : "Video: How to mount Lyve Cloud as a local drive on Windows with Mountain Duck ", 
"url" : "using-mountain-duck.html#video--how-to-mount-lyve-cloud-as-a-local-drive-on-windows-with-mountain-duck", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Video: How to mount Lyve Cloud as a local drive on Windows with Mountain Duck ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Managing Data ", 
"url" : "using-mountain-duck.html#managing-data-37586", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Managing Data ", 
"snippet" : "Once your Lyve Cloud storage is mounted as a drive, managing your files works much the same as working in any other network drive. Many of these operations may only be performed once a given bucket has synced with the local drive. Learn more about Mountain Duck’s  user interface documentation. The o...", 
"body" : "Once your Lyve Cloud storage is mounted as a drive, managing your files works much the same as working in any other network drive. Many of these operations may only be performed once a given bucket has synced with the local drive. Learn more about Mountain Duck’s  user interface documentation. The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK . " }, 
{ "title" : "Copying data ", 
"url" : "using-mountain-duck.html#copying-data-37586", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Copying data ", 
"snippet" : "To copy data: Select the data from your source drive and copy it. Navigate to your destination and paste your data. You can also drag and drop the data from one folder to another. If the bucket's or your service account’s permissions do not allow you to write to that bucket, an error message appears...", 
"body" : "To copy data: Select the data from your source drive and copy it. Navigate to your destination and paste your data. You can also drag and drop the data from one folder to another. If the bucket's or your service account’s permissions do not allow you to write to that bucket, an error message appears. " }, 
{ "title" : "Deleting bucket data ", 
"url" : "using-mountain-duck.html#deleting-bucket-data", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Deleting bucket data ", 
"snippet" : "To delete bucket data: Navigate to the synced bucket from which you want to delete the data. Right-click or Ctrl+click the file or folder inside the bucket to bring up the local context menu. Select Mountain Duck and select  Delete on Local Disk . Right-click or Ctrl+click the file and select  Delet...", 
"body" : "To delete bucket data: Navigate to the synced bucket from which you want to delete the data. Right-click or Ctrl+click the file or folder inside the bucket to bring up the local context menu. Select Mountain Duck and select  Delete on Local Disk . Right-click or Ctrl+click the file and select  Delete , then select  Yes  in the confirmation box to delete the object permanently from the bucket. " }, 
{ "title" : "Creating a folder in a bucket ", 
"url" : "using-mountain-duck.html#creating-a-folder-in-a-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Creating a folder in a bucket ", 
"snippet" : "To create a folder in a bucket: Navigate into the bucket where you want to create a new folder. Right-click or Ctrl+click the bucket and select New Folder. Type a new folder name, then select  Enter . The new folder immediately begins syncing with its Lyve Cloud destination....", 
"body" : "To create a folder in a bucket: Navigate into the bucket where you want to create a new folder. Right-click or Ctrl+click the bucket and select New Folder. Type a new folder name, then select  Enter . The new folder immediately begins syncing with its Lyve Cloud destination. " }, 
{ "title" : "Disconnecting Mountain Duck from Lyve Cloud ", 
"url" : "using-mountain-duck.html#disconnecting-mountain-duck-from-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Mountain Duck \/ Disconnecting Mountain Duck from Lyve Cloud ", 
"snippet" : "To disconnect to Lyve Cloud: In the Windows system tray or the macOS menu bar, select the  Mountain Duck client icon. The menu lists all of your connection bookmarks. Select a connection bookmark , and select  Disconnect . A notification pop-up appears when the connection is broken. For macOS, selec...", 
"body" : "To disconnect to Lyve Cloud: In the Windows system tray or the macOS menu bar, select the  Mountain Duck client icon. The menu lists all of your connection bookmarks. Select a connection bookmark , and select  Disconnect . A notification pop-up appears when the connection is broken. For macOS, select the icon against the connection bookmark to eject, ejecting will disconnect the connection. Also, click the connection and select Eject. Windows Mac Mountain Duck re-establishes any connections each time you restart your computer unless you specifically disconnect them. " }, 
{ "title" : "Using Rclone ", 
"url" : "using-rclone.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone ", 
"snippet" : "Use rclone to connect with Lyve Cloud and manage your files from the command line, or mount the cloud storage as a drive. Prerequisites You will need the access key and secret key for each account you’ll be using to connect with Rclone. For more information, see  Creating a service accountConsult yo...", 
"body" : "Use rclone to connect with Lyve Cloud and manage your files from the command line, or mount the cloud storage as a drive. Prerequisites You will need the access key and secret key for each account you’ll be using to connect with Rclone. For more information, see  Creating a service accountConsult your organization's policies and the EULA policies of the software before downloading 3rd-party applications. " }, 
{ "title" : "Connecting to Lyve Cloud from Linux ", 
"url" : "using-rclone.html#connecting-to-lyve-cloud-from-linux", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux ", 
"snippet" : "Installing Rclone Download rclone  for Linux, then extract the rclone binary to your desired location. To use rclone, open a terminal window and navigate to the directory where you saved the executable....", 
"body" : "Installing Rclone Download rclone  for Linux, then extract the rclone binary to your desired location. To use rclone, open a terminal window and navigate to the directory where you saved the executable. " }, 
{ "title" : "Configuring rclone to connect to Lyve Cloud ", 
"url" : "using-rclone.html#configuring-rclone-to-connect-to-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud ", 
"snippet" : "To configure a remote connection with rclone: Run  rclone config  to setup and select  n  for a new remote. No remotes found - make a new one        n) New remote        s) Set configuration password        q) Quit config        n\/s\/q> n Enter a name for the configuration. name> <Name> Select s3 sto...", 
"body" : "To configure a remote connection with rclone: Run  rclone config  to setup and select  n  for a new remote. No remotes found - make a new one       \nn) New remote        \ns) Set configuration password        \nq) Quit config       \nn\/s\/q> n Enter a name for the configuration. name> <Name> Select s3 storage. Type the storage to configure.\nChoose a number from below, or type in your own value        \n1 \/Fichier \n \\(fichier)  \n2 \/Akamai NetStorage\n \\(netstorage) \n3 \/ Alias for an existing remote \n \\ (alias)        \n4 \/ Amazon Drive \n \\ (amazon cloud drive)        \n5 \/ Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Lyve Cloud, Minio, RackCorp, SeaweedFS, and Tencent COS  \n \\ (s3)        \n6 \/ Backblaze B2   \n \\ (b2)  [snip]        \n46 \/ seafilehttp Connection \n \\ (seafile)\n Storage> 5 Choose Lyve Cloud  as the storage provider. Choose the S3 provider.\nChoose a number from below, or type in your own value\nPress Enter for the default (\"\")\n  1 \/ Amazon Web Services (AWS) S3\n \\ (AWS)\n 2 \/ Alibaba Cloud Object Storage System (OSS) formerly Aliyun\n \\ (Alibaba)\n 3 \/ Ceph Object Storage\n \\ (Ceph)\n 4 \/ Digital Ocean Spaces\n \\ (DigitalOcean)\n 5 \/ Dreamhost DreamObjects\n \\ (Dreamhost)\n 6 \/ IBM COS S3\n \\ (IBMCOS)\n 7 \/ Seagate Lyve Cloud\n \\ (LyveCloud)\n 8 \/ Minio Object Storage\n \\ (Minio)\n 9 \/ Netease Object Storage (NOS)\n \\ (Netease)\n10 \/ RackCorp Object Storage\n \\ (RackCorp)\n11 \/ Scaleway Object Storage\n \\ (Scaleway)\n12 \/ SeaweedFS S3\n \\ (SeaweedFS)\n13 \/ StackPath Object Storage\n \\ (StackPath)\n14 \/ Storj (S3 Compatible Gateway)\n \\ (Storj)\n15 \/ Tencent Cloud Object Storage (COS)\n \\ (TencentCOS)\n16 \/ Wasabi Object Storage\n \\ (Wasabi)\n17 \/ Any other S3 compatible provider\n \\ (Other)         \nProvider>7 Enter  false  to enter your credentials. Get AWS credentials from the runtime (environment variables or EC2\/ECS meta data if no env vars). \nOnly applies if access_key_id and secret_access_key is blank.Enter a boolean value (true or false). \nPlease Enter for the default (\"false\").Choose a number from below, or type in your own value     \n 1 \/ Enter AWS credentials in the next step     \n  \\ \"false\"  \n   2 \/ Get AWS credentials from the environment (env vars or IAM) \n     \\ \"true\"\n   env_auth>false Enter your access key and secret key . AWS Access Key ID.\nLeave blank for anonymous access or runtime credentials.\nEnter a string value. Press Enter for the default (\"\") \n    access_key_id> <access key>\nAWS Secret Access Key (password)\nLeave blank for anonymous access or runtime credentials.\nEnter a string value. Press Enter for the default (\"\")\n     secret_access_key> <secret key> Leave the region blank. Region to connect to.\nLeave blank if you are using an S3 clone and you don't have a region.\nEnter a string value. Press Enter for the default (\"\")\nChoose a number from below, or type in your own value\n1 \/ Use this if unsure. \n | Will use v4 signatures and an empty region.\n  \\ ()\n2 \/ Use this only if v4 signatures don't work. \n | E.g. pre Jewel\/v10 CEPH.\n  \\ \"other-v2-signature\"\nregion> <> Specify the endpoint for Lyve Cloud. For more information about endpoints, see  S3 API endpointsCurrently, Lyve Cloud supports only region-specific S3 endpoints. To access buckets created in different regions in the S3 client, add an endpoint connection for each of the regions. Endpoint for S3 API.\nRequired when using an S3 clone.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n 1 \/ Seagate Lyve Cloud US East 1 (Virginia)\n \\ (s3.us-east-1.lyvecloud.seagate.com)\n 2 \/ Seagate Lyve Cloud US West 1 (California)\n \\ (s3.us-west-1.lyvecloud.seagate.com)\n 3 \/ Seagate Lyve Cloud AP Southeast 1 (Singapore)\n \\ (s3.ap-southeast-1.lyvecloud.seagate.com)\nendpoint> 1 Press  Enter  to skip the location constraint as there is no location constraint. Location constraint - must be set to match the Region.\nLeave blank if not sure. Used when creating buckets only.\nEnter a string value. \nPress Enter for the default (\"\")location constraint> Choose default ACL (private). Canned ACL used when creating and or storing or copying objects.\nThis ACL is used for creating objects and if bucket_acl isn't set, for creating buckets too.\nNote that this ACL is applied when server-side copying objects as S3\nIt doesn't copy the ACL from the source but rather writes a fresh one.\nEnter a string value. Press Enter for the default (\"\")\nChoose a number from below, or type in your own value   \n1 \/ Owner gets FULL_CONTROL. \n | No one else has access rights (default). \n   \\(private)   \n2 \/ Owner gets FULL_CONTROL. \n | The ALLUsers group gets READ access. \n  \\(public-read)    \n3 \/Owner gets FULL_CONTROL. \n | The ALLUsers group gets READ and WRITE access.   \n[snip]\nacl>1 Select n  to save the default advanced configuration. Edit advanced config? (y\/n)\ny) Yes\nn) No (default)\ny\/n>n Review the displayed configuration and accept to save the  remote  and then quit. The config file should look like this: NAME]\n    type = s3\n    Provider = LyveCloud\n    env_auth = false\n    access_key_id = xxx\n    secret_access_key = yyy\n    region = us-west-1\n    endpoint = s3.us-east-1.lyvecloud.seagate.com\n    acl = private Click y to confirm the configuration. y) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny\/e\/d>y Type q to quit the configuration, else select any of the following to edit, delete, rename, copy, Set configuration password. Current remotes:\n\nName Type\n==== ====\nashrcl s3\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne\/n\/d\/r\/c\/s\/q>q " }, 
{ "title" : "How to configure Rclone ", 
"url" : "using-rclone.html#how-to-configure-rclone", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ How to configure Rclone ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Mounting Lyve Cloud as a drive ", 
"url" : "using-rclone.html#mounting-lyve-cloud-as-a-drive", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Mounting Lyve Cloud as a drive ", 
"snippet" : "Prior to mounting Lyve Cloud as a drive, connect and test the connection by viewing the contents of one or more buckets using the   rclone ls  command. To mount Lyve Cloud as a drive, use this command where: remote is the name of the remote server path\/to\/files is the exact path to the bucket(s) pat...", 
"body" : "Prior to mounting Lyve Cloud as a drive, connect and test the connection by viewing the contents of one or more buckets using the   rclone ls  command. To mount Lyve Cloud as a drive, use this command where: remote is the name of the remote server path\/to\/files is the exact path to the bucket(s) path\/to\/local\/mount  is the local directory: rclone mount remote:path\/to\/files \/path\/to\/local\/mount For more information, see  rclone’s mount command documentation . " }, 
{ "title" : "Managing data ", 
"url" : "using-rclone.html#managing-data-38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Mounting Lyve Cloud as a drive \/ Managing data ", 
"snippet" : "Here are several of the more commonly-needed commands for viewing and managing your data from the command line. See the rclone docs , including information on  global flags , for additional information....", 
"body" : "Here are several of the more commonly-needed commands for viewing and managing your data from the command line. See the rclone docs , including information on  global flags , for additional information. " }, 
{ "title" : "Viewing information about your buckets and directories ", 
"url" : "using-rclone.html#viewing-information-about-your-buckets-anddirectories", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Mounting Lyve Cloud as a drive \/ Viewing information about your buckets and directories ", 
"snippet" : "There are three list commands with easily readable output available:  ls, lsd, and  lsl . To list all data in a certain bucket, where  remote  is the name of the remote and  path  is the name of the bucket: rclone ls remote:path [flags]  To list the directories in a certain remote and see the total ...", 
"body" : "There are three list commands with easily readable output available:  ls, lsd, and  lsl . To list all data in a certain bucket, where  remote  is the name of the remote and  path  is the name of the bucket: rclone ls remote:path [flags]  To list the directories in a certain remote and see the total directory size, modification time, and number of objects in the directories rclone lsd remote:path [flags]  Or rclone lsd remote: [flags] To list all objects in a certain remote and see modification time, size and path where path is the remote path beginning with the bucket name. Any of the filtering options can be applied to this command. rclone lsl remote:path [flags]  Learn more about  ls , lsd , and lsl . " }, 
{ "title" : "How to use Rclone List commands? ", 
"url" : "using-rclone.html#how-to-use-rclone-list-commands-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Mounting Lyve Cloud as a drive \/ Viewing information about your buckets and directories \/ How to use Rclone List commands? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Uploading data to a bucket ", 
"url" : "using-rclone.html#uploading-data-to-a-bucket-38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket ", 
"snippet" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client...", 
"body" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK . To upload data into a bucket, use either of these commands: rclone copy C:\/path\/to\/filename remote:path [flags] \nrclone copy filename remote:path  You can also use  rclone copy  to copy a file or directory to a new location and rename the directory at the same time. Neither of these commands deletes the file from the source, and neither of these commands will copy unchanged files. Learn more about  copy  and  copyto . For more information on RClone, see https:\/\/rclone.org\/s3\/ " }, 
{ "title" : "How to use Rclone Copy-to and Copy-sync commands? ", 
"url" : "using-rclone.html#how-to-use-rclone-copy-to-and-copy-sync-commands-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ How to use Rclone Copy-to and Copy-sync commands? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Downloading data to local storage ", 
"url" : "using-rclone.html#downloadingdatato-local-storage", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Downloading data to local storage ", 
"snippet" : "This is the same as copying, but the source path is something in the remote or in a bucket, while the destination path is on your local storage. rclone copy remote:path C:\/path\/to\/filename [flags] ...", 
"body" : "This is the same as copying, but the source path is something in the remote or in a bucket, while the destination path is on your local storage. rclone copy remote:path C:\/path\/to\/filename [flags]  " }, 
{ "title" : "Delete bucket data ", 
"url" : "using-rclone.html#delete-bucket-data", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data ", 
"snippet" : "To delete files in a certain path from a certain bucket: rclone delete remote:path [flags] You can use flags to delete only files with certain characteristics: For example, to delete files that are over 100MB: rclone --min-size 100MB delete remote:path  To delete only a specific file: rclone deletef...", 
"body" : "To delete files in a certain path from a certain bucket: rclone delete remote:path [flags] You can use flags to delete only files with certain characteristics: For example, to delete files that are over 100MB: rclone --min-size 100MB delete remote:path  To delete only a specific file: rclone deletefile remote:path [flags] Learn more about rclone  delete  and  deletefile . " }, 
{ "title" : "Creating a new folder ", 
"url" : "using-rclone.html#creating-a-new-folder-38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data \/ Creating a new folder ", 
"snippet" : "To create a new folder For example, to create a file named blue in the current location: rclone mkdir blue Create folders in other paths, or with other permissions, by setting the proper  flags . Learn more about  mkdir . Delete a remote from Lyve Cloud To delete a remote from Lyve Cloud, delete the...", 
"body" : "To create a new folder For example, to create a file named blue in the current location: rclone mkdir blue Create folders in other paths, or with other permissions, by setting the proper  flags . Learn more about  mkdir . Delete a remote from Lyve Cloud To delete a remote from Lyve Cloud, delete the remote’s name using this command, changing REMOTE_NAME to the name of the remote to disconnect: rclone config delete REMOTE_NAME [flags] \\ Learn more about  config delete. rclone will also disconnect whenever you shut down your computer. " }, 
{ "title" : "Deleting a folder ", 
"url" : "using-rclone.html#deleting-a-folder-38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data \/ Deleting a folder ", 
"snippet" : "To delete an empty folder. For example, to delete a file named blue in the current location: rclone rmdir blue  Add flags to delete folders in locations other than the current directory. For example, to delete an empty folder named blue that contains other empty folders: rclone rmdirs blue  The fold...", 
"body" : "To delete an empty folder. For example, to delete a file named blue in the current location: rclone rmdir blue  Add flags to delete folders in locations other than the current directory. For example, to delete an empty folder named blue that contains other empty folders: rclone rmdirs blue  The folders must be empty for  rmdir or  rmdirs to work. Learn more about  rmdir and  rmdirs . " }, 
{ "title" : "Copying data ", 
"url" : "using-rclone.html#copying-data-38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data \/ Copying data ", 
"snippet" : "First, connect rclone to Lyve Cloud. For step by step instructions, see,  Configuring rclone to connect to Lyve Cloud . Once the configuration is complete, the rclone.config file must be updated as: [REMOTE NAME]type = s3provider = Otherenv_auth = falseaccess_key_id = XXXXXXXXXXsecret_access_key = Y...", 
"body" : "First, connect rclone to Lyve Cloud. For step by step instructions, see,  Configuring rclone to connect to Lyve Cloud . Once the configuration is complete, the rclone.config file must be updated as: [REMOTE NAME]type = s3provider = Otherenv_auth = falseaccess_key_id = XXXXXXXXXXsecret_access_key = YYYYYYYYYYYYYYYYYYYYYYYYYYYendpoint = https:\/\/s3.us-east-1.lyvecloud.seagate.comacl = privateregion = us-east-1 Start copying data from the existing cloud provider buckets to the buckets created in Lyve Cloud. To copy data $ rclone copy  SOURCE REMOTE:[SOURCE BUCKET] <TARGET REMOTE>[TARGET BUCKET>\/<PREFIX>] To copy all the data, including prefixes, from the source bucket to the target bucket. $ rclone copy  SRT:[ SB ]   TRT:[TB] To copy all objects with a prefix to the target. $ rclone copy  SRT:[ SB ]\/mypath1  TRT:[TB]\/mypath1 " }, 
{ "title" : "Deleting a remote from Lyve Cloud ", 
"url" : "using-rclone.html#deleting-a-remote-from-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data \/ Deleting a remote from Lyve Cloud ", 
"snippet" : "To delete a remote from Lyve Cloud, delete the remote’s name using this command, changing REMOTE_NAME to the name of the remote to disconnect: rclone config delete REMOTE_NAME [flags]  Learn more about  config delete . rclone will also disconnect whenever you shut down your computer....", 
"body" : "To delete a remote from Lyve Cloud, delete the remote’s name using this command, changing REMOTE_NAME to the name of the remote to disconnect: rclone config delete REMOTE_NAME [flags]  Learn more about  config delete . rclone will also disconnect whenever you shut down your computer. " }, 
{ "title" : "Migrating Data ", 
"url" : "using-rclone.html#migrating-data", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ Configuring rclone to connect to Lyve Cloud \/ Uploading data to a bucket \/ Delete bucket data \/ Migrating Data ", 
"snippet" : "To migrate data Set the Source and the Target as remote. For more information, see  Configuring rclone to connect to Lyve Cloud. Sync the Source and Target remote using rclone sync command. rclone sync <source remote name>:path <target remote name>:path Once the source and the target are synced, all...", 
"body" : "To migrate data Set the Source and the Target as remote. For more information, see  Configuring rclone to connect to Lyve Cloud. Sync the Source and Target remote using rclone sync command. rclone sync <source remote name>:path <target remote name>:path Once the source and the target are synced, all the data from the source is copied, removed or migrated to the target remotely. You can use the following flags in the command to check the status of the sync\/copy\/migration. --progress  Displays the real-time transfer progress. --interactive : Enables interactive mode and displays interactive for every action taken. " }, 
{ "title" : "How to use Rclone Delete and Purge commands? ", 
"url" : "using-rclone.html#how-to-use-rclone-delete-and-purge-commands--38449", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using Rclone \/ Connecting to Lyve Cloud from Linux \/ How to use Rclone Delete and Purge commands? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Using AWS CLI ", 
"url" : "using-aws-linux-cli.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI ", 
"snippet" : "The AWS Command Line Interface (AWS CLI) is an open-source tool enabling you to interact with Lyve Cloud S3 buckets. The S3 API provides direct access to Lyve Cloud buckets and you can also develop shell scripts to manage your resources. This section will guide you to acquire, test and configure the...", 
"body" : "The AWS Command Line Interface (AWS CLI) is an open-source tool enabling you to interact with Lyve Cloud S3 buckets. The S3 API provides direct access to Lyve Cloud buckets and you can also develop shell scripts to manage your resources. This section will guide you to acquire, test and configure the AWS CLI tool on Linux for use with Lyve Cloud. Consult your organization's policies and the EULA policies of the software before downloading 3rd party applications. " }, 
{ "title" : "Installing AWS S3 API utility ", 
"url" : "using-aws-linux-cli.html#installing-aws-s3-api-utility", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ Installing AWS S3 API utility ", 
"snippet" : "This section explains installing the S3 API utility. Ensure you have privileges to download and install the software....", 
"body" : "This section explains installing the S3 API utility. Ensure you have privileges to download and install the software. " }, 
{ "title" : "On Linux ", 
"url" : "using-aws-linux-cli.html#on-linux", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ Installing AWS S3 API utility \/ On Linux ", 
"snippet" : "Use the cURL command to download the installation file. For Linux x86 (64-bit) $ curl \"https:\/\/awscli.amazonaws.com\/awscli-exe-linux-x86_64.zip -o awscliv2.zip For Linux ARM $ curl https:\/\/awscli.amazonaws.com\/awscli-exe-linux-aarch64.zip  awscliv2.zip Run the command to unzip the installer. $ unzip...", 
"body" : "Use the cURL command to download the installation file. For Linux x86 (64-bit) $ curl \"https:\/\/awscli.amazonaws.com\/awscli-exe-linux-x86_64.zip -o awscliv2.zip For Linux ARM $ curl https:\/\/awscli.amazonaws.com\/awscli-exe-linux-aarch64.zip  awscliv2.zip Run the command to unzip the installer. $ unzip awscliv2.zip Run the command to install $ sudo .\/aws\/install Run the command to: Locate the AWS binary $ which aws \n$ \/usr\/local\/bin\/aws Verify the AWS CLI version All the S3 API commands will execute for AWS CLI 2.x.x version and above. $ aws --version \n aws-cli\/2.7.3 Python\/3.9.11 \nLinux\/5.15.0-1004-aws exe\/x86_64.ubuntu.22 prompt\/off Save the AWS CLI path in the bash profile file. export PATH=\/usr\/local\/bin:$PATH Reload the updated profile in your current session to apply the changes. $ source ~\/.bash_profile " }, 
{ "title" : "On MAC ", 
"url" : "using-aws-linux-cli.html#on-mac", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ Installing AWS S3 API utility \/ On MAC ", 
"snippet" : "Use the cURL command to download the installation file. $ curl https:\/\/awscli.amazonaws.com\/AWSCLIV2.pkg -o AWSCLIV2.pkg Run the installer and specify the package file as the source. $ sudo installer -pkg .\/AWSCLIV2.pkg -target \/home Run the command to: Locate the AWS binary $ which aws \/usr\/local\/b...", 
"body" : "Use the cURL command to download the installation file. $ curl https:\/\/awscli.amazonaws.com\/AWSCLIV2.pkg -o AWSCLIV2.pkg Run the installer and specify the package file as the source. $ sudo installer -pkg .\/AWSCLIV2.pkg -target \/home Run the command to: Locate the AWS binary $ which aws \n \/usr\/local\/bin\/aws Verify the AWS CLI version All the S3 API commands will execute for AWS CLI 2.x.x version and above. $ aws --version \n This is a sample output to get AWS version. aws-cli\/2.4.5 Python\/3.8.8\nDarwin\/18.7.0 botocore\/2.4.5 Save the AWS CLI path in the bash profile file. export PATH=\/usr\/local\/bin:$PATH Reload the updated profile in your current session to apply the changes. $ source ~\/.bash_profile " }, 
{ "title" : "On Windows ", 
"url" : "using-aws-linux-cli.html#on-windows", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ Installing AWS S3 API utility \/ On Windows ", 
"snippet" : "Download AWS CLI MSI installer. https:\/\/awscli.amazonaws.com\/AWSCLIV2.msi Run the msiexec command to run the MSI installer. C:\\> msiexec.exe \/i https:\/\/awscli.amazonaws.com\/AWSCLIV2.msi Run the command to: Locate the AWS.exe Get-Command aws CommandType Name Version Source ----------- ---- ------- --...", 
"body" : "Download AWS CLI MSI installer. https:\/\/awscli.amazonaws.com\/AWSCLIV2.msi Run the msiexec command to run the MSI installer. C:\\> msiexec.exe \/i https:\/\/awscli.amazonaws.com\/AWSCLIV2.msi\n Run the command to: Locate the AWS.exe Get-Command aws CommandType Name Version Source\n----------- ---- ------- ------\nApplication aws.exe 0.0.0.0 C:\\Program Files\\Amazon\\AWSCLIV2\\aws.exe Verify the AWS CLI version All the S3 API commands will execute for AWS CLI 2.x.x version and above. C:\\> aws --version\n aws-cli\/2.4.5 Python\/3.8.8 Windows\/10 exe\/AMD64 prompt\/off " }, 
{ "title" : "Configuring S3 API ", 
"url" : "using-aws-linux-cli.html#configuring-s3-api", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ Configuring S3 API ", 
"snippet" : "Prerequisites Create a service account to generate the Access and Secret key. For more information, see Creating a service accountAccess key Secret key Lyve Cloud S3 API endpoint. For more information, see S3 API endpointsTo configure S3 API: Use the aws configure command to set up the Lyve Cloud ac...", 
"body" : "Prerequisites Create a service account to generate the Access and Secret key. For more information, see Creating a service accountAccess key Secret key Lyve Cloud S3 API endpoint. For more information, see S3 API endpointsTo configure S3 API: Use the aws configure command to set up the Lyve Cloud account. Enter the following: Access key Secret keys Region: Lyve Cloud region to perform S3 operation. For more information, see S3 endpoint . Output format: Output format can be text, json, or table. Example: Create a profile (profile name - adminuser) The following example creates a profile named  adminuser $ aws configure --profile adminuser AWS Access Key ID\n[None]: xxxxxxxxxxxxxxx\n\nAWS Secret Access Key\n[None]: xxxxxxxxxxxxxxxxx\n\nDefault region name\n[None]: us-east-1\n\nDefault output format [None]: text You must specify   –-profile to access Lyve Cloud using S3API. Ensure you create Lyve CLoud's default profile to work with Lyve Cloud. Following successful configuration, you are ready to use AWS CLI to perform file system operations. Since AWS CLI works with an AWS URL by default, users must override the URL for Lyve Cloud. All configuration information is stored in a local file named credentials  in the home directory ~\/.aws\/config and ~\/.aws\/credentials . You can access the features of Amazon S3 using the AWS CLI. The AWS CLI provides two tiers of commands for accessing Amazon S3. AWS S3: These are high-level commands that simplify performing common tasks. AWS S3 API: These expose direct access to all Amazon S3 API operations For more information, see Leveraging the s3 and s3api Commands . " }, 
{ "title" : "AWS S3 ", 
"url" : "using-aws-linux-cli.html#aws-s3", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Using AWS S3 ", 
"url" : "using-aws-linux-cli.html#using-aws-s3", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating a bucket ", 
"url" : "using-aws-linux-cli.html#creating-a-bucket-50432", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Creating a bucket ", 
"snippet" : "Enter the syntax below, to create a bucket: $ aws s3 mb s3:\/\/<NEW BUCKET NAME> --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com Example $ aws s3 mb s3:\/\/testbkt0 --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com...", 
"body" : "Enter the syntax below, to create a bucket: $ aws s3 mb s3:\/\/<NEW BUCKET NAME> --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com Example $ aws s3 mb s3:\/\/testbkt0 --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com " }, 
{ "title" : "Listing Objects ", 
"url" : "using-aws-linux-cli.html#listing-objects-50432", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Listing Objects ", 
"snippet" : "Run a simple Linux ls command with a bucket named  testbkt0 . The resulting ls command is as follows: $ aws s3 ls s3:\/\/testbkt0 --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com The output of the above command is PRE folder1\/ 2021-07-01 21:04:43 411352 myfile.zip 202...", 
"body" : "Run a simple Linux ls command with a bucket named  testbkt0 . The resulting ls command is as follows: $ aws s3 ls s3:\/\/testbkt0 --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\n The output of the above command is PRE folder1\/\n2021-07-01 21:04:43 411352 myfile.zip\n2021-07-01 21:04:26 19 testfile.txt If you have the correct bucket name, access key and secret key, the  ls  command will work as expected and show your top-level bucket listing. To see the entire directory tree use the --recursive  option: $ aws s3 ls s3:\/\/testbkt0 --profile <profile_name> --recursive --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\n2021-06-29 22:27:26 0 folder1\/\n2021-07-01 20:35:26 411352 folder1\/SystemsControllersBrief.zip\n2021-07-01 21:04:43 411352 myfile.zip\n2021-07-01 21:04:26 19 testfile.txt " }, 
{ "title" : "Download a specific file ", 
"url" : "using-aws-linux-cli.html#download-a-specific-file", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Listing Objects \/ Download a specific file ", 
"snippet" : "To download a specifically named file use the “cp” command. Note the destination folder is the current directory in this example, denoted by “.”. $ aws s3 cp s3:\/\/testbkt0\/testfile.txt . --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com download: s3:\/\/testbkt0\/testfi...", 
"body" : "To download a specifically named file use the “cp” command. Note the destination folder is the current directory in this example, denoted by “.”. $ aws s3 cp s3:\/\/testbkt0\/testfile.txt . --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\ndownload: s3:\/\/testbkt0\/testfile.txt to .\/testfile.txt " }, 
{ "title" : "Download entire directories ", 
"url" : "using-aws-linux-cli.html#download-entire-directories", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Listing Objects \/ Download entire directories ", 
"snippet" : "To download entire directories or prefix including sub-directories or sub-prefix and files or object Modified , use the recursive option as an ls command $ aws s3 cp s3:\/\/testbkt0\/ . --recursive --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com download: s3:\/\/testbkt...", 
"body" : "To download entire directories or prefix including sub-directories or sub-prefix and files or object Modified , use the recursive option as an ls command $ aws s3 cp s3:\/\/testbkt0\/ . --recursive --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\ndownload: s3:\/\/testbkt0\/testfile.txt to .\/testfile.txt\ndownload: s3:\/\/testbkt0\/folder1\/SystemsControllersBrief.zip to folder1\/SystemsControllersBrief.zip\ndownload: s3:\/\/testbkt0\/myfile.zip to .\/myfile.zip " }, 
{ "title" : "Find a specific file by name ", 
"url" : "using-aws-linux-cli.html#find-a-specific-file-by-name", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Listing Objects \/ Find a specific file by name ", 
"snippet" : "If the path of the file is known, users can issue an ls or cp command to the file. The following example will download a file using the path to the current working directory. aws cp s3:\/\/testbkt0\/subfolder1\/subfolder2\/find_this_file.txt . --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1....", 
"body" : "If the path of the file is known, users can issue an ls or cp command to the file. The following example will download a file using the path to the current working directory. aws cp s3:\/\/testbkt0\/subfolder1\/subfolder2\/find_this_file.txt . --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com If the path of a file is not known, users can locate the path by using a part of the file’s name. Issue the command  aws ls and then add  grep lt <part of file name> to the end of the output. Make sure to include --recursive  option. The following example illustrates this where the test is part of the file’s name. $ aws s3 ls s3:\/\/testbkt0 --recursive --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com | grep test\n2021-07-01 21:04:26 19 testfile.txt " }, 
{ "title" : "Uploading a file to a bucket ", 
"url" : "using-aws-linux-cli.html#uploading-a-file-to-a-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Listing Objects \/ Uploading a file to a bucket ", 
"snippet" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client...", 
"body" : "The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK . $ aws s3 cp myfile s3:\/\/testbkt0 --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\nupload: .\/myfile to s3:\/\/testbkt0\/myfile " }, 
{ "title" : "Uploading a folder ", 
"url" : "using-aws-linux-cli.html#uploading-a-folder", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Using AWS S3 \/ Uploading a folder ", 
"snippet" : "To upload a folder and all its contents, use the following command, note without the folder name in the destination path only the contents of the source folder will be copied directly to the bucket. $ aws s3 cp localdir s3:\/\/testbkt0\/localdir --recursive --profile <profile_name> --endpoint-url=https...", 
"body" : "To upload a folder and all its contents, use the following command, note without the folder name in the destination path only the contents of the source folder will be copied directly to the bucket. $ aws s3 cp localdir s3:\/\/testbkt0\/localdir --recursive --profile <profile_name> --endpoint-url=https:\/\/s3.useast-1.lyvecloud.seagate.com\nupload: localdir\/local to s3:\/\/testbkt0\/localdir\/local\n$ aws s3 ls s3:\/\/testbkt0\/ --recursive --profile <profile_name> --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com\n2021-06-29 22:27:26 0 folder1\/2021-07-01 20:35:26 411352 folder1\/SystemsControllersBrief.zip\n2021-07-06 16:02:51 20 local2021-07-06 16:05:58 20 localdir\/local \n " }, 
{ "title" : "Copying a folder\/prefix ", 
"url" : "using-aws-linux-cli.html#copying-a-folder-prefix", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 \/ Copying a folder\/prefix ", 
"snippet" : "Complete this command to copy a folder using AWS CLI. $ aws s3 cp folder1 s3:\/\/lyve-bucket\/folder2 --recursive --profile <profile_name> --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com Alternatively, to place parameters  aws --profile <profile_name> --endpoint https:\/\/s3.us-east-1.lyvecloud.sea...", 
"body" : "Complete this command to copy a folder using AWS CLI. $ aws s3 cp folder1 s3:\/\/lyve-bucket\/folder2 --recursive --profile <profile_name> --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com Alternatively, to place parameters  aws --profile <profile_name> --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com s3 --recursive cp folder1 s3:\/\/lyve-bucket\/folder2 Command using an alias The alias we are using is: aws_east='aws --profile <profile_name> --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com s3' Copy folder to a bucket (placement of –recursive option is important) $ aws_east cp folder1\/ s3:\/\/lyve-bucket\/folder --recursive Upload: folder1\/SystemsControllersBrief.zip to s3:\/\/lyve-bucket\/folder\/SystemsControllersBrief.zip Recursive is necessary for copying the folder. Specifying the destination folder name is also necessary (below is an example where the contents of folder1 are copied into the bucket directly because the destination folder is not specified). $ aws_east cp folder1\/ s3:\/\/lyve-bucket\/ --recursive Upload: folder1\/SystemsControllersBrief.zip to s3:\/\/lyve-bucket\/SystemsControllersBrief.zip. Without the  –recursive  option, folder copy does not work. $ aws_east cp folder1\/ s3:\/\/lyve-bucket\/ \n\nupload failed: folder1\/ to s3:\/\/lyve-bucket\/\n Parameter validation failed: Invalid length for parameter Key, value: 0, valid min length: 1 " }, 
{ "title" : "AWS S3 API ", 
"url" : "using-aws-linux-cli.html#aws-s3-api", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API ", 
"snippet" : "This topic describes some of the commands you can use to manage buckets and objects using the AWS S3 API commands....", 
"body" : "This topic describes some of the commands you can use to manage buckets and objects using the AWS S3 API commands. " }, 
{ "title" : "Using S3 API with Lyve Cloud ", 
"url" : "using-aws-linux-cli.html#using-s3-api-with-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud ", 
"snippet" : "This topic describes some of the commands you can use to manage Lyve Cloud S3 buckets and objects using the S3 API commands....", 
"body" : "This topic describes some of the commands you can use to manage Lyve Cloud S3 buckets and objects using the S3 API commands. " }, 
{ "title" : "Creating a bucket ", 
"url" : "using-aws-linux-cli.html#creating-a-bucket-50432-1", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ Creating a bucket ", 
"snippet" : "The create-bucket S3 API creates a bucket. Use the following command to create a bucket: $aws s3api create-bucket -–bucket <bucket_name> –-profile <profile_name> --endpoint <endpoint> Example: The following example creates a bucket named lyve-bucket. $aws s3api create-bucket --bucket lyve-bucket --p...", 
"body" : "The create-bucket S3 API creates a bucket. Use the following command to create a bucket: $aws s3api create-bucket -–bucket <bucket_name> –-profile <profile_name> --endpoint <endpoint>\n Example: The following example creates a bucket named lyve-bucket. $aws s3api create-bucket --bucket lyve-bucket --profile --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com The output displays the endpoint to access the bucket lyve-bucket . {  \n\"Location\":\n\"http:\/\/s3.us-east-1.lyvecloud.seagate.com\/lyve-bucket\"\n} " }, 
{ "title" : "Listing buckets ", 
"url" : "using-aws-linux-cli.html#listing-buckets-50432", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ Listing buckets ", 
"snippet" : "The list-buckts S3API lists the bucket. Use the following command to list the bucket. $aws s3api list-buckets –-profile <profile_name> --endpoint <endpoint> Example: The following example lists all the buckets in Lyve Cloud. $aws s3api list-buckets --profile adminuser --endpoint https:\/\/s3.us-east-1...", 
"body" : "The list-buckts S3API lists the bucket. Use the following command to list the bucket. $aws s3api list-buckets –-profile <profile_name> --endpoint <endpoint> Example: The following example lists all the buckets in Lyve Cloud. $aws s3api list-buckets --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com {  \n\"Buckets\": [\n        {           \n\"Name\": \"lyve-bucket\",           \n\"CreationDate\": \"2022-03-11T13:57:33.598000+00:00\"\n        }\n    ],   \n\"Owner\": {       \n\"DisplayName\": \"\",       \n\"ID\":\n\"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\n    }\n} " }, 
{ "title" : "Listing Objects ", 
"url" : "using-aws-linux-cli.html#listing-objects-50432-1", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ Listing Objects ", 
"snippet" : "The list-objects S3 API lists the objects in a bucket. Use the following command to list objects: $aws s3api list-objects -–bucket <bucket_name> –-profile <profile_name> --endpoint <endpoint> Example: The following example displays the names of objects in the lyve-bucket bucket. $aws s3api list-obje...", 
"body" : "The list-objects S3 API lists the objects in a bucket. Use the following command to list objects: $aws s3api list-objects -–bucket <bucket_name> –-profile <profile_name> --endpoint <endpoint> \n Example: The following example displays the names of objects in the lyve-bucket bucket. $aws s3api list-objects --bucket lyve-bucket --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com {  \n\"Contents\": [\n        {           \n\"Key\": \"testupload1.txt\",           \n\"LastModified\": \"2022-03-11T15:31:52.020000+00:00\",           \n\"ETag\":\n\"\\\"473c9ae7c14380c5d8f80c6d8042ae6c\\\"\",           \n\"Size\": 41475,           \n\"StorageClass\": \"STANDARD\",           \n\"Owner\": {               \n\"DisplayName\": \"\",               \n\"ID\":\n\"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\n            }\n        }\n    ]\n} Alternatively, you can use the following command to list objects, using list objects V2 API. $aws s3api list-objects-v2 --bucket lyve-bucket --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com {   \n\"Contents\": [\n        {           \n\"Key\": \"testupload1.txt\",           \n\"LastModified\": \"2022-03-11T15:31:52.020000+00:00\",           \n\"ETag\":\n\"\\\"473c9ae7c14380c5d8f80c6d8042ae6c\\\"\",\n            \"Size\":\n41475,           \n\"StorageClass\": \"STANDARD\",           \n\"Owner\": {               \n\"DisplayName\": \"\",               \n\"ID\": \"\"\n            }\n        }\n    ]\n} " }, 
{ "title" : "Copying objects ", 
"url" : "using-aws-linux-cli.html#copying-objects", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ Copying objects ", 
"snippet" : "The copy-object S3 API creates a copy of an object that is already stored. Use the following command to copy objects: $aws s3api copy-object –copy-source <source-bucket\/object-name> –key <object-name> –-bucket <Destination-bucket> –-profile <profile_name> --endpoint <endpoint> Example: The following...", 
"body" : "The copy-object S3 API creates a copy of an object that is already stored. Use the following command to copy objects: $aws s3api copy-object –copy-source <source-bucket\/object-name> –key <object-name> –-bucket <Destination-bucket> –-profile <profile_name> --endpoint <endpoint> Example: The following command copies an object from lyve-bucket to lyve-bucket2. $aws s3api copy-object --copy-source lyve-bucket\/testupload1.txt --key testupload1.txt --bucket lyve-bucket2 --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com {  \n\"CopyObjectResult\": {       \n\"ETag\":\n\"\\\"473c9ae7c14380c5d8f80c6d8042ae6c\\\"\",       \n\"LastModified\": \"2022-03-14T08:45:03.129000+00:00\"\n    }\n} " }, 
{ "title" : "Uploading object to a bucket ", 
"url" : "using-aws-linux-cli.html#uploading-object-to-a-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ Uploading object to a bucket ", 
"snippet" : "You must have WRITE permissions for the bucket to add an object to that bucket. The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, usi...", 
"body" : "You must have WRITE permissions for the bucket to add an object to that bucket. The object name can contain any of these special characters like @, #, *, $, %, &amp;amp;, !, ?, , , ;, ’, ”, |, +, =, &amp;lt;, &amp;gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of the S3 client SDK . Use the following command to upload an object: $aws s3api put-object -–bucket <bucket_name> --key <Object_key> --body <Object_data> –-profile <profile_name> --endpoint <endpoint> Example: The following example uploads an object to the bucket. $aws s3api put-object --bucket lyve-bucket --key testupload1.txt --body testupload1.txt --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com The output displays the Etag and Encryption method through which the object is encrypted. {\n    \"ETag\":\n\"\\\"473c9ae7c14380c5d8f80c6d8042ae6c\\\"\", \n\"ServerSideEncryption\": \"AES256\"\n} " }, 
{ "title" : "", 
"url" : "using-aws-linux-cli.html#-50432", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Deleting objects ", 
"url" : "using-aws-linux-cli.html#deleting-objects", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ \/ Deleting objects ", 
"snippet" : "The delete-object S3 API deletes an object from the bucket. Once you delete an object you can no longer retrieve it. Use the following command to delete objects: $aws s3api delete-object --bucket <bucket_name> --key <object-name> –-profile <profile_name> --endpoint <endpoint> Example: The following ...", 
"body" : "The delete-object S3 API deletes an object from the bucket. Once you delete an object you can no longer retrieve it. Use the following command to delete objects: $aws s3api delete-object --bucket <bucket_name> --key <object-name> –-profile <profile_name> --endpoint <endpoint> Example: The following example deletes an object named testupload2.txt from a bucket named lyve-bucket. $aws s3api delete-object --bucket lyve-bucket --key testupload2.txt --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com Use the following command to validate if the object has been deleted: $aws s3api list-objects --bucket lyve-bucket --profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com The output does not show the file testupload2.txt . {\n\"Contents\": [\n        {          \n\"Key\": \"testupload1.txt\",          \n\"LastModified\": \"2022-03-11T15:31:52.020000+00:00\",\n            \"ETag\":\n\"\\\"473c9ae7c14380c5d8f80c6d8042ae6c\\\"\",          \n\"Size\": 41475,          \n\"StorageClass\": \"STANDARD\",          \n\"Owner\": {              \n\"DisplayName\": \"\",              \n\"ID\":\n\"02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4\"\n            }\n        }\n    ]\n} " }, 
{ "title" : "Deleting bucket ", 
"url" : "using-aws-linux-cli.html#deleting-bucket", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Connecting S3 clients \/ Using AWS CLI \/ AWS S3 API \/ Using S3 API with Lyve Cloud \/ \/ Deleting objects \/ Deleting bucket ", 
"snippet" : "The delete-bucket S3 API deletes the S3 bucket. You must delete all the objects in the bucket before deleting a bucket. Once you delete a bucket you cannot retrieve the bucket nor the objects within the bucket. Use the following command to delete a bucket: $aws s3api delete-bucket --bucket <bucket_n...", 
"body" : "The delete-bucket S3 API deletes the S3 bucket. You must delete all the objects in the bucket before deleting a bucket. Once you delete a bucket you cannot retrieve the bucket nor the objects within the bucket. Use the following command to delete a bucket: $aws s3api delete-bucket --bucket <bucket_name> –-profile <profile_name> --endpoint <endpoint> Example: The following example deletes a bucket named my-bucket2. $aws s3api delete-bucket --bucket lyve-bucket2--profile adminuser --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com " }, 
{ "title" : "Frequently Asked Questions ", 
"url" : "frequently-asked-questions.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "General ", 
"url" : "general.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ General ", 
"snippet" : "What is Lyve Cloud? Lyve Cloud is a simple, trusted, and efficient on-demand solution for mass capacity storage. Predictable economics, verifiable trust, and ease of use make Lyve Cloud the right choice for storing your data. How do I sign up for Lyve Cloud? To sign up for Lyve Cloud services, conta...", 
"body" : "What is Lyve Cloud? Lyve Cloud is a simple, trusted, and efficient on-demand solution for mass capacity storage. Predictable economics, verifiable trust, and ease of use make Lyve Cloud the right choice for storing your data. How do I sign up for Lyve Cloud? To sign up for Lyve Cloud services, contact our sales team at  sales.lyvecloud@seagate.com . What are the S3 service URL's for Lyve Cloud‘s regions? The S3 service URL's for Lyve Cloud: Eastern region (Northern Virginia) is https:\/\/s3.us-east-1.lyvecloud.seagate.com Western region (California) is  https:\/\/s3.us-west-1.lyvecloud.seagate.com Asia Pacific region (Singapore) is https:\/\/s3.ap-southeast-1.lyvecloud.seagate.com The management console URL is  https:\/\/<account-id>.console.lyvecloud.seagate.com Is Lyve Cloud S3 compatible? Yes, Lyve cloud is S3-compatible object storage that uses S3 API, and it works with other S3-compatible applications. What kind of workloads or applications are ideal for Lyve Cloud services? Lyve Cloud is designed to handle all of your data storage needs for workloads such as backup, disaster recovery and big data analytics. Can I store my data in Lyve Cloud and leverage resources and applications from other cloud providers? Yes. Lyve Cloud is co-located with Azure, Amazon, and other cloud providers to facilitate high-speed data transfer. Lyve Cloud lets you keep control of your data while leveraging computing and application resources from other cloud providers. How will I be notified of Lyve Cloud service updates? The Lyve Cloud support team sends notifications regarding any system updates. Updates are sent to the email addresses of all registered Lyve Cloud users. Can I change my registered email address? Currently, you cannot change the registered email address from within your Lyve Cloud account. For more information, contact the Lyve Cloud support team at  support.lyvecloud@seagate.com . Can I reset my password? Yes, you can reset your password, select the Forgot Password link on the login screen and follow the instruction. You can also contact your Lyve Cloud admin to reset the password. How does Lyve Cloud assure data is consistent and all applications have a single source of truth? Lyve Cloud uses a robust data consistency model for writing an object to the disk. Therefore, the data read by any application is identical, providing a single source of truth. How does Lyve Cloud manage data integrity and protect against data corruption? Lyve Cloud maintains data integrity as a core mission. At the physical layer, Lyve Cloud addresses data integrity by using Seagate’s enterprise-class hard disks and cloud-scale data durability and self-healing technologies. The erasure coding algorithm ensures 11 x 9’s of data durability, and the self-healing technology can be configured to protect against silent data corruption or bit rot by performing an integrity check of all objects within a bucket at least once a year. Does Lyve Cloud guarantee permanent deletion of data? To permanently delete data, client applications should use S3 API calls to delete all objects and buckets that the application creates. Does Lyve Cloud support multipart uploads? Lyve Cloud supports multipart uploads where the object part or chunk can range from 5 MB to 5 GB, with a maximum number of 10,000 parts per an object. This enables the efficient upload of large files and recovery from transmission errors. What level of uptime does Lyve Cloud support? Lyve Cloud architecture implements redundancy at the hardware and software stack. All critical software components are designed to tolerate multiple failures. Lyve Cloud makes all reasonable efforts to maintain monthly uptime of 99.9% with applicable service credits. Are there a minimum and maximum sizes for stored objects? There is no minimum object size, and the maximum object size for a single PUT operation is 5 TB. Lyve Cloud recommends using multipart uploads for files larger than 100 MB. What kind of data can I store in Lyve Cloud? Lyve Cloud allows you to store any data in any format as long as it complies with Lyve Cloud's Terms and Conditions. What versions of TLS \/ SSL does Lyve Cloud support? Lyve Cloud is compatible with TLS 1.2. Does Lyve Cloud support WORM\/Write-Once-Read-Many for data immutability? Lyve Cloud supports S3 object lock, which includes WORM\/data immutability. This can be enabled at the bucket level. Does Lyve Cloud support management and data access auditing? After enabling the Audit Log feature, Lyve Cloud keeps an audit log of all user access to your Lyve Cloud console and all client application’s  S3 operations. Users can designate a bucket to receive these audit logs from Lyve Cloud. Can I connect Lyve Cloud with third-party tools? Yes. You can connect to Lyve Cloud using either CyberDuck, Rclone, or any S3 browser. Using these tools, create folders in buckets or upload files. Consult your organization's policies and the EULA policies of the software before downloading 3rd party applications. To connect to Lyve Cloud with third-party tools, see  Connecting S3 clients. Which operating systems environments are supported by the Lyve Cloud console? Lyve Cloud console supports Windows, Linux, and Mac operating systems. Lyve Cloud provides high standards of data protection and security. To ensure customer data is not compromised, we may not support clients running operating systems or application versions that are not actively supported by the relevant ISV or an open-source community. " }, 
{ "title" : "User and Roles ", 
"url" : "user-and-roles.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ User and Roles ", 
"snippet" : "Can I switch admin roles in Lyve Cloud? Yes. If you are an administrator and would like to give\/transfer your role to another user, you must first create a new user and assign their role as an administrator. The new administrator must edit your role to Storage Admin or Auditor. For information on ho...", 
"body" : "Can I switch admin roles in Lyve Cloud? Yes. If you are an administrator and would like to give\/transfer your role to another user, you must first create a new user and assign their role as an administrator. The new administrator must edit your role to Storage Admin or Auditor. For information on how to add and edit users, please visit  Managing Users and Roles. " }, 
{ "title" : "Buckets ", 
"url" : "buckets.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Buckets ", 
"snippet" : "Does Lyve Cloud have any object naming limitations? The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause iss...", 
"body" : "Does Lyve Cloud have any object naming limitations? The object name can contain any of these special characters like @, #, *, $, %, &amp;, !, ?, , , ;, ’, ”, |, +, =, &lt;, &gt;, ^, (, ), {, }, [, ] and alphanumeric characters like 0-9, a-z, A-Z . However, using any of these characters can cause issues due to limiting factors of S3 client SDK . Do buckets have a maximum storage limit? No. There is no storage limit for data stored in a single bucket in Lyve Cloud. How do I manage my buckets using API? Lyve Cloud does not recommend managing buckets through API's. We recommend managing buckets directly from the Lyve Cloud console or from your data management platform. How do I utilize multipart upload and identify which parts of data have successfully been uploaded? Lyve Cloud supports multipart uploads of objects up to 5TBs. Using this method, large files are broken into smaller pieces for a more efficient upload. The pieces are then put back together at the end of the process. A multipart upload consists of three steps: Multipart upload initiation : When a multipart upload is initiated, AWS S3 will return an upload ID, which is a unique identifier for the multipart upload. This upload ID is a required field for all upload parts, list parts, complete upload or stop upload commands. Parts upload : In this step, you can specify the part numbers which you would like to upload. A part number uniquely identifies a part and its place in the uploading object. For each part upload, please save the part number and ETag value. These will be needed to complete step 3. Multipart upload completion : In this step, S3 will complete the upload by piecing the parts in order based on the part number. After a successful complete request, the individual parts will no longer exist. For complete instructions, see  Uploading and copying objects using multipart upload . If an object is above 5TB in size, then the multipart upload completion command will not succeed.  If a multi-part upload fails, the upload can resume with the part of the upload which failed. To view which parts of an upload succeeded, use the ListParts command. This will return all uploaded parts with their size and each one's part number. For more information, see:  ListParts . When should I use ListMultipartUploads API? List multipart upload lists the in-progress multipart uploads that are initiated but are not yet completed or aborted. This API allows writing code that will the uploads that are not completed successfully on time. Lyve Cloud is performing this automatically, by cancelling all pending multi parts after 24 hours. How can a customer confirm the encryption status of their objects? Lyve Cloud enforces standard TLS 1.2 with 256-bit advanced encryption standard (AES) Galois\/Counter Mode (GCM)—otherwise known as AES-256-GCM—to establish secure communications to the customer in transit and at rest. As an authenticated encryption algorithm, GCM provides proven security of the symmetric-key cryptographic cipher that has wide adoption for its performance. Seagate storage hardware is validated by Federal Information Processing Standards (FIPS) 140-2\/3, which directly aligns with the Lyve Cloud focus on security and performance. To view the objects encryption status, please follow the steps below. Pre-requisites Download a command-line tool such as AWS CLI Open your command line application (Command Prompt for PC, Terminal for Mac) and use the following command. --profile profile name --endpoint URL s3api head-object--bucket bucket name--key file name  C:\\Users\\515515>aws --profile Kevin --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com s3api head-object --bucket brawleytest --key Goals.docx  Result bytes   20900   application\/vnd.openxmlformats--officedocument.wordprocessingml.document \"34162b3bec92a8334bd9fca388477f85\"      Mon, 06 Dec 2021 21:10:00 GMT   AES256 bytes   20900   application\/vnd.openxmlformats--officedocument.wordprocessingml.document \"34162b3bec92a8334bd9fca388477f85\"      Mon, 06 Dec 2021 21:10:00 GMT   AES256 METADATA        20210828T175628Z        1b6a154b13045741f4b61ab07ed55567754f44aa6796cc250b2a506c6c83a11a \nMETADATA        20210828T175628Z        1b6a154b13045741f4b61ab07ed55567754f44aa6796cc250b2a506c6c83a11a  The encryption is shown here as AES256 which is highlighted in bold. Can I check bucket utilization using commands? The content and data quantity in a bucket can be viewed through the following commands. --profile profile name –endpoint URL s3 ls –summarize –human-readable –recursive s3:\/\/bucket  C:\\Users\\515515>aws –profile sv15 –endpoint https:\/\/s3.us-west-1.lyvecloud.seagate.com s3 ls –summarize –human-readable –recursive s3:\/\/mybuck  Result 2021-07-03 22:06:34         6 Bytes my-test-file.txt \n2021-07-03 22:07:48       12 Bytes my-test-file1.txt \n2021-07-03 22:29:33       11 Bytes my-test-file2.txt \n2021-07-01 00:46:18      531 Bytes service-acounts.txt \nTotal Objects: 4 Total Size: 560 Bytes  This command will list all contents in the bucket. How can I create a bucket with object immutability (lock) enabled with a set duration using S3 API? Create a bucket and enable object immutability using the following command: You can enable object immutability only while creating a bucket. aws s3api create-bucket --bucket &amp;lt;bucket name&amp;gt; --object-lock-enabled-for-bucket --profile &amp;lt;profile name&amp;gt; --endpoint &amp;lt;endpoint&amp;gt; aws --profile va3 --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com s3api create-bucket --bucket mybucket --object-lock-enabled-for-bucket\n Response { \n\"Location\": http:\/\/s3.us-east-1.lyvecloud.seagate.com\/my-bucket-with-object-lock\n}\n Set the duration using the following command: aws s3api --profile &lt;profile name&gt; put-object-lock-configuration --bucket &lt;bucketname&gt; --object-lock-configuration &lt;value&gt; --endpoint &lt;endpoint&gt; aws --profile va3 --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com s3api create-bucket --bucket mybucket --object-lock-enabled-for-bucket Response aws s3api --profile lcprd put-object-lock-configuration --bucket mybucket --object-lock-configuration ObjectLockEnabled=\"Enabled\",Rule={DefaultRetention={Mode=\"COMPLIANCE\",Days=1}} --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com\n\n\n How many buckets can you create in Lyve Cloud? You can create up to 100 buckets per account. If you need additional buckets, you can create a maximum of 1000 buckets. To create additional buckets, Create a support ticket . " }, 
{ "title" : "Service Account ", 
"url" : "service-account.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Service Account ", 
"snippet" : "What if I lose service account credentials? As a reminder, Service accounts allow applications to authenticate and access Lyve Cloud buckets and objects. The access key and secret key (credentials) are generated when you create a service account. This information must be saved at the time of account...", 
"body" : "What if I lose service account credentials? As a reminder, Service accounts allow applications to authenticate and access Lyve Cloud buckets and objects. The access key and secret key (credentials) are generated when you create a service account. This information must be saved at the time of account creation and cannot be recovered afterwards. If necessary you can edit the service account , however, editing does not generate credentials.   If you lose or misplace your service account credentials and believe you need new credentials, you must create a new service account.  You may also delete the old service account. " }, 
{ "title" : "Billing ", 
"url" : "billing.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Billing ", 
"snippet" : "What storage classes does Lyve Cloud offer? Lyve Cloud offers a single storage tier and a single consistent price based on the amount of data stored. How much does it cost to use Lyve Cloud? The Lyve Cloud service is based on a simple pricing model. Lyve Cloud charges a monthly fee based on the aver...", 
"body" : "What storage classes does Lyve Cloud offer? Lyve Cloud offers a single storage tier and a single consistent price based on the amount of data stored. How much does it cost to use Lyve Cloud? The Lyve Cloud service is based on a simple pricing model. Lyve Cloud charges a monthly fee based on the average capacity of data stored during a given month. You are not charged for S3 API calls or data egress. How is the capacity utilization metric calculated? Lyve Cloud measures the total storage consumption four times a day. Storage consumption number recorded is the average of the four instances per day. The monthly average usage is calculated by taking the average of all daily records for a given month. For example, if the daily average for the first 10 days of the month is 10 TB, the average for next 10 days is 20 TB and the average for last 10 days of the month is 30 TB, then the total consumption for the month is 20 TB ((10+20+30)\/3=20). What is the cost impact of using Lyve Cloud’s immutability feature? Once object immutability is switched on, bucket versioning is automatically enabled. Updating a file creates a new version of the objects being stored. This process results in an increase in storage usage and cost. What is the cost impact of using Lyve Cloud’s audit logging features? Enabling audit logging creates log files for buckets or console activity. These log files are stored and treated like all other billable storage. I’m not planning to use the object immutability feature but I may be overwriting a file with the same name multiple times. What is the cost impact of this? Let us consider an example to answer this question. On day 1, you store a file called \"file1.txt\". On day 2, you then overwrite \"file1.txt\" with a new copy of \"file1.txt\" while not changing the file name. The original copy of \"file1.txt\" is, therefore, overwritten and the cost is impacted only by changes in the file size of the latest “file1.txt” that may have occurred. Are there any charges for egress or API requests in Lyve Cloud? There are no egress or API charges in Lyve Cloud. Can I get an extension of a free trial? Yes, please use the extend option on your trial banner or contact your Lyve Cloud sales representative for an extension. I am currently using the free trail, how can I become a paid customer? Please contact your Lyve Cloud sales representative to become a paid customer. How is Lyve Cloud billing information collected, and how do I get billed? Invoices reflect a calculation of the average monthly storage usage for the previous month. Invoices are available on the first day of every month. You then have an agreed time period to complete payment. Where can I get a copy of my invoice? You can view and download a copy of your invoice from the Lyve Cloud console. For more information see  Viewing your invoices. " }, 
{ "title" : "Security ", 
"url" : "security.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Security ", 
"snippet" : "How is my data protected? Lyve Cloud protects customer data in transit by using: Transport layer security (TLS) for data in-flight TLS 1.2 (AES-256-GCM) Encryption for data at rest The data is always encrypted at rest using one of two server-side methods: Encryption with a client-provided key (part ...", 
"body" : "How is my data protected? Lyve Cloud protects customer data in transit by using: Transport layer security (TLS) for data in-flight TLS 1.2 (AES-256-GCM) Encryption for data at rest The data is always encrypted at rest using one of two server-side methods: Encryption with a client-provided key (part of S3 request headers) - SSE-C Encryption with a Key Management System (KMS) - SSE-S3 All data, regardless of whether it is encrypted or not on the client-side (SSE-C), is encrypted using AES 256-bit encryption at rest. The keys are never shared and can be rotated based on the customer’s security policy. Data in flight is encrypted using TLS 1.2, and client applications can only connect using HTTPS protocol. Lyve Cloud follows industry best practices for design and security models. Contact  sales.lyvecloud@seagate.com  for a complete overview of security analysis conducted by a third party. Does Lyve Cloud mine my data? No, Lyve Cloud does not mine any customer data.  All data stored in Lyve Cloud is encrypted. We strongly recommend that customers use client-side encryption for complete data protection. What happens to my data if I am no longer a Lyve Cloud customer? Lyve Cloud does not make any secondary copies of the data. To permanently delete the data, the client application should use the S3 API calls to delete all objects and buckets it created.  Once this is complete, customers can email  support.lyvecloud@seagate.com  to request that their account information be permanently deleted. This ensures that any remaining customer information is removed from the Lyve Cloud cluster. What authentication mechanisms are supported? Access to the Lyve Cloud admin portal is supported by multiple authentication schemes, including: Multi-factor authentication using either SMS OTP (One Time Password) or an authenticator mobile app Federated login using the customer’s IDP login flow How secure are Lyve Cloud datacenters? Lyve Cloud prioritizes a secure and protected infrastructure. A dedicated staff manages and protects each site 24×7, year-round. Each site is equipped with security cameras to monitor inside the data center and the surrounding area. Facilities are unmarked so as to not draw attention from outside. Building access is controlled using biometric measures. Which OTP applications can be used for MFA login? Lyve Cloud supports the use of third-party authenticator apps as verification methods for MFA logins. You can use any authenticator app that generates temporary codes based on the time-based one-time password. There are many free and paid authenticator apps to choose from. Widely-used options include Google Authenticator, Microsoft Authenticator, DUO, Authy, Okta Verify, Auth0 Guardian, OneLogin Protect, and Oracle Authenticator. Can I use CORS with Lyve Cloud? Currently, Lyve Cloud does not support Cross-Origin Resource Sharing (CORS), nor does it support hosting static websites using custom domains or anonymous access to public buckets. How can I change my authentication method? You must contact your administrator to reset MFA for the user. To reset MFA, see Resetting MFA For An Individual IAM User . After resetting MFA, you must again enroll in MFA. For more information, see Enrolling in MFA . Why must I use a mobile phone to set up MFA? Your device is unique to you. This helps to ensure that your account can only be accessed by the person in possession of your phone. Even if someone has your Lyve Cloud credentials, they will not be able to access your Lyve Cloud account without your mobile phone. My mobile device with authenticator app is lost or stolen, what do I do? To change your phone number you must contact the administrator of your account to reset MFA. For more information, see Resetting MFA For An Individual IAM User . After the administrator resets MFA, you must again enroll in MFA on your new device. For more information, see Enrolling in MFA . Can Email be used as the 2nd method of Auth for MFA? No, email is not supported as an MFA method. We only support the authenticator apps and SMS. This is because email credentials can be easily compromised or reset. With a mobile device, it is more difficult to get the SMS code or use the authenticator app to access your account than it is to access an email account. Can organizations use their own SAML with MFA? Yes, organizations can use their own SAML with MFA. Lyve Cloud MFA always applies to password users even if federated login is enabled for the account. I have lost my recovery code, how do I login to the Lyve Cloud console? You must contact your administrator to reset MFA. To reset MFA for the user, see Resetting MFA For An Individual IAM User . After resetting MFA, you must again enroll in MFA. For more information, see Enrolling in MFA . How can I change my Multi-Factor Authentication (MFA) Phone Number? An administrator must reset the MFA for a user to change the associated phone number. To reset MFA for the user, see Resetting MFA For An Individual IAM User . After resetting MFA, you must again enroll for MFA. For more information, see Enrolling in MFA . Can I change my authenticator app? Yes, you can change the authenticator app by installing the preferred authenticator app. You must contact your administrator to reset MFA for the user. To reset MFA, see Resetting MFA for an individual IAM user . After resetting MFA, you must again enroll in MFA. For more information, see Enrolling in MFA . Can I register multiple Lyve Cloud accounts in an authenticator app for MFA? You may use different authenticator apps for different Lyve Cloud accounts. If you are required to use the same authenticator app for multiple Lyve Cloud accounts, refer to the MFA application's help section to learn how to add multiple accounts. Follow the steps based on the desired authenticator app. Refer to few of the commonly used authenticator apps: Google Microsoft Oracle " }, 
{ "title" : "Support ", 
"url" : "support.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Support ", 
"snippet" : "How can I contact Support? There are two ways to contact Lyve Cloud Support. Create a support ticket from the Lyve Cloud console. For more information see,  Creating a ticket. Email  support.lyvecloud@seagate.com What can I expect after a support ticket is raised or an email is sent? Communication O...", 
"body" : "How can I contact Support? There are two ways to contact Lyve Cloud Support. Create a support ticket from the Lyve Cloud console. For more information see,  Creating a ticket. Email  support.lyvecloud@seagate.com What can I expect after a support ticket is raised or an email is sent? Communication Once you submit a ticket, you will get an auto-generated email from  support.lyvecloud@seagate.com  that includes your ticket number.    Your ticket is added to a queue and is processed in the order that it is received. However, support times do vary depending on issue severity.  If you need to contact support for any reason regarding your ticket, you can reply to the auto-generated email, or you can leave a comment in the portal under your ticket details. SLA  We will use reasonable efforts to provide a workaround within 24 hours. Afterwards, we will do a root cause analysis and fix the issue to ensure it does not occur again.   Escalation  Lyve Cloud uses an internal escalation process to ensure tickets are resolved quickly and that they receive the right technical expertise.  Following this process, there may be cases where customers learn their ticket has been escalated to another part of the support organization.   Resolution  When the ticket is resolved, our support team will send out a post-support survey to assess the effectiveness of our support program. Your feedback is highly valuable to us. What details are required when contacting support through email? When emailing support, you must include as much information as possible. Please include the following in your request: Your account ID. It can be found on your Lyve Cloud URL - <account_id>.console.lyvecloud.seagate.com Summary and description of the issue you are experiencing Environment (i.e. us-east-1 (Virginia), us-west-1 (California), ap-southeast-1 (Singapore), etc.) URL Link  ( S3 API endpoints) Attachment with an error screen, if any When can I contact Lyve Cloud Support? Lyve Cloud support teams are staffed around the globe, working 24\/7, 365 days a year. When will the support team contact me after I create a support ticket? A new ticket will trigger an auto-response acknowledgement that you will receive within 30 minutes. Our support team will revert you within 4 hours. Seagate will use reasonable efforts to provide a workaround within 24 hours. Later, we will do a root cause analysis and fix the issue to ensure it does not occur again. Where to find help? Before contacting support, it may be quick and convenient to find the information you need in our documentation. The Lyve Cloud Documentation provides several resources to answer your questions. Resources include our Quick Start Guide, Video tutorials, Connecting to S3 clients, Known Issues, FAQs, and much more. " }, 
{ "title" : "Partner ", 
"url" : "partner.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Partner ", 
"snippet" : "Where can I view my customers’ storage activity? Lyve Cloud partners can view the monthly storage activity of their customers by viewing the Usage Trend dashboard on the Lyve Cloud console. Total number of users, buckets, service accounts, average usage, and support tickets can also be viewed on the...", 
"body" : "Where can I view my customers’ storage activity? Lyve Cloud partners can view the monthly storage activity of their customers by viewing the Usage Trend dashboard on the Lyve Cloud console. Total number of users, buckets, service accounts, average usage, and support tickets can also be viewed on the Customer distributions dashboard on the Lyve Cloud console. Where can I view my bill? Your bill is sent to the email address registered with the partner account. How can I calculate storage cost per customer? Invoices feature monthly storage used by customers to let partners determine to price for each customer. Lyve Cloud Partner Portal provides usage by customers that can be downloaded from the Home page dashboard. This enables partners to determine to price for each of their customers. " }, 
{ "title" : "Sub-accounts ", 
"url" : "sub-accounts.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Frequently Asked Questions \/ Sub-accounts ", 
"snippet" : "What happens to all the sub-accounts in the master account once the master account is disabled? Once a master account is disabled, all associated sub-accounts are disabled. Will billing stop once a sub-account is disabled? Yes, billing is stopped once a sub-account is disabled. For example, If the s...", 
"body" : "What happens to all the sub-accounts in the master account once the master account is disabled? Once a master account is disabled, all associated sub-accounts are disabled. Will billing stop once a sub-account is disabled? Yes, billing is stopped once a sub-account is disabled. For example, If the sub-account is disabled on June 15, the usage calculation for June month is pro-rata. The sub-account is billed from June 1 to June 15. Will all users of a sub-account be disabled when a sub-account is disabled? Yes, all sub-account users are disabled when a sub-account is disabled. Are service accounts disabled after a sub-account is disabled? Yes, all service accounts are disabled when a sub-account is disabled. " }, 
{ "title" : "Release Notes ", 
"url" : "release-notes.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes ", 
"snippet" : "The Lyve Cloud Release Notes provide information about the latest Lyve Cloud feature release. Information that doesn’t apply to the release will be archived....", 
"body" : "The Lyve Cloud Release Notes provide information about the latest Lyve Cloud feature release. Information that doesn’t apply to the release will be archived. " }, 
{ "title" : "Release 5.5 ", 
"url" : "release-5-5.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.5 ", 
"snippet" : "October 12, 2022...", 
"body" : "October 12, 2022 " }, 
{ "title" : "What's new? ", 
"url" : "release-5-5.html#what-s-new--74868", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.5 \/ What's new? ", 
"snippet" : "This section describes new Lyve Cloud features and enhancements in this release....", 
"body" : "This section describes new Lyve Cloud features and enhancements in this release. " }, 
{ "title" : "Features ", 
"url" : "release-5-5.html#features-74868", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.5 \/ What's new? \/ Features ", 
"snippet" : "Lyve Cloud Compute by Zadara allows users to rent virtual servers on which to run their own applications adjacent to Lyve Cloud S3-compatible storage. It provides a web service through which a user can create a virtual machine image and boot a virtual machine. Users can create, launch, and terminate...", 
"body" : "Lyve Cloud Compute by Zadara allows users to rent virtual servers on which to run their own applications adjacent to Lyve Cloud S3-compatible storage. It provides a web service through which a user can create a virtual machine image and boot a virtual machine. Users can create, launch, and terminate instances as needed, paying for active servers. Lyve Cloud Compute also provides control over autoscaling groups, virtual private networks, block storage and other related resources. It can be managed using a reach UI, as well as CLI and EC2-compatible API. For more information, see Lyve Cloud Compute. Lyve Cloud Analytics by Iguazio is a production-centric data analytics solution that accelerates data engineering, data science, and data governance, all while encouraging development and machine learning operations best practices. This architecture is built on a foundation of data storage, computing, monitoring, and security. The platform's tight integration provides users with a unified approach that reduces the technical debt associated with modern analytic and machine learning systems. For more information, see Lyve Cloud Analytics . " }, 
{ "title" : "Release 5.4 ", 
"url" : "release-5-4.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.4 ", 
"snippet" : "August 13, 2022...", 
"body" : "August 13, 2022 " }, 
{ "title" : "What's new? ", 
"url" : "release-5-4.html#what-s-new--55747", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.4 \/ What's new? ", 
"snippet" : "This section describes new Lyve Cloud features and enhancements in this release....", 
"body" : "This section describes new Lyve Cloud features and enhancements in this release. " }, 
{ "title" : "Features ", 
"url" : "release-5-4.html#features-55747", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.4 \/ What's new? \/ Features ", 
"snippet" : "The Sub-accounts feature allows you to maintain a multi-level account structure and replaces the partner portal. The sub-accounts is available for Managed Service Providers (MSP) and Resellers. If you are not an MSP or Reseller and still want to use sub-accounts, contact the Lyve Cloud support team ...", 
"body" : "The Sub-accounts feature allows you to maintain a multi-level account structure and replaces the partner portal. The sub-accounts is available for Managed Service Providers (MSP) and Resellers. If you are not an MSP or Reseller and still want to use sub-accounts, contact the Lyve Cloud support team at support.lyvecloud@seagate.com . Lyve Cloud provisions the master account, and the master account provisions Sub-accounts. These Sub-accounts are managed by and connected to the master account. Each sub-account can provision and manage its storage, users, and account settings. For more information, see Understanding Sub-accounts. " }, 
{ "title" : "Enhancements ", 
"url" : "release-5-4.html#enhancements-55747", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.4 \/ What's new? \/ Enhancements ", 
"snippet" : "The service account expiration enhances the security of the service account. You can now set the expiration duration for a service account. When a service account expires, the secret credentials associated also expire. For more information, see Service Account settings. You can Clone to create a dup...", 
"body" : "The service account expiration enhances the security of the service account. You can now set the expiration duration for a service account. When a service account expires, the secret credentials associated also expire. For more information, see Service Account settings. You can Clone to create a duplicate Service Account, where it creates new access and secret keys. " }, 
{ "title" : "Release 5.3 ", 
"url" : "release-5-3.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.3 ", 
"snippet" : "January 15, 2022...", 
"body" : "January 15, 2022 " }, 
{ "title" : "What's new? ", 
"url" : "release-5-3.html#what-s-new--36882", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.3 \/ What's new? ", 
"snippet" : "This section describes new Lyve Cloud features and enhancements that are available in the latest release....", 
"body" : "This section describes new Lyve Cloud features and enhancements that are available in the latest release. " }, 
{ "title" : "Features ", 
"url" : "release-5-3.html#features-36882", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.3 \/ What's new? \/ Features ", 
"snippet" : "A new  Start Here  panel located on the right side of the Header pane is available for quick access to basic tasks, training videos, and other helpful resources to get started with Lyve Cloud. Global Account Management allows customers to create buckets in two different regions or create service acc...", 
"body" : "A new  Start Here  panel located on the right side of the Header pane is available for quick access to basic tasks, training videos, and other helpful resources to get started with Lyve Cloud. Global Account Management allows customers to create buckets in two different regions or create service accounts to access buckets in different regions. Currently, there are two regions called California (us-west-1) and Virginia (us-east-1). This provides simplified management of multiple regions on Lyve Cloud console and the ability to increase redundancy and availability. For more information see Understanding Global Accounts. We are introducing a Marketplace that provides more information about partner solutions, including Backup and Recovery, Surveillance and Compute that are certified with Lyve Cloud. Billing features: View the estimated bill for next month’s payment cycle. For more information see  Understanding the home page dashboard. View and download average storage use reports from the previous months or years since the beginning of account creation. For more information see  General Reports . See the Billing tab recently added to the main navigation to track and download invoices. " }, 
{ "title" : "Enhancements ", 
"url" : "release-5-3.html#enhancements-36882", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.3 \/ What's new? \/ Enhancements ", 
"snippet" : "Compliance Mode has now been renamed to Object Immutability. For more information, see  Using object immutability . Instant copy functionality of the S3 endpoint from the bucket details page. For more information, see  Supported S3 API Calls . Support for S3 Select capability, which allows the retri...", 
"body" : "Compliance Mode has now been renamed to Object Immutability. For more information, see  Using object immutability . Instant copy functionality of the S3 endpoint from the bucket details page. For more information, see  Supported S3 API Calls . Support for S3 Select capability, which allows the retrieval of a subset of data from an object and can improve data transfer efficiency. " }, 
{ "title" : "Release Features Overview video ", 
"url" : "release-5-3.html#release-features-overview-video", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Release Notes \/ Release 5.3 \/ What's new? \/ Release Features Overview video ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Troubleshooting Guide ", 
"url" : "troubleshooting-guide.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Troubleshooting Guide ", 
"snippet" : "Experiencing issues? Choose from the topics below to follow step-by-step guides to help you get back on track....", 
"body" : "Experiencing issues? Choose from the topics below to follow step-by-step guides to help you get back on track. " }, 
{ "title" : "Login issue ", 
"url" : "login-issue.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Troubleshooting Guide \/ Login issue ", 
"snippet" : "Experiencing issues logging into Lyve Cloud? Are you unable to reach the console login page or having issues loading the page that is preventing you from logging in? Are you having trouble reaching your S3 buckets? Problems accessing the Lyve Cloud login page or your S3 buckets are usually caused by...", 
"body" : "Experiencing issues logging into Lyve Cloud? Are you unable to reach the console login page or having issues loading the page that is preventing you from logging in? Are you having trouble reaching your S3 buckets? Problems accessing the Lyve Cloud login page or your S3 buckets are usually caused by two issues: The Lyve Cloud IP addresses are blocked by your organization’s router or firewall. Certain services used by Lyve Cloud to enable secure login are blocked by your organization’s firewall rules or your router’s access control list. Solution Ensure Lyve Cloud domains are allowed. Make sure the following URLs are added to the allow list of your firewall: Lyve Cloud domains: Lyve Cloud Console URL <account_ID>.console.lyvecloud.seagate.com US-West-1 US-East-1 AP-Southeast-1 s3.us-west-1.lyvecloud.seagate.com s3.us-east-1.lyvecloud.seagate.com s3.ap-southeast-1.lyvecloud.seagate.com <bucket>.s3.us-west-1.lyvecloud.seagate.com <bucket>.s3.us-east-1.lyvecloud.seagate.com <bucket>.s3.ap-southeast-1.lyvecloud.seagate.com Additional content for authentication: Lyve Cloud Console Documentation Portal Lyve Cloud Status cdn.auth0.com Docs.lyvecloud.seagate.com Status.lyvecloud.seagate.com cdn.jsdelivr.net *.cloudfront.net *.cloudfront.net maxcdn.bootstrapcdn.com userfiles-kb.s3.amazonaws.com www.google.com code.jquery.com fonts.gstatic.com www.gstatic.com ajax.googleapis.com fonts.gstatic.com fonts.googleapis.com polyfill.io fonts.gstatic.com Verify Port 443 access to the domains listed in step 1 . Check to verify that Port 443 is not blocked by your router or firewall. If the port is blocked, consult your IT department to enable access. Reload the Lyve Cloud page to verify changes made in steps 1 and 2 are allowing login access. If you continue to experience issues logging into Lyve Cloud after confirming access to the port and domains recommended in this document, please contact your Seagate Customer Success Manager for further assistance. " }, 
{ "title" : "Resources ", 
"url" : "resources.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Solutions ", 
"url" : "solutions.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Partner Solutions ", 
"url" : "partner-solutions.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Commvault ", 
"url" : "partner-solutions.html#commvault", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Commvault ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "partner-solutions.html#-43250", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Commvault \/ ", 
"snippet" : "Lyve Cloud with Commvault Solution Brief Lyve Cloud with Commvault delivers cost-optimized data management solutions that support the frictionless transfer of backup data for storage, fast retrieval, and long-term retention. Commvault software delivers integrated data management features from a sing...", 
"body" : "Lyve Cloud with Commvault Solution Brief Lyve Cloud with Commvault delivers cost-optimized data management solutions that support the frictionless transfer of backup data for storage, fast retrieval, and long-term retention. Commvault software delivers integrated data management features from a single platform, while Lyve Cloud provides highly scalable, reliable, and secure cloud storage. Learn more " }, 
{ "title" : "Veritas ", 
"url" : "partner-solutions.html#veritas", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Veritas ", 
"snippet" : "Lyve Cloud with Veritas Solution Brief The Lyve Cloud with Veritas solution addresses the IT need for data to be secure, portable and cost-optimized. A cost-efficient cloud storage and data management solution, Lyve Cloud with Veritas supports frictionless movement of protected backup data, long-ter...", 
"body" : "Lyve Cloud with Veritas Solution Brief The Lyve Cloud with Veritas solution addresses the IT need for data to be secure, portable and cost-optimized. A cost-efficient cloud storage and data management solution, Lyve Cloud with Veritas supports frictionless movement of protected backup data, long-term retention and on-demand access. Learn more " }, 
{ "title" : "Cohesity ", 
"url" : "partner-solutions.html#cohesity", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Cohesity ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "partner-solutions.html#-43250-1", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Cohesity \/ ", 
"snippet" : "Lyve Cloud and Cohesity Solution Brief The combined Lyve Cloud and Cohesity solution delivers a single, simplified data management platform and economical, accessible, and secure backup operations. The joint solution enables enterprises to build resilience and take control of data. Learn more...", 
"body" : "Lyve Cloud and Cohesity Solution Brief The combined Lyve Cloud and Cohesity solution delivers a single, simplified data management platform and economical, accessible, and secure backup operations. The joint solution enables enterprises to build resilience and take control of data. Learn more " }, 
{ "title" : "Rubrik ", 
"url" : "partner-solutions.html#rubrik", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Rubrik ", 
"snippet" : "Lyve Cloud with Rubrik Solution Brief As enterprises face explosive data growth, success and innovation will rely heavily on data management solutions offering ease, cost, optimization, security, and scalability—from the data center to the cloud. Lyve Cloud with Rubrik is rising to the call, deliver...", 
"body" : "Lyve Cloud with Rubrik Solution Brief As enterprises face explosive data growth, success and innovation will rely heavily on data management solutions offering ease, cost, optimization, security, and scalability—from the data center to the cloud. Lyve Cloud with Rubrik is rising to the call, delivering a solution that enables simplified backup and recovery so that businesses can transform their IT organizations into business enablers and reclaim valuable time for critical projects. Learn more " }, 
{ "title" : "IBM ", 
"url" : "partner-solutions.html#ibm", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ IBM ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "partner-solutions.html#-43250-2", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ IBM \/ ", 
"snippet" : "Lyve Cloud with IBM Solution Brief Seagate Lyve Cloud and IBM Spectrum Protect have partnered to help you go beyond traditional storage, backup, and recovery. The joint solution delivers simplicity, scalability, cost efficiency, data protection, and recovery across multi-cloud environments - all whi...", 
"body" : "Lyve Cloud with IBM Solution Brief Seagate Lyve Cloud and IBM Spectrum Protect have partnered to help you go beyond traditional storage, backup, and recovery. The joint solution delivers simplicity, scalability, cost efficiency, data protection, and recovery across multi-cloud environments - all while helping you leverage secondary data for improved analytics, reporting, and competitive business advantage. Learn more " }, 
{ "title" : "Veeam ", 
"url" : "partner-solutions.html#veeam", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Veeam ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "", 
"url" : "partner-solutions.html#-43250-3", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Veeam \/ ", 
"snippet" : "Lyve Cloud with Veeam Solution Brief The combined solution lets you accelerate your digital transformation and cloud adoption, delivering low-cost, fast, and reliable cloud storage on-demand. The seamless, end-to-end Backup-as-a-service and Disaster-Recovery-as-a-Service scalable solution cuts stora...", 
"body" : "Lyve Cloud with Veeam Solution Brief The combined solution lets you accelerate your digital transformation and cloud adoption, delivering low-cost, fast, and reliable cloud storage on-demand. The seamless, end-to-end Backup-as-a-service and Disaster-Recovery-as-a-Service scalable solution cuts storage costs by 70% and easily adapts to business changes, allowing enterprises to meet recovery objectives and the challenge of storing and protecting all their data. Learn more " }, 
{ "title" : "VXG ", 
"url" : "partner-solutions.html#vxg", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ VXG ", 
"snippet" : "Lyve Cloud with VXG Solution Brief Today’s video surveillance systems are designed to scale for increased camera count, higher retention requirements, and evolving camera and surveillance technologies. Lyve Cloud provides storage designed specifically for the demands of video surveillance with capab...", 
"body" : "Lyve Cloud with VXG Solution Brief Today’s video surveillance systems are designed to scale for increased camera count, higher retention requirements, and evolving camera and surveillance technologies. Lyve Cloud provides storage designed specifically for the demands of video surveillance with capabilities to retain and manage highresolution video, provide high-throughput, and elastic scalability. Lyve Cloud is simple, trusted and efficient - the proven cloud backup system tested and optimized to work flawlessly with video surveillance. Learn more " }, 
{ "title" : "Equinix ", 
"url" : "partner-solutions.html#equinix", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner Solutions \/ Equinix ", 
"snippet" : "Lyve Cloud with Equinix Solution Brief Seagate Lyve Cloud and Equinix Metal is the easy choice for accelerating digital transformation. The solution simplifies the migration of applications and workloads into a hybrid, multicloud implementation with world-class security and provides simple, transpar...", 
"body" : "Lyve Cloud with Equinix Solution Brief Seagate Lyve Cloud and Equinix Metal is the easy choice for accelerating digital transformation. The solution simplifies the migration of applications and workloads into a hybrid, multicloud implementation with world-class security and provides simple, transparent pricing for the best user experience at optimal cost. Learn more " }, 
{ "title" : "Partner deployment solutions ", 
"url" : "partner-deployment-solutions.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions ", 
"snippet" : "This section guides you to deploy Lyve Cloud with various partner solutions....", 
"body" : "This section guides you to deploy Lyve Cloud with various partner solutions. " }, 
{ "title" : "Lyve Cloud with CloudBoost ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-cloudboost", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost ", 
"snippet" : "In this section, you will learn how to configure CloudBoost and Dell Networker to enable backups to Lyve Cloud. Login to the CloudBoost Management Console as an administrator. You can access the CloudBoost appliance using CLI or GUI. On-Prem CloudBoost Management console: HTTPS:\/\/<CLOUDBOOST IP ADDR...", 
"body" : "In this section, you will learn how to configure CloudBoost and Dell Networker to enable backups to Lyve Cloud. Login to the CloudBoost Management Console as an administrator. You can access the CloudBoost appliance using CLI or GUI. On-Prem CloudBoost Management console: HTTPS:\/\/<CLOUDBOOST IP ADDRESS>:7443 CLI: ssh admin@<CLOUDBOOST IP ADDRESS> " }, 
{ "title" : "Configuring CloudBoost client network ", 
"url" : "partner-deployment-solutions.html#configuring-cloudboost-client-network", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Configuring CloudBoost client network ", 
"snippet" : "To configure network settings for the CloudBoost appliance: Connect to the CloudBoost CLI. Authenticate with the default password, which is password . Set the password for the new administrator. Type the status command to see the current network configuration of the appliance. If the IP address is d...", 
"body" : "To configure network settings for the CloudBoost appliance: Connect to the CloudBoost CLI. Authenticate with the default password, which is password . Set the password for the new administrator. Type the status command to see the current network configuration of the appliance. If the IP address is dynamically assigned, then skip to step 8. To statically set the IP address and netmask, type the command: net config interface static IP address netmask address You must type the following command for each network listed in the status command output if you have multiple networks. net config eth0 XX.X.XX.XXX netmask 0.0.0.0 Type the command to add the gateway manually. route add IP address netmask netmask address gw gateway\naddress For example, route add 0.0.0.0 netmask 0.0.0.0 gw xx.x.xx.x You must add multiple routes to the gateways if you have multiple networks. Type the command to set DNS manually. dns set primary <primary IP address> dns set secondary <secondary IP address> dns set tertiary &lt;tertiary IP address&gt; For example, to set DNS manually. \nDNS set primary 10.5.96.91\n\nDNS set secondary 10.5.96.92\n\nDNS set tertiary 10.5.96.93 Type the following command to set the fully qualified domain name fqdn servername.yourcompanydomain Consider the following: Custom FQDN_name names are not supported. The FQDN_name must be lowercase and not include the underscore character (_). Sample command to set the fully qualified domain name. FQDN: cloudboostqa.int.us-west-1-dev.lyvecloud.seagate.com You must set the FQDN to access the On-Prem CloudBoost Management Console. Type the following command to verify the networking setup and check the appliance's status. status After verifying the system's basic networking settings, configure CloudBoost using the On-Prem CloudBoost Management Console. " }, 
{ "title" : "Cloud profiles ", 
"url" : "partner-deployment-solutions.html#cloud-profiles", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles ", 
"snippet" : "Before you configure a CloudBoost appliance in the On-Prem CloudBoost Management Console, create a cloud profile for the storage. For creating a cloud profile, please use a Service Account with All Operations (full access) that enables cloud boost to create bucket tenants. Obtain the necessary crede...", 
"body" : "[video] Before you configure a CloudBoost appliance in the On-Prem CloudBoost Management Console, create a cloud profile for the storage. For creating a cloud profile, please use a Service Account with All Operations (full access) that enables cloud boost to create bucket tenants. Obtain the necessary credentials for the cloud provider. Use a web browser and sign in to the On-Prem CloudBoost Management Console as an administrator. Type the On-Prem CloudBoost Management Console address in the following format: https: \/\/<FQDN of the cloud appliance>:7443\n\n The username to access the console is admin , and use the password you updated during the deployment of CloudBoost. On the left menu, select Cloud Profile . Select Edit to revise the default cloud profile and enter the following: Display Name : View the default display name. This is a non-editable field. Cloud Storage Provider : View the default Cloud Storage Provider. Endpoint : Enter your Lyve Cloud bucket URL. Access Key ID : Enter your Lyve Cloud bucket access key ID. Secret Access Key : Enter your Lyve Cloud bucket secret access key. Select Save to complete. Video: Setting up cloud boost profile " }, 
{ "title" : "Enabling remote client mount ", 
"url" : "partner-deployment-solutions.html#enabling-remote-client-mount", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Enabling remote client mount ", 
"snippet" : "When you enable remote client mounts, you create the password you must use for the task. Optionally, you can share the admin username and new password with anyone who needs to mount with the client remotely. Open a CLI window for the appliance. Type the following command, where the value for the pas...", 
"body" : "When you enable remote client mounts, you create the password you must use for the task. Optionally, you can share the admin username and new password with anyone who needs to mount with the client remotely. Open a CLI window for the appliance. Type the following command, where the value for the password is the password you created. Remote-mount-password enable password Log in using the administrator's credentials. Type the following command, where the value for the password is the password you created. remote-mount-password enable password " }, 
{ "title" : "Understanding Networker ", 
"url" : "partner-deployment-solutions.html#understanding-networker", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker ", 
"snippet" : "This section shows configuration and operations with the following typical architecture: Networker uses Cloud Boost and Lyve Cloud as a destination....", 
"body" : "This section shows configuration and operations with the following typical architecture: Networker uses Cloud Boost and Lyve Cloud as a destination. " }, 
{ "title" : "Logging in Networker ", 
"url" : "partner-deployment-solutions.html#logging-in-networker", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker \/ Logging in Networker ", 
"snippet" : "Launch the NetWorker Runtime Environment to begin the login process. Enter the following and select Connect to continue: NMC: Server IP Hostname : Networker server IP address. Port number : The port number is 9000. Proceed through the Security Warning and run the application. Enter the credentials t...", 
"body" : "Launch the NetWorker Runtime Environment to begin the login process. Enter the following and select Connect to continue: NMC: Server IP Hostname : Networker server IP address. Port number : The port number is 9000. Proceed through the Security Warning and run the application. Enter the credentials to log in. From the Devices tab, right click on Devices , and select New Device Wizard . Select CloudBoost as the device type. View the preconfiguration checklist and select Next . Enter the following configuration options and select Next to proceed: CloudBoost Storage : Choose Use Embedded Storage . CloudBoost Appliance : Select Use an existing CloudBoost Appliance . CloudBoost Credentials : Enter your CloudBoost username and password. Configuration Method : Choose Browse & Select as the recommended method. You should now see your device from the Devices page. Select the new device to view the Device Properties. " }, 
{ "title" : "Configuring Cloud Boost device for Networker ", 
"url" : "partner-deployment-solutions.html#configuring-cloud-boost-device-for-networker", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker \/ Logging in Networker \/ Configuring Cloud Boost device for Networker ", 
"snippet" : "To configure Cloud Boost device for Networker: In the Devices tab, expand your device from the left menu. Right select on Devices and select New Device Wizard . In the Device configuration wizard, Select the Device Type , choose CloudBoost and click Next . View the CloudBoost Pre-configuration Check...", 
"body" : "To configure Cloud Boost device for Networker: In the Devices tab, expand your device from the left menu. Right select on Devices and select New Device Wizard . In the Device configuration wizard, Select the Device Type , choose CloudBoost and click Next . View the CloudBoost Pre-configuration Checklist and select Next . In the CloudBoost Configuration Options section, complete the following: In the CloudBoost Appliance section, select Use an existing CloudBoost Appliance . In the CloudBoost Attributes and enter Hostname (FQDN) as cloudboostqa.int.us-west-1-dev.lyvecloud.seagate.com . In the CloudBoost Credentials section, enter the username and password and select Next . Username:   remotebackup . Password: Set the password by logging into the CloudBoost Appliance and using the remote-mount-password enable password command. In the Browse and Select the CloudBoost Device Path section, select New Folder and then select next. Configure CloudBoost Pool Review Configuration Settings Check Results and select Finish . Once you add the CloudBoost Device, you can view it under Devices -->CloudBoost appliance. " }, 
{ "title" : "Configuring backup ", 
"url" : "partner-deployment-solutions.html#configuring-backup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker \/ Logging in Networker \/ Configuring Cloud Boost device for Networker \/ Configuring backup ", 
"snippet" : "Once you add the device to Networker, configure and run the backup. To configure backup: Login to Networker and Launch Networker Administrator and select the Protection tab. In the left navigation pane, select Groups and right select to create a new group. Complete the following to create Group : Na...", 
"body" : "Once you add the device to Networker, configure and run the backup. To configure backup: Login to Networker and Launch Networker Administrator and select the Protection tab. In the left navigation pane, select Groups and right select to create a new group. Complete the following to create Group : Name : Name of the group. Clients : Select the client from the list. In the left navigation pane, select Policies and right-click to create a new policy. Enter the name of the new policy. Right-click on the new policy, select New and enter the name in the Groups section. Select created the group to Create New Action . In Policy Action Wizard , Select Specify Action Information and enter the following: Name: Specifies the name of the identity. Period: Specifies the duration, For example, Weekly by day. Select Specify the Backup Options in the Data Movement section, specify the Destination Pool , and select Next . In the Backup Action summary , select Configure . " }, 
{ "title" : "Running backup ", 
"url" : "partner-deployment-solutions.html#running-backup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker \/ Logging in Networker \/ Running backup ", 
"snippet" : "To run the backup: In the Protection tab, select Policies . Right select on backup and choose Start to start the backup. Click the Monitoring tab to monitor the backup....", 
"body" : "To run the backup: In the Protection tab, select Policies . Right select on backup and choose Start to start the backup. Click the Monitoring tab to monitor the backup. " }, 
{ "title" : "Restoring backup ", 
"url" : "partner-deployment-solutions.html#restoring-backup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with CloudBoost \/ Cloud profiles \/ Understanding Networker \/ Logging in Networker \/ Restoring backup ", 
"snippet" : "To restore the backup: In the Recover tab, right-click Recover from the menu options. Select New Recover , and in the Recover Configuration wizard, complete the following: Enter the Name (FQDN) of the source host. All other information is retrieved. Choose files to restore. For example, \/home\/user\/s...", 
"body" : "To restore the backup: In the Recover tab, right-click Recover from the menu options. Select New Recover , and in the Recover Configuration wizard, complete the following: Enter the Name (FQDN) of the source host. All other information is retrieved. Choose files to restore. For example, \/home\/user\/source Choose a restore destination. For example, \/home\/user\/restore Review the details and select Run Recovery . You can monitor the progress in the Monitor tab. Once the recovery job completes successfully, verify the restore file(s) in the destination location. " }, 
{ "title" : "Lyve Cloud with Dell Networker Data Domain ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-dell-networker-data-domain", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Dell Networker Data Domain ", 
"snippet" : "This section helps you to integrate Dell NetWorker and Data Domain Cloud Tier with Lyve Cloud. Prerequisites The following are prerequisites to add a cloud bucket to the Data Domain Cloud Tier. For more information on Data Domain prerequisites, please refer to the DDOS Administration Guide. A config...", 
"body" : "This section helps you to integrate Dell NetWorker and Data Domain Cloud Tier with Lyve Cloud. Prerequisites The following are prerequisites to add a cloud bucket to the Data Domain Cloud Tier. For more information on Data Domain prerequisites, please refer to the DDOS Administration Guide. A configured Lyve Cloud storage account. An enabled and configured Cloud Tier. A Lyve Cloud endpoint SSL CA certificate (See Retrieve Lyve Cloud SSL certificate of Lyve Cloud Endpoint for more information) Configuration Overview You can achieve the configuration for Lyve Cloud and Dell Networker in two simple steps. Add Lyve Cloud bucket to Data Domain Cloud Tier. Create a Data Domain Cloud Tier device in NetWorker. " }, 
{ "title" : "Adding Lyve Cloud to Data Domain Cloud Tier ", 
"url" : "partner-deployment-solutions.html#adding-lyve-cloud-to-data-domain-cloud-tier", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Dell Networker Data Domain \/ Adding Lyve Cloud to Data Domain Cloud Tier ", 
"snippet" : "Import Lyve Cloud endpoint CA certificate. Login to the Data Doman System Manager portal and proceed with the following: Select Data Management > File System > Cloud Units . Select Manage Certificates from the toolbar. Select Add from the Manage Certificates for Cloud dialogue box. Choose I want to ...", 
"body" : "Import Lyve Cloud endpoint CA certificate. Login to the Data Doman System Manager portal and proceed with the following: Select Data Management > File System > Cloud Units . Select Manage Certificates from the toolbar. Select Add from the Manage Certificates for Cloud dialogue box. Choose I want to upload the certificate as a .pem file . Browse to locate the certificate file, then select Add . Add a Lyve Cloud bucket. To add a Lyve Cloud bucket to Data Domain Cloud Tier, select Add to enter the Storage region and select Add to complete. The new Cloud Unit should appear. " }, 
{ "title" : "Creating a Data Domain Cloud Tier device in NetWorker ", 
"url" : "partner-deployment-solutions.html#creating-a-data-domain-cloud-tier-device-in-networker", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Dell Networker Data Domain \/ Creating a Data Domain Cloud Tier device in NetWorker ", 
"snippet" : "NetWorker can protect data by generating a copy of data and sending it to a remote destination. The remote copy (archive copy) is kept off-premises using Cloud storage. For quick recovery, you can save the first backup copy on-premises. In this section, you will learn how to create a device located ...", 
"body" : "NetWorker can protect data by generating a copy of data and sending it to a remote destination. The remote copy (archive copy) is kept off-premises using Cloud storage. For quick recovery, you can save the first backup copy on-premises. In this section, you will learn how to create a device located on Lyve Cloud for an archive clone repository. To create a data domain cloud tier device: Open Network Administrator , right-click on Devices and select New Device Wizard . Select DD Cloud Tier as the Device Type. Select Next . Enter Data Domain DDBOOST Username and Password . Select Next . Choose an existing folder or create a new folder. Choose an existing pool or create a new pool. Choose a storage node. Configure the Data Domain management user and choose a cloud unit. In the CA Certificate section, select Pull CA Certificate to get a Data Domain certificate. Select Configure and Finish to create a clone pool. " }, 
{ "title" : "How to login and add a Cloudboost device? ", 
"url" : "partner-deployment-solutions.html#how-to-login-and-add-a-cloudboost-device-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Dell Networker Data Domain \/ Creating a Data Domain Cloud Tier device in NetWorker \/ How to login and add a Cloudboost device? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud with Rubrik ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-rubrik", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Rubrik ", 
"snippet" : "In this section, you will learn how to deploy Lyve Cloud with Rubrik. Prerequisites Before deployment, you will need the following: A Lyve Cloud storage account. Permission with All Operations A Rubrik account. Configuration Overview The configuration for Lyve Cloud with Rubrik is divided into two s...", 
"body" : "In this section, you will learn how to deploy Lyve Cloud with Rubrik. Prerequisites Before deployment, you will need the following: A Lyve Cloud storage account. Permission with All Operations A Rubrik account. Configuration Overview The configuration for Lyve Cloud with Rubrik is divided into two simple tasks. Create a Lyve Cloud Storage Account. For more information, see Provisioning storage buckets. Create a new cloud storage target on Rubrik using information from Lyve Cloud. " }, 
{ "title" : "Creating a new storage target on Rubrik ", 
"url" : "partner-deployment-solutions.html#creating-a-new-storage-target-on-rubrik", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Rubrik \/ Creating a new storage target on Rubrik ", 
"snippet" : "Using the credentials from your Lyve Cloud account, complete the following steps. Select the Rubrik Dashboard gear icon and Archival Locations to begin writing to buckets in Lyve Cloud. Select + to add a new archival location. Enter the following details and select Add . Archive Type : Choose Object...", 
"body" : "Using the credentials from your Lyve Cloud account, complete the following steps. Select the Rubrik Dashboard gear icon and Archival Locations to begin writing to buckets in Lyve Cloud. Select + to add a new archival location. Enter the following details and select Add . Archive Type : Choose Object Store . Object Store Vendor : Choose S3 Compatible . Access Key : Enter your Lyve Cloud Access Key . Secret Key : Enter your Lyve Cloud Secret Key . Host Name : Enter the bucket endpoint URL. Bucket Prefix : Enter your bucket prefix. The Bucket Prefix in Rubrik must match the bucket name in Lyve Cloud. If the bucket does not exist in Lyve Cloud, it will create new bucket(s) with a prefix. These buckets are prefixed with a naming convention as <prefix>-rubrik-<number> . For example, abc-rubrik-0 Number of Buckets : Enter the number of buckets associated with the prefix. Archive Location Name : Enter an Archive Location Name . RSA key : Enter the RSA key . If you do not have an RSA key, you must generate and paste the key. For more information, see Generating RSA key. Select SLA Domain in themenu, to create a new SLA Domain or edit a Rubrik SLA template, set the following and select Next : SLA Domain Name : Select SLA Domain Continuous Data Protection : Leave toggle in defaulted position Advanced Frequencies : Leave toggle in defaulted position Take Snapshots : Set the frequency of snapshots for Hours, Days, and Months. Keep Snapshots : Set duration to keep snapshots for Days and Months. Snapshot Window : Specify the duration to take snapshots from and the first full snapshot Set Archiving and Replication Rules to customize archiving and retention periods on-premises or in the cloud. Specify the following details and select Next : Retention on Brik : Set the on-premises retention period Archiving : Toggle on. Choose the Archival Location Name from step 1 and select Enable Instant Archive . Replication : Leave toggle in the default position Review settings under Review Impact and select Submit when finished. For more information on Rubrik, visit www.rubrik.com " }, 
{ "title" : "Generating RSA key ", 
"url" : "partner-deployment-solutions.html#generating-rsa-key", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Rubrik \/ Creating a new storage target on Rubrik \/ Generating RSA key ", 
"snippet" : "RSA key is used for encryption. The RSA key can be generated on any system with the OpenSSL toolkit (common on Linux and Mac OS). To generate an RSA key: Run the following command on the command line: # sudo openssl genrsa -out rubrik_encryption_key.pem 2048 The command generates the required key an...", 
"body" : "RSA key is used for encryption. The RSA key can be generated on any system with the OpenSSL toolkit (common on Linux and Mac OS). To generate an RSA key: Run the following command on the command line: # sudo openssl genrsa -out rubrik_encryption_key.pem 2048 The command generates the required key and writes it to a file called rubrik_encryption_key.pem . To view the key, execute the following command: # cat rubrik_encryption_key.pem This image shows an example of an RSA key. The key is the entire contents of the file, including the -----BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- markers Copy and paste the content of key file in the RSA key field. " }, 
{ "title" : "Lyve Cloud with IBM Spectrum Protect ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-ibm-spectrum-protect", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with IBM Spectrum Protect ", 
"snippet" : "This section will teach you how to deploy Lyve Cloud as a new cloud tier to an existing IBM Spectrum Protect Server. Prerequisites Before deployment, you will need the following: A Lyve Cloud Storage Account Permission with All Operations An IBM Spectrum Protect Account Follow IBM Spectrum Protect B...", 
"body" : "This section will teach you how to deploy Lyve Cloud as a new cloud tier to an existing IBM Spectrum Protect Server. Prerequisites Before deployment, you will need the following: A Lyve Cloud Storage Account Permission with All Operations An IBM Spectrum Protect Account Follow IBM Spectrum Protect Best Practices for your workload and environment. Configuration Overview The configuration for Lyve Cloud with IBM Spectrum Protect is divided into two tasks: Create a Lyve Cloud Storage Account. For more information, see Provisioning storage buckets. Do not enable object immutability when creating buckets, this feature is not currently supported by IBM Spectrum Protect. Add a cloud container storage pool to IBM Spectrum Protect. For more information, see the Spectrum Protect reference guide, Configuring a Cloud Container Storage Pool . Adding a Cloud Tier on IBM Spectrum Protect Server To add cloud tier on IBM Spectrum Protect: In the IBM Spectrum Protect Operations Center , select Storage Pools from the Storage menu to add a Cloud Container Storage Pool. Select + Storage Pool to start the storage pool wizard, and complete the following steps to create a storage pool. Select General to configure a cloud container storage pool in the Add Storage Pool section, then select Next . Enter a Name for the storage pool and Server as an identifier and select Next . Choose Off-premises cloud to configure a cloud-container storage pool in Lyve Cloud and select Next . Enter the following credentials and select Next . Encryption : Check Enable Cloud type : Choose S3 API Access key ID : Enter your Lyve Cloud access key Secret access key : Enter your Lyve Cloud secret access key Cloud storage class : Choose S3 Standard On the Local Storage page, specify existing file system directories for disk storage and select Add Storage Pool . Review the Storage Pool Configuration . Select Close & View Policies to start using the new data pool. For more information on IBM Spectrum Protect, visit: http:\/\/www.ibm.com\/products\/data-protection-andrecovery " }, 
{ "title" : "Lyve Cloud with Cohesity ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-cohesity", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Cohesity ", 
"snippet" : "In this section, you will learn how to deploy Lyve Cloud, as a new storage target, on the Cohesity Data Platform. Prerequisites A Lyve Cloud Storage Account Permission with All Operations A Cohesity Account...", 
"body" : "In this section, you will learn how to deploy Lyve Cloud, as a new storage target, on the Cohesity Data Platform. Prerequisites A Lyve Cloud Storage Account Permission with All Operations A Cohesity Account " }, 
{ "title" : "Configuration Overview ", 
"url" : "partner-deployment-solutions.html#configuration-overview-53118", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Cohesity \/ Configuration Overview ", 
"snippet" : "The configuration for Lyve Cloud with Cohesity Data Platform is divided into two tasks. Configure Lyve Cloud Storage Account. For more information, see Provisioning Storage Buckets . Create a new cloud storage target on the Cohesity Data Platform (Lyve Cloud service account credentials). For more in...", 
"body" : "The configuration for Lyve Cloud with Cohesity Data Platform is divided into two tasks. Configure Lyve Cloud Storage Account. For more information, see Provisioning Storage Buckets . Create a new cloud storage target on the Cohesity Data Platform (Lyve Cloud service account credentials). For more information, see Creating and registering a new cloud storage target on Cohesity. " }, 
{ "title" : "Creating and registering a new cloud storage target on Cohesity ", 
"url" : "partner-deployment-solutions.html#creating-and-registering-a-new-cloud-storage-target-on-cohesity", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Cohesity \/ Creating and registering a new cloud storage target on Cohesity ", 
"snippet" : "To create and register cloud storage: Create an External Target . Navigate to the Inventory section of the Cohesity Dashboard. Select External Target and select Register External Target . Register an External Target On the New Target page, enter the following, and select Register : Choose Archival :...", 
"body" : "To create and register cloud storage: Create an External Target . Navigate to the Inventory section of the Cohesity Dashboard. Select External Target and select Register External Target . Register an External Target On the New Target page, enter the following, and select Register : Choose Archival : This allows copying of data in Lyve Cloud and is used to import data to other locations for disaster recovery. Type: Select S3 Compatible from the list. Bucket Name : Enter the bucket name. You must add the same bucket created while configuring a Lyve Cloud account as a target bucket. This cannot be modified after you register the Target. Access Key ID : Enter your access key, a private key for authentication to connect a bucket created in Lyve Cloud. The access key is displayed when you create a new service account in Lyve Cloud. A service account contains bucket credentials for Lyve Cloud to access a bucket. Secret Access Key : Enter your secret key, a private key password for authentication to connect a bucket created in Lyve Cloud. Enter your secret key, a private key password for authentication to connect a bucket created in Lyve Cloud. The secret key displays when you create a new service account in Lyve Cloud. Endpoint : Enter S3 endpoint of the Lyve Cloud bucket. Secure Connection (HTTPS) : Toggle secure connection to ON. AWS Signature version : Choose the AWS signing process version for adding authentication information to the requests. ver 2 : Select signature version 2 to authenticate. ver 4 : To use signature version 4, you must contact Cohesity support at support@cohesity.com and mention the ticket number ENG-297700 to add a region. Encryption : Toggle encryption to ON, so that the stored data is sent in an encrypted format. Compression : Toggle Compression to ON, where it determines whether to send data to the External Target in a compressed format. Source Side Deduplication : Toggle to ON, where it determines whether to deduplicate data before sending it to the External Target. Bandwidth Throttling : Toggle to OFF, as it limits the maximum data transfer rate from and to this External Target during a time window. Create Protection Policy Create a Data Protection Policy for clients that need backups. For more information on Cohesity, visit cohesity.com . " }, 
{ "title" : "Lyve Cloud with Commvault ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-commvault-53118", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Commvault ", 
"snippet" : "This guide will teach you how to install and deploy Lyve Cloud with Commvault. Prerequisites A Lyve Cloud Storage Account Create a permission with All Operations Configuration Overview The configuration for Lyve Cloud with the Commvault is as follows: Configure a Lyve Cloud Storage Account. For more...", 
"body" : "This guide will teach you how to install and deploy Lyve Cloud with Commvault. Prerequisites A Lyve Cloud Storage Account Create a permission with All Operations Configuration Overview The configuration for Lyve Cloud with the Commvault is as follows: Configure a Lyve Cloud Storage Account. For more information, see Provisioning storage buckets . Install Commvault from the Commvault Store. Use the Commvault Express 11.24 media kit for installation. Configure Lyve Cloud with Commvault. " }, 
{ "title" : "Installing Commvault ", 
"url" : "partner-deployment-solutions.html#installing-commvault", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Commvault \/ Installing Commvault ", 
"snippet" : "To install Commvault: Select Create a new CommCell , select Next , and choose All in one . Review the Installation Path , Index Cache Path , and CommServe Database Installation Path . If no changes are necessary, select Next . Define the Disaster Recovery Path . Choose Use Local Path , and select th...", 
"body" : "To install Commvault: Select Create a new CommCell , select Next , and choose All in one . Review the Installation Path , Index Cache Path , and CommServe Database Installation Path . If no changes are necessary, select Next . Define the Disaster Recovery Path . Choose Use Local Path , and select the ellipses to choose a local folder to store recovery files. Advance to review the Installation Summary and proceed to continue the installation. Select Finish . " }, 
{ "title" : "Creating a new account ", 
"url" : "partner-deployment-solutions.html#creating-a-new-account", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Commvault \/ Creating a new account ", 
"snippet" : "In the Commvault Command Center , enter your registered email address and new password, and select Create account . Once logged in, you will be directed to the CommCell UI ....", 
"body" : "In the Commvault Command Center , enter your registered email address and new password, and select Create account . Once logged in, you will be directed to the CommCell UI . " }, 
{ "title" : "Configuring Lyve Cloud with Commvault ", 
"url" : "partner-deployment-solutions.html#configuring-lyve-cloud-with-commvault", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Commvault \/ Configuring Lyve Cloud with Commvault ", 
"snippet" : "To configure Lyve Cloud with Commvault In the CommCell UI, open the Control Panel , and select Credentials Manager . Select Add to create a user credential. Enter the following to create Lyve Cloud credentials: Account Type : Select Cloud Account . Cloud vendor type : Select S3 Compatible Storage . ...", 
"body" : "To configure Lyve Cloud with Commvault In the CommCell UI, open the Control Panel , and select Credentials Manager . Select Add to create a user credential. Enter the following to create Lyve Cloud credentials: Account Type : Select Cloud Account . Cloud vendor type : Select S3 Compatible Storage . Credential name : Enter a name for your Lyve Cloud credential. Access Key ID : Enter the Access Key to your Lyve Cloud bucket. The access key is displayed when you create a new service account in Lyve Cloud. A service account contains bucket credentials for Lyve Cloud to access a bucket. Secret Access Key : Enter the Secret Access Key to your Lyve Cloud bucket. Your secret key is a private key password for authentication to connect a bucket created in Lyve Cloud. The secret key displays when you create a new service account in Lyve Cloud. Select OK to proceed. New credentials are displayed on the Manage user credentials page. In the CommCell Browser. Open Storage Resources , right-click on Storage Pools, select Add Storage Pool , select Cloud , and then select Cloud Storage . In the Create New Storage Pool Wizard ., enter a storage pool name and select Next . On the Configure storage dialogue, choose to Create and select the Cloud Storage button to set your cloud storage settings. In the General tab, enter the following details in the Add Cloud Storage page and select OK to complete: Type : Select S3 Compatible Storage from the list. MediaAgent : Choose your MediaAgent. Service Host : Enter the Lyve Cloud bucket endpoint URL. Credential : Choose your newly created Lyve Cloud credential. Bucket : Select Detect to list all accessible buckets and choose your Lyve Cloud bucket. Select Next . In the Enable deduplication on Storage pool dialogue, uncheck Enable Deduplication if you choose to use this feature, and select Next . Unselect Software Encryption and select Next . Select Finish to create the Storage Pool. The new storage pool is listed under Storage Pools of the CommCell Browser. " }, 
{ "title" : "How to install and configure Commvault with Lyve Cloud? ", 
"url" : "partner-deployment-solutions.html#how-to-install-and-configure-commvault-with-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Commvault \/ How to install and configure Commvault with Lyve Cloud? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-veritas-netbackup-media-server-deduplication--msdp-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) ", 
"snippet" : "This section guides you to deploy Lyve Cloud as a new cloud tier to an existing Veritas NetBackup Media Server Deduplication Pool (MSDP) storage server. Prerequisites A Lyve Cloud Storage Account: Permission with All Operations Service Account credential for the storage account. RHEL\/CentOS 7.3+ Med...", 
"body" : "This section guides you to deploy Lyve Cloud as a new cloud tier to an existing Veritas NetBackup Media Server Deduplication Pool (MSDP) storage server. Prerequisites A Lyve Cloud Storage Account: Permission with All Operations Service Account credential for the storage account. RHEL\/CentOS 7.3+ Media server\/ OR NetBackup Appliance. NBU 8.3 (NBU Appliance 3.3.0.1) or higher with MSDP set up. For more information, see How to set up MSDP server . Confirm the MSDP server has 1TB of unused space on the storage pool. Download the latest NetBackup CloudProvider.xml . Configuration Overview The configuration for Lyve Cloud with NetBackup has the following tasks: Configure Lyve Cloud Storage Account. For more information, see Provisioning storage bucketsAdd a cloud tier to NetBackup by completing the following steps: Configuring an MSDP storage serverAdding cloud buckets as volumesCreating storage units" }, 
{ "title" : "Configuring an MSDP storage server ", 
"url" : "partner-deployment-solutions.html#configuring-an-msdp-storage-server", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Configuring an MSDP storage server ", 
"snippet" : "To configure the storage server on Veritas NetBackup: Login to the Veritas NetBackup Java Console application and select Configure Cloud Storage Server . Select Next to create a cloud storage server and disk pool. Select Seagate Lyve Cloud (S3) as your cloud storage provider. Select Next to continue...", 
"body" : "To configure the storage server on Veritas NetBackup: Login to the Veritas NetBackup Java Console application and select Configure Cloud Storage Server . Select Next to create a cloud storage server and disk pool. Select Seagate Lyve Cloud (S3) as your cloud storage provider. Select Next to continue. Enter the field information: Service Host: Enter your Lyve Cloud bucket endpoint URL . Storage server name: Enter the Storage server name . Media server name: Enter your Media server name . Select Add Cloud Storage and provide the following information in the General settings tab. Service host: Enter your Lyve Cloud bucket endpoint URL Service endpoint: Enter your Lyve Cloud bucket endpoint URL (same as Service host) HTTP port: Enter the HTTP port as 80 HTTPS port: Enter the HTTPS port as 443 Storage server name: Enter the storage server name   Endpoint access style: Select as Path Style from the list. Select Region Settings and provide the following information and then select Add : Region name Location constraint (same as Region name) Service host Select Ok to proceed. Select the Media server and enter the Access key ID and Secret access key of your Lyve Cloud bucket. Select Advanced Settings and perform the following and then select Ok to proceed: Check Use SSL , Select Data Transfer , Deselect Check Certificate Revocation , Keep the default Object Size , Compression , and Encryption and select Next to proceed. Select Yes to proceed without compression and encryption settings. Review the storage server summary and select Next . View the status of the server tasks and select Next to continue. If performed correctly, the message will appear as shown below. Select Next to configure your Disk pool. " }, 
{ "title" : "Adding cloud buckets as volumes ", 
"url" : "partner-deployment-solutions.html#adding-cloud-buckets-as-volumes", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Adding cloud buckets as volumes ", 
"snippet" : "To add a cloud bucket: Choose your bucket(s) as your backup destination. Alternatively, you can select Add New Volume to create a new backup destination. Select Next to continue. Provide a disk pool name and leave the remaining fields as the default. Verify the disk pool configuration summary and se...", 
"body" : "To add a cloud bucket: Choose your bucket(s) as your backup destination. Alternatively, you can select Add New Volume to create a new backup destination. Select Next to continue. Provide a disk pool name and leave the remaining fields as the default. Verify the disk pool configuration summary and select Next to continue. You will receive a message confirming the creation of your disk pool. Select Next to continue. " }, 
{ "title" : "Creating storage units ", 
"url" : "partner-deployment-solutions.html#creating-storage-units", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Creating storage units ", 
"snippet" : "To create a storage unit: Create a storage unit for your disk pool and select Next to continue. Enter the name Choose Only use the selected media servers Set the Maximum concurrent jobs and Maximum Fragment size Select Finish to complete the disk pool configuration wizard....", 
"body" : "To create a storage unit: Create a storage unit for your disk pool and select Next to continue. Enter the name Choose Only use the selected media servers Set the Maximum concurrent jobs and Maximum Fragment size Select Finish to complete the disk pool configuration wizard. " }, 
{ "title" : "Creating a policy ", 
"url" : "partner-deployment-solutions.html#creating-a-policy", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Creating a policy ", 
"snippet" : "To create a policy: Open the Policies page, right-click on your media server, and select New Policy . Create a Policy name and select OK . The following window will appear. Choose the Policy storage from the dropdown menu and select OK to proceed to the Schedules section. Select Add to add a schedul...", 
"body" : "To create a policy: Open the Policies page, right-click on your media server, and select New Policy . Create a Policy name and select OK . The following window will appear. Choose the Policy storage from the dropdown menu and select OK to proceed to the Schedules section. Select Add to add a schedule and enter the following fields per your requirements. Select OK to proceed to the Clients section. Select New to create a client. Enter a client name, select the operating system for your backups, and select OK . Once your client is added, the new client is displayed. From the Backup Selections page, select Browse . It lists all folders present on the client. Select the folders to backup from the list and select OK . " }, 
{ "title" : "Running manual backups ", 
"url" : "partner-deployment-solutions.html#running-manual-backups", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Running manual backups ", 
"snippet" : "To run backups manually: Navigate to the Policies page, right-click on the new policy created and select Manual Backup . In our example, we created the policy stg-backup-policy . You will find the backup policy, schedule, and client machine of your manual backup. Select OK to begin the manual backup...", 
"body" : "To run backups manually: Navigate to the Policies page, right-click on the new policy created and select Manual Backup . In our example, we created the policy stg-backup-policy . You will find the backup policy, schedule, and client machine of your manual backup. Select OK to begin the manual backup. " }, 
{ "title" : "Monitoring and viewing Jobs ", 
"url" : "partner-deployment-solutions.html#monitoring-and-viewing-jobs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veritas NetBackup Media Server Deduplication (MSDP) \/ Monitoring and viewing Jobs ", 
"snippet" : "To monitor jobs: Select Activity Monitor to view backup logs by associated Job Id . Double-click on a Job Id to view detailed logs of the backup. Select Detailed Status to view the status logs of the backup. Once the backup is completed, verify that the content has been copied to your bucket. If you...", 
"body" : "To monitor jobs: Select Activity Monitor to view backup logs by associated Job Id . Double-click on a Job Id to view detailed logs of the backup. Select Detailed Status to view the status logs of the backup. Once the backup is completed, verify that the content has been copied to your bucket. If you are experiencing issues when connecting to a region. Delete the existing cloud storage configuration from the Master Servers tab, resolve the problematic area and try again. Alternatively, you may add cloud storage from the media server using the following command: \/usr\/openv\/volmgr\/bin\/tpconfig -add -storage_server storage-west -stype seagate_raw -sts_user_id ACCESS_KEY -password SECRET_KEY For more installation and configuration support with Veritas NetBackup, please visit veritas.com\/support . " }, 
{ "title" : "Lyve Cloud with Veeam Backup and Replication ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-veeam-backup-and-replication", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veeam Backup and Replication ", 
"snippet" : "Veeam Backup and Replication consists of three components: Backup Server (GUI, installed on Windows 64-bit OS) Backup Proxy (installed on Windows or Linux machine) Backup Repository (Any disk-based storage) with copy\/archive to object storage Prerequisites A Lyve Cloud Storage Account Permission wit...", 
"body" : "Veeam Backup and Replication consists of three components: Backup Server (GUI, installed on Windows 64-bit OS) Backup Proxy (installed on Windows or Linux machine) Backup Repository (Any disk-based storage) with copy\/archive to object storage Prerequisites A Lyve Cloud Storage Account Permission with All Operations A Veeam Backup and Replication Account " }, 
{ "title" : "Configuration Overview ", 
"url" : "partner-deployment-solutions.html#configuration-overview-53118-1", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veeam Backup and Replication \/ Configuration Overview ", 
"snippet" : "The configuration for Lyve Cloud with Veeam Backup & Replication is divided into three simple tasks. Create a Lyve Cloud Service Account. Create a bucket with Object Immutability enabled. For correct setup with Veeam, you must configure your storage bucket with object immutability turned on. Turn of...", 
"body" : "The configuration for Lyve Cloud with Veeam Backup & Replication is divided into three simple tasks. Create a Lyve Cloud Service Account. Create a bucket with Object Immutability enabled. For correct setup with Veeam, you must configure your storage bucket with object immutability turned on. Turn off Object Immutability Navigate to your bucket and turn object immutability off. This is required to enable versioning for this bucket, which allows Veeam to make it a configurable option for Backup and Replication. Create bucket permission. For more information, see Creating bucket access permissions. Create a service account. For more information, see Creating a service account. Create a Veeam environment. For more information, see How to Install Backup and Replication . Add a cloud tier to Veeam Backup & Replication. For more information, see How to move data to Veeam Cloud Tier . Add a capacity tier to Veeam Backup & Replication to store and archive backups in the cloud. For more information, see Veeam Scale-out Repository . " }, 
{ "title" : "Deploying Lyve Cloud with Veeam ", 
"url" : "partner-deployment-solutions.html#deploying-lyve-cloud-with-veeam", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with Veeam Backup and Replication \/ Deploying Lyve Cloud with Veeam ", 
"snippet" : "To deploy Lyve Cloud with Veeam: Add a cloud tier on the Veeam Backup server. Launch New Object Repository Wizard. Open the Backup Infrastructure tab, and select Backup Repositories . In the menu, select Add Repository . In the Add Backup Repository dialog, choose Object Storage and select S3 Compat...", 
"body" : "To deploy Lyve Cloud with Veeam: Add a cloud tier on the Veeam Backup server. Launch New Object Repository Wizard. Open the Backup Infrastructure tab, and select Backup Repositories . In the menu, select Add Repository . In the Add Backup Repository dialog, choose Object Storage and select S3 Compatible to add the backup repository. In the New Object Storage Repository dialogue, specify a Name and Description and select Next . In the Accounts section, select the S3 URL of your Lyve Cloud Bucket from the Buckets section of the Lyve Cloud console, and paste it to the Service Point field in Object Repository Wizard. In the New Object Storage Repository , specify the following in the Account section and select Next . Service Point : Enter the S3 Bucket URL. For more information, see Creating buckets. Select the S3 URL of your Lyve Cloud Bucket from the Buckets section of the Lyve Cloud console and paste it. Region : Type in the region specified in the S3 URL. Credentials : Select Add.. and provide the access and secret keys. For more information, see Creating a service account. Gateway Server : Select Use the following gateway server and choose the appropriate server from the list. Select the Bucket menu from the New Object Storage Repository wizard and specify the following: Bucket : Browse the bucket from the desired location. Folder : Browse the folder from the desired location. To prohibit the deletion of data blocks from Lyve Cloud object storage, select the Make recent backups immutable for check box and specify the immutability period. This option can only be used with a Lyve Cloud S3 bucket with versioning enabled. For more information, see Immutability . Verify the details in the Summary menu, and select Finish to complete the object storage repository configuration. Adding capacity tier on Veeam Backup server. Add a new scale-out Backup Repository. In the Backup Infrastructure menu, select Add Scale-out Repository , and then select Scale-out Repositories . In the New Scale-out Backup Repository wizard, select Performance Tier in the menu. Select Backup repositories to include in this scale-out backup repository. In the Placement Policy menu, choose the Placement Policy as Data Locality or Performance . In the Capacity Tier menu, choose the capacity tier. Select Add to specify Lyve Cloud Object Storage to copy backups. Select Summary to review, and select Finish . If you configured your storage bucket with object immutability enabled, you will need to turn it off by editing the bucket once you’ve completed this final step. Make sure versioning is active for continued ransomware protection. " }, 
{ "title" : "Lyve Cloud with LucidLink ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-lucidlink", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with LucidLink ", 
"snippet" : "In this section, you will learn how to configure Lyve Cloud to LucidLink. Prerequisites A Lyve Cloud Storage Account Permission with All Operations Install the LucidLink client application Configuration Overview The Lyve Cloud configuration with LucidLink is divided into three simple tasks: Create a...", 
"body" : "In this section, you will learn how to configure Lyve Cloud to LucidLink. Prerequisites A Lyve Cloud Storage Account Permission with All Operations Install the LucidLink client application Configuration Overview The Lyve Cloud configuration with LucidLink is divided into three simple tasks: Create a Lyve Cloud Service Account. For more information, see Provisioning Storage Buckets . Create a filespace domain in LucidLink. Initialize your filespace to connect with your Lyve Cloud storage account. " }, 
{ "title" : "Creating a LucidLink Filespace Domain to Lyve Cloud ", 
"url" : "partner-deployment-solutions.html#creating-a-lucidlink-filespace-domain-to-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with LucidLink \/ Creating a LucidLink Filespace Domain to Lyve Cloud ", 
"snippet" : "To create a Lucidlink filespace domain: Register and create an account to access LucidLink on LucidLink.com . Login and create your Domain name . The domain is the suffix applied to all your Filespace using the filespace.domain format. Select Create a new Filespace , from the domain dashboard. Choos...", 
"body" : "To create a Lucidlink filespace domain: Register and create an account to access LucidLink on LucidLink.com . Login and create your Domain name . The domain is the suffix applied to all your Filespace using the filespace.domain format. Select Create a new Filespace , from the domain dashboard. Choose Custom Filespace for storage-dependent workloads and select Next . Enter the name of your Filespace and select Next . The name of the Filespace must be unique. Select Other Cloud to connect to Lyve Cloud. Enter the Lyve Cloud Endpoint URL and Region name for the bucket, and then select configure advanced settings and Next to complete. In the Advanced Settings , enter your Bucket name and select the block size to allow uploading data to Lyve Cloud in chunks. We suggest a block size of 8 MB. Select Next . The bucket name must match the bucket created in your Lyve Cloud storage account. Review the details and edit the fields if required, and select Create . Once the Filespace is created, it displays on the LucidLink Dashboard with the option to Initialize the Filespace. From your filespace dashboard, select Initialize to connect the Filespace with your bucket configuration to the local system. " }, 
{ "title" : "Connecting LucidLink to Lyve Cloud ", 
"url" : "partner-deployment-solutions.html#connecting-lucidlink-to-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with LucidLink \/ Connecting LucidLink to Lyve Cloud ", 
"snippet" : "To connect Lucidlink to Lyve Cloud: Download the LucidLink Client Application. For more information, see Prerequisites. Once installed, navigate to the Initialize a filespace menu and select Launch the Lucidlink desktop client . Enter the Lyve Cloud Access key and Secret key in the Add your cloud cr...", 
"body" : "To connect Lucidlink to Lyve Cloud: Download the LucidLink Client Application. For more information, see Prerequisites. Once installed, navigate to the Initialize a filespace menu and select Launch the Lucidlink desktop client . Enter the Lyve Cloud Access key and Secret key in the Add your cloud credentials page. Set root credentials and select Initialize . The root password is required to administer your Filespace and access the encrypted data. Ensure that you save the credentials as the root credentials cannot be retrieved later. Once initialized, the disk space name Lucid(L): is available on your local system. You can upload data of any size into the Lucid(L) disk, allowing the data to upload to the cloud in the block size. The block size is assigned in Advanced settings. " }, 
{ "title" : "Connecting to the filespace from another machine ", 
"url" : "partner-deployment-solutions.html#connecting-to-the-filespace-from-another-machine", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with LucidLink \/ Connecting to the filespace from another machine ", 
"snippet" : "Launch the LucidLink client application and select Connect to another Filespace to connect to the filespace on another machine. If a filespace is already connected to the machine and you want to switch the user or filespace, right click and select Switch user or filespace and select Connect to anoth...", 
"body" : "Launch the LucidLink client application and select Connect to another Filespace to connect to the filespace on another machine. If a filespace is already connected to the machine and you want to switch the user or filespace, right click and select Switch user or filespace and select Connect to another filespace . Enter the filespace name in filespace.domain format. Once completed, follow the steps from Connecting LucidLink to Lyve Cloud to initialize the filespace. Enter the root password set while initializing the filespace. " }, 
{ "title" : "Lyve Cloud with BorgBackup ", 
"url" : "partner-deployment-solutions.html#lyve-cloud-with-borgbackup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup ", 
"snippet" : "This guide will teach you how to deploy and perform data backups on Lyve Cloud with BorgBackup. Prerequisites The following prerequisites are required before installing BorgBackup. A Lyve Cloud storage account Permission with all operations Install BorgBackup using YUM install for RHEL\/CentOS Instal...", 
"body" : "This guide will teach you how to deploy and perform data backups on Lyve Cloud with BorgBackup. Prerequisites The following prerequisites are required before installing BorgBackup. A Lyve Cloud storage account Permission with all operations Install BorgBackup using YUM install for RHEL\/CentOS Install restic on CentOS Configuration Overview The configuration for Lyve Cloud with BorgBackup is divided into three simple tasks. Create a Lyve Cloud Service Account. For more information, see Provisioning Storage Buckets . Installing and initializing BogBackup to Lyve Cloud. Executing backups to Lyve Cloud using Restic. " }, 
{ "title" : "Installing BorgBackup and Restic ", 
"url" : "partner-deployment-solutions.html#installing-borgbackup-and-restic", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Installing BorgBackup and Restic ", 
"snippet" : "The following instructions are tested using BorgBackup version 1.1.18 and CentOS Linux release 7.9.2009 (Core), and Restic 0.13.1 compiled with go1.18 on Linux\/amd64. To install Borgbackup and Restic: Install BorgBackup using YUM install for RHEL\/CentOS on CentOS. Open a shell prompt. Run the comman...", 
"body" : "The following instructions are tested using BorgBackup version 1.1.18 and CentOS Linux release 7.9.2009 (Core), and Restic 0.13.1 compiled with go1.18 on Linux\/amd64. To install Borgbackup and Restic: Install BorgBackup using YUM install for RHEL\/CentOS on CentOS. Open a shell prompt. Run the command to install epel repository. yum install epel-release Run the following command to install BorgBackup. yum install borgbackup Install Restic on CentOS Run the command to install the yum-plugin-copr rpm package. yum install yum-plugin-copr Install restic via copr repository. yum copr enable copart\/restic \n yum install restic " }, 
{ "title" : "Initializing BorgBackup ", 
"url" : "partner-deployment-solutions.html#initializing-borgbackup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Initializing BorgBackup ", 
"snippet" : "To initialize BorgBackup Run the borg init command with your repository file path to initialize the file space. In this example, the file path is \/home\/admin\/borg-repo . borg init --encryption=repokey \/home\/admin\/borg-repo Create a new passphrase to access your repository. To access the repository, ...", 
"body" : "To initialize BorgBackup Run the borg init command with your repository file path to initialize the file space. In this example, the file path is \/home\/admin\/borg-repo . borg init --encryption=repokey \/home\/admin\/borg-repo Create a new passphrase to access your repository. To access the repository, you need to enter the passphrase. Enter your new passphrase. Enter new passphrase: Re-enter your passphrase to confirm. Enter same passphrase again: Type Y to display your passphrase Do you want your passphrase to be displayed for verification? [yN] : y The following is the output, where your passphrase is displayed. Your passphrase (between double quotes) : For example123. Make sure the passphrase displayed above is exactly what you wanted. By default, repositories initialized with this version produces security errors if written with an older version (up to and including Borg 1.0.8). If you want to use these older versions, you can disable the check by running: borg upgrade --disable-tam \/home\/admin\/borg-repo See https:\/\/borgbackup.readthedocs.io\/en\/stable\/changes.html#pre-1-0-9-mmanifest-spoofing-vulnerability for details about the security implications. You will need both KEY AND PASSPHRASE to access the repo. If you used a repo key mode, the key is stored in the repo, but you should back it up separately. Optionally, use “borg key export” to export the key in printable format. Write down the passphrase. Store both at a safe place(s). " }, 
{ "title" : "Using BorgBackup ", 
"url" : "partner-deployment-solutions.html#using-borgbackup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using BorgBackup ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Executing Backups ", 
"url" : "partner-deployment-solutions.html#executing-backups", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using BorgBackup \/ Executing Backups ", 
"snippet" : "Use the following procedure for new and incremental backups. Run the create command to create a backup in the Borg repository. The following example creates a backup to the borg repository called Thursday of files borg-scr and borg-src-2 . borg create –stats \/path\/to\/source-folder::new-borg-repo-nam...", 
"body" : "Use the following procedure for new and incremental backups. Run the create command to create a backup in the Borg repository. The following example creates a backup to the borg repository called Thursday of files borg-scr and borg-src-2 . borg create –stats \/path\/to\/source-folder::new-borg-repo-name path\/to\/borg-repo-destination \n borg create --stats \/home\/admin\/borg-repo::Thursday \/home\/admin\/borg-src \/home\/admin\/borg-src-2  Enter your passphrase to view your repository’s archival history with data including Archive fingerprint, start time, end time, and more. Enter passphrase for key \/home\/admin\/borg-repo:\n------------------------------------------------------------------------\nArchive name: Thursday\n\nTime (start) : Thu, 2022-06-02 06:20:40Time (start): Thu, 2022-06-02 06:20:40\n\nTime (end) : Thu, 2022-06-02 06:20:40\n\nDuration: 0.02 seconds\n\nNumber of files: 2\n\nUtilization of max. archive size: 0% \n\n------------------------------------------------------------------------\n\n Original size Compressed size Deduplicated size \n\nThis archive: 1.42 kB 1.07kB 1.07kB\n\nAll archives: 1.42 kB 1.07kB 1.07 kB \n\n Unique chunks Total chunks \n\nChunk index: 4 4 " }, 
{ "title" : "Listing archives in the repository ", 
"url" : "partner-deployment-solutions.html#listing-archives-in-the-repository", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using BorgBackup \/ Listing archives in the repository ", 
"snippet" : "To list archives in the repository: Use the command borg list and enter your repository file path to list all archives in the repository. borg list \/home\/admin\/borg-repo Enter your passphrase to access your repository.  In this example, we see a list of our single archive titled Thursday with the ar...", 
"body" : "To list archives in the repository: Use the command borg list and enter your repository file path to list all archives in the repository. borg list \/home\/admin\/borg-repo Enter your passphrase to access your repository.  In this example, we see a list of our single archive titled Thursday with the archive date and fingerprint. Enter passphrase for key \/home\/admin\/borg-repo: \n Thursday Thu, 2022-06-02 06:20:40 [Archive fingerprint] To list the contents of an archive, use the command borg list , enter your repository file path, and :Your_archive_name . This example lists the contents of the Thursday archive. borg list \/home\/admin\/borg-repo::Thursday Enter your passphrase to access the repository. Enter passphrase for key \/home\/admin\/borg-repo:\n\ndrwxr-xr-x root root 0 Thu, 2022-06-02 06:08:05 home\/admin\/borg-src \n\n-rw-r—r-- root root 24 Thu, 2022-06-02 06:08:05 home\/admin\/borg-src\/file1.txt \n\ndrwxr-xr-x root root 0 Thu, 2022-06-02 06:08:58 home\/admin\/borg-src-2\n\n-rw-r—r-- root root 25 Thu, 2022-06-02 06:08:58 home\/admin\/borg-scr-2\/file2.txt " }, 
{ "title" : "Initializing Restic ", 
"url" : "partner-deployment-solutions.html#initializing-restic", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Initializing Restic ", 
"snippet" : "The following instructions are intended for Restic and BorgBackup only. These commands have not been tested on similar tools. Open Terminal and enter your Lyve Cloud Access Key and Secret Key ID. For more information, see Creating a service account . export AWS_ACCESS_KEY_ID=<access key>export AWS_A...", 
"body" : "The following instructions are intended for Restic and BorgBackup only. These commands have not been tested on similar tools. Open Terminal and enter your Lyve Cloud Access Key and Secret Key ID. For more information, see Creating a service account . export AWS_ACCESS_KEY_ID=<access key>export AWS_ACCESS_KEY_ID=<access key>\nexport AWS_SECRET_ACCESS_KEY=<Secret key> Initializing the Restic repository. Enter your Lyve Cloud bucket endpoint URL and create a password for your Restic repository. restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt init Create a new password to access your repository. enter password for new repository: Re-enter the password to access your repository. enter the password again:enter password again: Once the password is successfully created, the repository is created. created restic repository fc56337ecc at s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt Please note that knowledge of your password is required to access the repository. Losing your password means that your data is irrecoverably lost. " }, 
{ "title" : "Using Restic with BorgBackup ", 
"url" : "partner-deployment-solutions.html#using-restic-with-borgbackup", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using Restic with BorgBackup ", 
"snippet" : "To use restic with Borgbackup To perform your backup with Restic, enter your bucket endpoint URL followed by the verbose backup command and repository file path, as shown in the example below: restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt --verbose backup \/home\/admin\/borg-r...", 
"body" : "To use restic with Borgbackup To perform your backup with Restic, enter your bucket endpoint URL followed by the verbose backup command and repository file path, as shown in the example below: restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt --verbose backup \/home\/admin\/borg-repo Open and enter your password to access the restic repository. open repository  \n enter password for repository: The output is shown as follows Files: 10 new, 0 changed, 0 unmodified Dirs: 5 new, 0 changes, 0 unmodified Data Blobs: 8 new Tree Blobs: 6 new Added to the repo: 49.873 KiB processed 10 files, 43.127 KiB in 0:02 snapshot ddf98cf6 saved " }, 
{ "title" : "Listing snapshots ", 
"url" : "partner-deployment-solutions.html#listing-snapshots", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using Restic with BorgBackup \/ Listing snapshots ", 
"snippet" : "To list snapshots: To list all snapshots, enter your bucket endpoint URL followed by the snapshots command. restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt snapshots Enter your password. enter password for repository: The output will display as shown. ID     Time             ...", 
"body" : "To list snapshots: To list all snapshots, enter your bucket endpoint URL followed by the snapshots command. restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt snapshots Enter your password. enter password for repository: The output will display as shown. ID     Time             Host                                Tags   Paths             -------------------------------------------------------------------------------------------------------------------------------- ddf98cf6 2022-06-02 08:35:41    admin-centos.symphony.local                      \/home\/admin\/borg-repo         ----------------------------------------------------------------------------------------- -------------------------------- 1 snapshots " }, 
{ "title" : "Restoring backups ", 
"url" : "partner-deployment-solutions.html#restoring-backups", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using Restic with BorgBackup \/ Restoring backups ", 
"snippet" : "To restore backup: Enter your bucket endpoint URL, restore (snapshot ID), and set the target location for your local machine. The following example shows as seen in the example below: restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt restore ddf98cf6 --target .\/borg-restic-rest...", 
"body" : "To restore backup: Enter your bucket endpoint URL, restore (snapshot ID), and set the target location for your local machine. The following example shows as seen in the example below: restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/borg-restic-bkt restore ddf98cf6 --target .\/borg-restic-restore  Enter your password. enter password for repository: The output will display as shown below: repository fc56337e opened successfully, password is correct restoring <Snapshot ddf98cf6 of [\/home\/admin\/borg-repo] at 2022-06-02 08:35:41.456400179 +0000 UTC by root@admin-centos.symphony.local >to .\/borg-restic-restore  Run the list command followed by the restore file path to list the contents of the archive downloaded from Lyve Cloud. borg list \/home\/admin\/borg-restic-restore\/home\/admin\/borg-repo\/ The output will display as shown below: Thursday Thu, 2022-06-02 06:20:40 [Archive fingerprint] Run the extract command followed by the borg repository file path and archival name to restore the borg repository downloaded from the cloud. borg extract \/home\/admin\/borg-restic-restore\/home\/admin\/borg-repo\/::Thursday Enter your password for the borg repository. Enter passphrase for key \/home\/admin\/borg-restic-restore\/home\/admin\/borg-repo: The image displays as: " }, 
{ "title" : "Deleting archives ", 
"url" : "partner-deployment-solutions.html#deleting-archives", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Solutions \/ Partner deployment solutions \/ Lyve Cloud with BorgBackup \/ Using Restic with BorgBackup \/ Deleting archives ", 
"snippet" : "The snapshot in the example above is removed, but the data that was referenced by files in this snapshot is still stored in the repository. To clean up unreferenced data, the prune command must be used. To delete archives: Run the delete command followed by the borg file path and archive name to rec...", 
"body" : "The snapshot in the example above is removed, but the data that was referenced by files in this snapshot is still stored in the repository. To clean up unreferenced data, the prune command must be used. To delete archives: Run the delete command followed by the borg file path and archive name to recover disk space by manually deleting the archive on the local machine. borg delete \/home\/admin\/borg-restic-restore\/home\/admin\/borg-repo\/::Thursday Enter your passphrase to confirm the deletion. Enter passphrase for key \/home\/admin\/borg-restic-restore\/home\/admin\/borg-repo: XXX  To delete a snapshot from the restic repository in the cloud, use the following command and specify the snapshot ID to be removed. restic -r s3.endpoint.URL\/target-bucket forget snapshot ID  In this example, we are deleting snapshot f8a6becd from the restic-new-bkt in Lyve Cloud. restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/restic-new-bkt forget f8a6becd Enter the password for the repository. In the output below, snapshot ID,  f8a6becd, has been deleted from the 99eb2711 repository. enter password for repository:  Repository 99eb2711 opened successfully, password is correct  (0:00) 100.00% 1\/1 files deleted...  Cleaning up unreferenced data The snapshot in the example above is removed, but the data that was referenced by files in this snapshot is still stored in the repository. To clean up unreferenced data, the prune command must be used. Use the following command to clean up snapshots of unreferenced data that is still stored in the repository. restic -r s3.endpoint\/restic-repo-bkt prune restic -r s3:https:\/\/s3.us-west-1.lyvecloud.seagate.com\/restic-new-bkt prune  \n\nenter password for repository: \n\nRepository 99eb2711 opened successfully, password is correct \n\nLoading indexes… \n\nLoading all snapshots… \n\nFinding data that is still in use for 1 snapshots \n\n[0:00] 100.00 % 1 \/ 1 snapshots… \n\n(snip) \n\nDeleting obsolete index files \n\n[0:00] 100.00% 3 \/ 3 files deleted… \n\nRemoving 3 old packs \n\n[0:00] 100.00% 3 \/ 3 files deleted… " }, 
{ "title" : "Data Security Overview ", 
"url" : "data-security-overview.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview ", 
"snippet" : "Seagate® is the industry leader in data-at-rest protection—data security is in our DNA. From consumers walking into a retailer to pick up a backup drive to hyper scale clients purchasing Exos® enterprise-class drives, all our customers trust us with their data. Our proven technology helps to ensure ...", 
"body" : "Seagate® is the industry leader in data-at-rest protection—data security is in our DNA. From consumers walking into a retailer to pick up a backup drive to hyper scale clients purchasing Exos® enterprise-class drives, all our customers trust us with their data. Our proven technology helps to ensure that customers have the highest level of encryption possible, encryption that complies with the strictest government standards. From terabyte-scale drives to the exabyte-scale cloud, Seagate stands for security, delivering on the promise that all customer data will remain customer data. Building on our history and Seagate Secure™ leadership, data security is a core design tenet of the Lyve cloud exabyte-scale storage as a service. This focus on data security starts with the hardware and extends outward to all aspects of the Lyve Cloud service—including infrastructure, software, features, and process—to align with mature industry standards and benchmarks, as well as third-party certification. Seagate is your secure data custodian—a trusted partner to ensure the confidentiality, integrity, and availability of your data. " }, 
{ "title" : "The Importance of Security and Data Privacy ", 
"url" : "data-security-overview.html#the-importance-of-security-and-data-privacy", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ The Importance of Security and Data Privacy ", 
"snippet" : "The security and privacy of enterprise data is a top priority for our customers. That’s because most businesses are subject to industry-specific compliance regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) for healthcare organizations. Similarly, regulations like t...", 
"body" : "The security and privacy of enterprise data is a top priority for our customers. That’s because most businesses are subject to industry-specific compliance regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) for healthcare organizations. Similarly, regulations like those of the European Union’s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) must also be observed. But compliance isn’t the only reason that customers are worried about their data. With cyber-attacks becoming increasingly more common, enterprises are looking for protection from malicious ransomware attacks. They’re also looking for solutions that will protect their data from accidental deletion and manipulation. As the value of data continues to rise, organizations want assurance that their data remains uncorrupted. " }, 
{ "title" : "Secure Service ", 
"url" : "secure-service.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Service ", 
"snippet" : "The security and availability of a service’s infrastructure and services are only as good as the people and processes that manage the infrastructure and software. Comprised of talented industry veterans, Lyve Cloud has a mature information security management system (ISMS) modeled after IS0 27001. R...", 
"body" : "The security and availability of a service’s infrastructure and services are only as good as the people and processes that manage the infrastructure and software. Comprised of talented industry veterans, Lyve Cloud has a mature information security management system (ISMS) modeled after IS0 27001. Rigorous controls, strong processes, and comprehensive policies govern the management of Lyve Cloud, resulting in a highly secure, reliable exabyte storage service clearly aligned with the Trust Services Criteria (TSC) principles—security, availability, process integrity, confidentiality, and privacy.  Lyve Cloud has successfully completed its ISO 27001 and SOC2 certifications. We have a planned roadmap to add additional certifications based on our customer needs. Security is an evolving process. We continue to take steps toward improving the overall system security and delivering on our promise of trust. " }, 
{ "title" : "Secure Design ", 
"url" : "secure-design.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Design ", 
"snippet" : "The Lyve Cloud service runs on a hardened infrastructure that aligns to industry standards such as those set by the National Institute of Standards and Technology (NIST) and the International Organization for Standardization (ISO). During design, the Lyve Cloud team reviewed best practices across le...", 
"body" : "The Lyve Cloud service runs on a hardened infrastructure that aligns to industry standards such as those set by the National Institute of Standards and Technology (NIST) and the International Organization for Standardization (ISO). During design, the Lyve Cloud team reviewed best practices across leading standards and benchmarks to establish best-in-class hardening guidelines for the entire hardware and software stack.   System and infrastructure deployments are handled through automated configuration management tools to ensure continued compliance with desired state and hardening standards. This capability allows for consistent configurations and security while providing the ability to scale the service rapidly.   Architecturally, the Lyve Cloud service was designed with massive-scale multitenancy in mind from the get-go. Stringent network segmentation and service\/process isolation architecture provide multiple layers of security controls. Highly available and resilient infrastructure supports customers’ tenant-isolated components, such as the application programming interface (API) gateway, key management, encryption, and the core object storage.   Beginning with the initial design and throughout the duration of testing and implementation, Lyve Cloud partnered with a leading security consulting group. Extensive review of the design and controls were carried out, which culminated in thorough white-box, black-box, and grey-box penetration testing of the service—leaving no stone unturned. " }, 
{ "title" : "Secure Features ", 
"url" : "secure-features.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Features ", 
"snippet" : "Data security and privacy begins from the moment customers login to the Lyve Cloud portal. This is where users create user accounts and manage their S3 buckets and storage-as-a-service subscription with two-factor identification. When creating an S3 bucket, users can enable object immutability and o...", 
"body" : "Data security and privacy begins from the moment customers login to the Lyve Cloud portal. This is where users create user accounts and manage their S3 buckets and storage-as-a-service subscription with two-factor identification. When creating an S3 bucket, users can enable object immutability and object versioning, which will make objects immutable for a fixed amount of time. To access S3 buckets, customers can create bucket permissions for write- or read-only access. Further, they can create service accounts and select corresponding access permissions. This service account will have its own secret access key, and its credentials will grant access for the application targeting the customer’s S3 bucket. Customers can also turn on audit logs per S3 bucket to keep records of their S3 bucket access and usage. From start to finish, all aspects of the Lyve Cloud portal are user friendly and easily navigable. Customers can rest assured knowing data in flight and at rest is fully encrypted. They can also breathe easy knowing their data integrity is validated to meet compliance and data privacy requirements. Within the Lyve Cloud portal, customers can have clear visibility into Lyve Cloud S3 storage usage. As such, it’s imperative that all Lyve Cloud login, user console access, and service account credentials be stored in a safe and secure location. From the first bits of data transmitted over the wire to the exabytes of data stored on disk, Lyve Cloud’s comprehensive data protection assures the confidentiality and integrity of your data throughout its life cycle. This starts with secure communication through transport layer security (TLS), continues through authentication and integrity validation in the API protocol, as well as robust envelope encryption of the object storage with secure key management, and ends with cryptographically secure erasure processes. In this section, we’ll dive deeper into these and other security features of the Lyve Cloud service. " }, 
{ "title" : "Transport Security ", 
"url" : "secure-features.html#transport-security", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Features \/ Transport Security ", 
"snippet" : "The Lyve Cloud service enforces standard TLS 1.2 with 256-bit advanced encryption standard (AES) Galois\/Counter Mode (GCM)—otherwise known as AES-256-GCM—to establish secure communications to the customer. As an authenticated encryption algorithm, GCM provides proven security of the symmetric-key cr...", 
"body" : "The Lyve Cloud service enforces standard TLS 1.2 with 256-bit advanced encryption standard (AES) Galois\/Counter Mode (GCM)—otherwise known as AES-256-GCM—to establish secure communications to the customer. As an authenticated encryption algorithm, GCM provides proven security of the symmetric-key cryptographic cipher with wide adoption for its performance. " }, 
{ "title" : "Authentication, Authorization, and Data Integrity ", 
"url" : "secure-features.html#authentication--authorization--and-data-integrity", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Features \/ Authentication, Authorization, and Data Integrity ", 
"snippet" : "Authentication, authorization, and data integrity are handled in every transaction with the Lyve Cloud API through the authorization header. The authorization header contains both the account’s access key and a cryptographic signature. By validating the account access key and verifying the signature...", 
"body" : "Authentication, authorization, and data integrity are handled in every transaction with the Lyve Cloud API through the authorization header. The authorization header contains both the account’s access key and a cryptographic signature. By validating the account access key and verifying the signature—which contains a checksum of the data chunk—the Lyve Cloud API can ensure the validity and integrity of the request before processing it further. " }, 
{ "title" : "Envelope Encryption and Key Management ", 
"url" : "secure-features.html#envelope-encryption-and-key-management", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Secure Features \/ Envelope Encryption and Key Management ", 
"snippet" : "A key security feature of Lyve Cloud is that all data is encrypted before it’s stored, regardless of whether it’s encrypted at the source. There is no option to dial back the protection. Two options for server-side encryption are supported: Server-side encryption with client-provided key (SSE-C) Ser...", 
"body" : "A key security feature of Lyve Cloud is that all data is encrypted before it’s stored, regardless of whether it’s encrypted at the source. There is no option to dial back the protection. Two options for server-side encryption are supported: Server-side encryption with client-provided key (SSE-C) Server-side encryption with a key generated by the Lyve Cloud key management system (KMS) (SSE-S3) In both SSE-C and SSE-S3, the key used for object encryption—the object encryption key (OEK)—is uniquely generated using a cryptographically secure pseudo-random number generator (CSPRNG). The OEK is never stored in clear text; rather, it’s stored in encrypted form as part of the object metadata. The OEK is encrypted by the key encrypting key (KEK), which is generated by a key-derivation algorithm using either the client-provided key (SSE-C) or Lyve Cloud KMS key (SSE-S3) and other object-specific metadata. The cryptographic primitive used for all the object encryption operations is AES-256-GCM. " }, 
{ "title" : "Shared Trust Security ", 
"url" : "shared-trust-security.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Data Security Overview \/ Shared Trust Security ", 
"snippet" : "The Lyve Cloud Shared Trust Security model is a series of recommendations based on industry best practices. With integrity and the security of customer data as a top priority, Lyve Cloud is here to support customer success....", 
"body" : "The Lyve Cloud Shared Trust Security model is a series of recommendations based on industry best practices. With integrity and the security of customer data as a top priority, Lyve Cloud is here to support customer success. " }, 
{ "title" : "Cloud data migration ", 
"url" : "cloud-data-migration.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration ", 
"snippet" : "Are you planning to migrate storage or duplicate data from an existing cloud provider to Lyve Cloud?  Data migration or duplication of data to Lyve Cloud provides a trusted solution for disaster recovery, satisfies second or third copy backup needs and of course, Lyve Cloud can be your new location ...", 
"body" : "Are you planning to migrate storage or duplicate data from an existing cloud provider to Lyve Cloud?  Data migration or duplication of data to Lyve Cloud provides a trusted solution for disaster recovery, satisfies second or third copy backup needs and of course, Lyve Cloud can be your new location for primary data storage. Lyve Cloud is a simple and intuitive cloud solution for cost-effective data storage. With Lyve Cloud, there is no limit on the amount of data an organization can migrate and store. " }, 
{ "title" : "Migration preparation ", 
"url" : "migration-preparation.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Migration preparation ", 
"snippet" : "The following can help organizations migrate their data to the cloud efficiently....", 
"body" : "The following can help organizations migrate their data to the cloud efficiently. " }, 
{ "title" : "Consider your Migration Strategy ", 
"url" : "migration-preparation.html#consider-your-migration-strategy", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Migration preparation \/ Consider your Migration Strategy ", 
"snippet" : "You can migrate all storage from an existing cloud provider to Lyve Cloud or you may duplicate your stored data, continually or one-time. Lyve Cloud has evaluated third-party tools supporting these scenarios, as highlighted later in this document.  Lyve Cloud is S3-compatible, which makes re-using a...", 
"body" : "You can migrate all storage from an existing cloud provider to Lyve Cloud or you may duplicate your stored data, continually or one-time. Lyve Cloud has evaluated third-party tools supporting these scenarios, as highlighted later in this document.  Lyve Cloud is S3-compatible, which makes re-using any existing tools or techniques that you have already used in your cloud provider environment easy to implement.     While many proven options are available, the most appropriate choice depends on factors such as: One-time and ongoing costs Performance Meeting technical and business goals " }, 
{ "title" : "Review Data Migration Costs ", 
"url" : "migration-preparation.html#review-data-migration-costs", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Migration preparation \/ Review Data Migration Costs ", 
"snippet" : "The cost of egress fees or transfer fees may vary depending on your current public cloud provider. First-generation cloud providers charge not only for the data you store, but also for downloading or transferring data from their cloud (egress fees). Breaking free of this hidden cost is a common reas...", 
"body" : "The cost of egress fees or transfer fees may vary depending on your current public cloud provider. First-generation cloud providers charge not only for the data you store, but also for downloading or transferring data from their cloud (egress fees). Breaking free of this hidden cost is a common reason many customers choose Lyve Cloud. Here are some things to consider: Estimate the cost to transfer data from your current provider directly to Lyve Cloud. Establish Lyve Cloud as your new target for cloud storage. Re-upload source data to Lyve Cloud if transfer fees from your current provider are too expensive. This approach may require additional time and will be affected by the size of your data sets, and your network bandwidth to Lyve Cloud. However, using this method circumvents paying egress fees. Determine if your current provider limits the amount of data you are allowed to transfer, either per day or per month. Some cloud providers claim not to charge egress fees but will have egress limits. " }, 
{ "title" : "Migration Tool, Types and Cost ", 
"url" : "migration-preparation.html#migration-tool--types-and-cost", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Migration preparation \/ Migration Tool, Types and Cost ", 
"snippet" : "Some migration tools are hosted in the public cloud and will not require installation on to personal computers. All these Cloud Migration tools commonly charge a transfer fee or charge for computing and bandwidth. Note these fees are in addition to egress fees for extracting data from the source clo...", 
"body" : "Some migration tools are hosted in the public cloud and will not require installation on to personal computers. All these Cloud Migration tools commonly charge a transfer fee or charge for computing and bandwidth. Note these fees are in addition to egress fees for extracting data from the source cloud. You may also wish to consider deploying a self-hosted tool in your own environment. Self-hosted tools typically charge a license fee. " }, 
{ "title" : "Select a Migration Tool ", 
"url" : "migration-preparation.html#select-amigration-tool", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Migration preparation \/ Select a Migration Tool ", 
"snippet" : "The following tools have been evaluated for use with Lyve Cloud. Some of these tools are purpose-built for migration and duplication use cases, but some of the listed tools (such as Cloudsfer, Flexify.IO, and Clouchdrop) are generic utilities that can be used for copying scenarios. If you have quest...", 
"body" : "The following tools have been evaluated for use with Lyve Cloud. Some of these tools are purpose-built for migration and duplication use cases, but some of the listed tools (such as Cloudsfer, Flexify.IO, and Clouchdrop) are generic utilities that can be used for copying scenarios. If you have questions about the capabilities or pricing of these tools, please contact the appropriate tool vendor. Cloudsfer (cloud-hosted tool) Flexify.IO (cloud-hosted tool) Movebot (cloud-hosted tool) AWS CLI (self-hosted tool) RClone (self-hosted tool) " }, 
{ "title" : "Best practices ", 
"url" : "best-practices.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Best practices ", 
"snippet" : "Monitor the migration process: You must keep track of the migration process by checking the migration report summary, migration status, or the dashboard once you start the data migration. Monitor Cloud usage: You should have details of the amount of data to be migrated. This cloud usage will help yo...", 
"body" : "Monitor the migration process: You must keep track of the migration process by checking the migration report summary, migration status, or the dashboard once you start the data migration. Monitor Cloud usage: You should have details of the amount of data to be migrated. This cloud usage will help you to calculate the egress charges if any charged by the existing provider. Ensure to optimize the network: Optimize your network before you start the data migration. You can use the default network used by public cloud providers or you can choose a dedicated network connection for a better and faster form your internet service provider. " }, 
{ "title" : "Setting up Lyve Cloud validated tools ", 
"url" : "setting-up-lyve-cloud-validated-tools.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools ", 
"snippet" : "This section will help you to migrate the data from your existing cloud provider to Lyve Cloud using any of the validated data migration tools....", 
"body" : "This section will help you to migrate the data from your existing cloud provider to Lyve Cloud using any of the validated data migration tools. " }, 
{ "title" : "Using Cloudsfer ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-cloudsfer", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using Cloudsfer ", 
"snippet" : "Cloudsfer is validated for use with Lyve Cloud. Cloudsfer offers cloud data migration from on-premise to cloud or from cloud to cloud with over 20 cloud storage providers, including Dropbox, Google Drive, Box, and Egnyte.  Follow these instructions to use Cloudsfer with Lyve Cloud. Sign in to your C...", 
"body" : "Cloudsfer is validated for use with Lyve Cloud. Cloudsfer offers cloud data migration from on-premise to cloud or from cloud to cloud with over 20 cloud storage providers, including Dropbox, Google Drive, Box, and Egnyte.  Follow these instructions to use Cloudsfer with Lyve Cloud. Sign in to your Cloudsfer account. Select Migration Plans. Select the cloud provider as a Source. For example, select Amazon S3 . Enter the source credentials and select Connect . Select S3 compatible  as Target in the dropdown and enter the credentials. Enter the following details and select Submit . If the S3 buckets are hosted in different regions, the client talks to the us-east-1 region by default. Hence, the migration may not work other than us-east-1  region. Region endpoint URL for your buckets Access key Secret access key Select a Bucket Name from the Bucket Name list. Cloudsfer will list your buckets in the dropdown. Choose the bucket(s) to transfer the data. Verify the source and target details on the main page. Select Options on the right side, and then select Advanced . Select these optional settings if required, and then select Save . Cloudsfer displays the new migration plan. You can perform the following actions for the migration plan. Change the plan name Start the migration Modify the migration plan settings by clicking the Actions icon and selecting the appropriate action: Delete the migration plan Get migration status details To initiate the migration, select Start now , then select Execute on the confirmation. Switch to the Migration report summary and see all migration actions and their statuses. " }, 
{ "title" : "Using Flexify.IO ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-flexify-io", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using Flexify.IO ", 
"snippet" : "Flexify.IO (Flexify) is a cloud storage virtualization and migration solution that allows migrating data from one cloud provider to Lyve Cloud. To use Flexify.IO with Lyve Cloud: Create an account with Flexify.IO. In the Flexify.IO Management Console select the Data tab, then select Add Storage Acco...", 
"body" : "Flexify.IO (Flexify) is a cloud storage virtualization and migration solution that allows migrating data from one cloud provider to Lyve Cloud. To use Flexify.IO with Lyve Cloud: Create an account with Flexify.IO. In the Flexify.IO Management Console select the Data tab, then select Add Storage Account. Select from the list of providers, then select Add Storage. Where, Storage provider=Other Product=S3 Compatible Endpoint= Enter Lyve Cloud S3 API endpoint Enter Lyve Cloud’s Access Key ID and Secret Access Key. Select Encrypt transfer with SSL\/TLS and Refresh now by listing objects check box. Select Add Storage again and select your previous storage provider. Flexify.IO supports Amazon S3, Microsoft Azure, Google Cloud Storage and many other cloud storages. Enter the Secret Key and Access key for your previous storage provider and add the storage. Select Transfer Data. Chose mode (Copy or Move), buckets you want to migrate from, and Lyve Cloud as a destination. Optionally, select Advanced settings and fine-tune options such as conflict resolution, performance settings, or define a pattern to migrate only subset of the objects. Click Start Migration and monitor the migration progress. " }, 
{ "title" : "Using Movebot ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-movebot", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using Movebot ", 
"snippet" : "Couchdrops’s Movebot is a cloud-hosted service and is a next-generation cloud data migration tool that offers fast, simple and cost-effective cloud storage migrations. To use Movebot with Lyve Cloud Select MoveBot, then select Launch Migration Now. Sign in to MoveBot account and select Create new mi...", 
"body" : "Couchdrops’s Movebot is a cloud-hosted service and is a next-generation cloud data migration tool that offers fast, simple and cost-effective cloud storage migrations. To use Movebot with Lyve Cloud Select MoveBot, then select Launch Migration Now. Sign in to MoveBot account and select Create new migration, then choose Simple Migration. Configure the source. Storage Type: Select the source from the list. As an example, the image shows Amazon S3 as the Storage Type. S3 Access Key: S3 access key. S3 Access Key Secret: S3 secret key. S3 Bucket (optional): S3 Bucket name to migrate. The following image displays an example of the credentials after selecting Amazon S3 as the source. Enter the destination credentials: Storage Type: Select Lyve Cloud as the destination address. Lyve Cloud Region: Select the Lyve Cloud Region to be used.  (US East 1 or US West 1). Lyve Cloud Bucket:  Bucket created in Lyve Cloud to store data. For more information see  Listing buckets. Lyve Cloud Access Key: Enter Lyve Cloud's access key and secret access key. For more information, see  Creating a service account. Lyve Cloud Access and Secret Key: Enter Lyve Cloud's access key and secret access key. For more information, see  Creating a service account. Choose and select the folders to migrate data. Finalize the migration process by configuring the migration options, notification settings, and performance. Due to egress fees, these may affect the total migration cost. The dashboard displays the migration process. The following images display how the migration progress is tracked, where the status shows the current job status: Pending Scanning Creating directories Transitioning files and Permissions The Summary displays the migration status. Make sure all the data is transferred to Lyve Cloud. For example, use any third part tool like S3 browser, Cyberduck, etc. to check if the required data successfully transfers to Lyve Cloud. The Movebot dashboard displays all the configured data migration, and the migration is available to execute again. " }, 
{ "title" : "Using AWS CLI ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-aws-cli-41155", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using AWS CLI ", 
"snippet" : "The AWS CLI (Command Line Interface) tool is certified for use with Lyve Cloud.   Prerequisites Install AWS CLI   To use the AWS CLI with Lyve Cloud you need to download and install AWS CLI on a host, you cannot use AWS CLI to move data directly from a cloud to another cloud.  Before you start the m...", 
"body" : "The AWS CLI (Command Line Interface) tool is certified for use with Lyve Cloud.   Prerequisites Install AWS CLI   To use the AWS CLI with Lyve Cloud you need to download and install AWS CLI on a host, you cannot use AWS CLI to move data directly from a cloud to another cloud.  Before you start the migration, first download the data locally from the source cloud, then upload the data to the Lyve cloud.   These instructions guide you to download data locally from AWS S3 and upload it to Lyve Cloud: To access your AWS S3 buckets with the CLI, use the configure command. The configure command will bring up a series of prompts that creates the default profile. $ aws configure Enter the AWS S3 access key and secret access key. AWS Access Key ID [****************aa]:\n\nAWS Secret Access Key [****************aa]:\n\nDefault region name [a]:\n\nDefault output format [a]: Use the configure command to create an additional profile to access Lyve Cloud buckets aws configure --profile lyvecloud Enter Lyve Cloud's access key and secret access key. AWS Access Key ID [****************aa]:\n\nAWS Secret Access Key [****************aa]:\n\nDefault region name [a]:\n\nDefault output format [a]: Run the  aws configure list-profiles command to see all your configured profiles. aws configure list-profiles\n\ndefault\n\nlyvecloud Run the command to transfer all files from the AWS bucket to the local directory: mv- for moving the object(s) cp- for copying the object(s) aws s3 cp s3:\/\/<source_bucket>\/ <local_directory> --recursive Following is an example of the copy command aws s3 cp s3:\/\/3101\/ C:\\222 –recursive Transfer all files from the local directory to the destination bucket at LyveCloud aws s3 cp <local_directory>\/ \ns3:\/\/<destination_bucket>\/ --recursive --profile lyvecloud \n--endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com aws s3 cp C:\\222\\ s3:\/\/adi-migration\/ --recursive --endpoint-url=https:\/\/s3.us-east-1.lyvecloud.seagate.com Executing the command uploads all the three local files. When listing objects using the AWS CLI S3 ls  command  or S3 API list-objects  command in a bucket with object immutability enabled, the output will only display the current active object version. Previous object versions will not appear using these commands. For instance, if you have deleted an object, these commands may show the bucket as empty. To display previous versions of objects, use the following command:  $ aws --profile <YOUR RPOFILE> --endpoint https:\/\/s3.us-west-1.lyvecloud.seagate.com s3api list-object-versions --bucket <YOUR BUCKET> --prefix <PATH TO OBJECTS> ;  The output is in JSON format. Look at the Key and VersionID to identify objects and versions.  If you are unable to view your bucket contents with AWS  list-buckets  command, you must verify that your bucket has the correct permission settings. Check to verify that your permission settings apply to all buckets  or use a consistent prefix for your buckets that will determine consistent permissions. " }, 
{ "title" : "Using rclone ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-rclone-41155", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using rclone ", 
"snippet" : "This topic covers data migration from an existing cloud provider to Lyve Cloud.  You must install rclone on Linux, Windows or other UNIX systems. rclone reads data from the source cloud and sends it to Lyve Cloud. For more information, see,  Using rclone with Lyve Cloud . To migrate data: Set the So...", 
"body" : "This topic covers data migration from an existing cloud provider to Lyve Cloud.  You must install rclone on Linux, Windows or other UNIX systems. rclone reads data from the source cloud and sends it to Lyve Cloud. For more information, see,  Using rclone with Lyve Cloud . To migrate data: Set the Source and the Target as remote. For more information, see  Connecting to Lyve Cloud from Linux. Sync the Source and Target remote using rclone sync command. rclone sync <source remote name>:path <target remote name>:path Once the source and the target are synced, all the data from the source is copied, removed or migrated to the target remotely. You can use the following flags in the command to check the status of the sync\/copy\/migration. --progress: Displays the real-time transfer progress --interactive: Enables interactive mode and displays interactive for every action taken " }, 
{ "title" : "Using StorageDNA Fabric ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#using-storagedna-fabric", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using StorageDNA Fabric ", 
"snippet" : "StorageDNA Fabric (SDNA) is enterprise data management software that consists of a controller and data managers. Both components can be deployed locally or in the cloud on physical or virtual Linux computers. One of its features is Cloud-to-Cloud data migration. This topic covers migrating data from...", 
"body" : "StorageDNA Fabric (SDNA) is enterprise data management software that consists of a controller and data managers. Both components can be deployed locally or in the cloud on physical or virtual Linux computers. One of its features is Cloud-to-Cloud data migration. This topic covers migrating data from Azure Blob Storage to Lyve Cloud via the SDNA Fabric Object Migration feature. In this section, Azure Blob Storage is considered as one of the examples from various storage providers. Some of the steps may differ slightly depending on the source cloud provider. However, the overall process will be the same. To migrate the data you need to: Create configuration Create pools Create project Run the project " }, 
{ "title" : "Creating a configuration ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#creating-a-configuration", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using StorageDNA Fabric \/ Creating a configuration ", 
"snippet" : "You must create source and target configurations for the cloud providers. In this case, we have Microsoft Azure as a source cloud provider and Lyve Cloud as a target cloud provider. You can attach more than one bucket to the configuration. Before you start to configure the source for Microsoft Azure...", 
"body" : "You must create source and target configurations for the cloud providers. In this case, we have Microsoft Azure as a source cloud provider and Lyve Cloud as a target cloud provider. You can attach more than one bucket to the configuration. Before you start to configure the source for Microsoft Azure, ensure the following: Pre-requisites for Microsoft Azure and Lyve Cloud Login to your Azure portal and Lyve Cloud console. Copy the secret and access key value. These keys are required for source and target remote configurations. Microsoft Azure : Locate your storage account in Microsoft Azure and Copy the value from the Key1 field under Access Keys . Lyve Cloud : Copy the following: Access key and Secret key: For more information, see Creating a service accountBucket S3 endpoint URL: The endpoint is a URL specific to the bucket region. In this case, the bucket is in the US west region so the endpoint is https:\/\/s3.us-west-1.lyvecloud.seagate.com. For more information, see S3 endpoint URL Region: For more information, see S3 API endpointsSource remote (Microsoft Azure) configuration To manage your remote configurations: Login to DNA Fabric using credentials and then select Settings . In Remote Configurations, select Add to add a new remote configuration: Select Provider like Microsoft Azure from the drop down. Enter the Config Name . In the Edit the remote configuration settings , update the following: account : Microsoft Azure account name key : Microsoft Azure account access key. To locate the access key, see Pre-requisites for Microsoft Azure and Lyve Cloud. Target remote (Lyve Cloud) configuration Follow the first two steps as mentioned in Source remote configuration . In the Edit the remote configuration settings update the following: Access key Secret Key Endpoint Region " }, 
{ "title" : "Creating pools ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#creating-pools", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using StorageDNA Fabric \/ Creating pools ", 
"snippet" : "Pools are created to migrate data from one cloud to another. You must create a remote pool for source and target providers before creating a project. One pool for source and target allows you to migrate objects for that bucket (container), so if there are more buckets to migrate, you must create a s...", 
"body" : "Pools are created to migrate data from one cloud to another. You must create a remote pool for source and target providers before creating a project. One pool for source and target allows you to migrate objects for that bucket (container), so if there are more buckets to migrate, you must create a source and target pool for each bucket. For example: If there are four buckets (B1, B2, B3, B4) in Microsoft Azure and you want to migrate all four buckets to Lyve Cloud. In this case, you must create four pools for the source and four pools for the target. In each pool, you select the configuration and select the buckets available for the configuration. To create pools Select Pools tab. In the Project Type, select Object Migration from the list. In the Pool Type, select Remote Sources from the list. Click + to create a new storage object and enter the following: Label of remote source: Enter name of the remote source. This label appears on the project selection screen. Client Machine: Select the client machine. Select Configuration: Select the configuration of the source cloud. For information on source configuration, see Creating configuration. Select Bucket: Choose the bucket to migrate data. The bucket list displays all the buckets (containers) available in Microsoft Azure. " }, 
{ "title" : "Creating projects ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#creating-projects", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using StorageDNA Fabric \/ Creating projects ", 
"snippet" : "Migration is conducted by defining projects. When creating a project, you specify the source and target with buckets or sub-folders, also called a prefix in S3 terminology. You must create different projects if more than one bucket is required to migrate. For example, if you have two buckets (B1 and...", 
"body" : "Migration is conducted by defining projects. When creating a project, you specify the source and target with buckets or sub-folders, also called a prefix in S3 terminology. You must create different projects if more than one bucket is required to migrate. For example, if you have two buckets (B1 and B2) in Microsoft Azure that you want to migrate separately in Lyve Cloud, you must create two projects. Each project has one source and one target bucket attached. Once the project is created, you must run this project to start the migration. To create a project: In the Projects tab, select + to create a new project. In the project creation wizard: In the section, Select Your Projects from the List Below select the required option and click Next . In the Workflow filter , select Data Mobility and Cloud Object and Multi-Site Workflows and then select Object to Object Replication for Backup, Archieve, Sharing from the list. Select the following and select Next . In The Basics section: Enter the name of your project Select the owner of the project from the list Enter the project Description In the Source Machine section: Select the source and target machine. Select your disk source and cloud target from the list. Enter the credentials to complete the job creation on the selected system and select Finish . After the project is created it appears in the list of projects in central projects. " }, 
{ "title" : "Running projects ", 
"url" : "setting-up-lyve-cloud-validated-tools.html#running-projects", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Cloud data migration \/ Setting up Lyve Cloud validated tools \/ Using StorageDNA Fabric \/ Running projects ", 
"snippet" : "You must run a project manually. Specify the source data within the source bucket container. To manually run the project: In the Projects tab, select the project you want to run. Select Store , and verify the Store Job Summary and Folder Selection before you start running the job. If you want to mig...", 
"body" : "You must run a project manually. Specify the source data within the source bucket container. To manually run the project: In the Projects tab, select the project you want to run. Select Store , and verify the Store Job Summary and Folder Selection before you start running the job. If you want to migrate the entire source bucket in Add Folder , specify \/ ’ as source data. If you want to migrate one or more subfolders within this bucket, specify the subfolder names. In the Action Toolbar select Start Job to manually run the job. The Job Process Status diaglog displays the status of the project where the running job can be monitored. After the job is complete, you can view the contents of any S3 client in the target bucket. The data remains in the same format as within the source bucket. " }, 
{ "title" : "How to create a pre-signed URL ", 
"url" : "how-to-create-a-pre-signed-url.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ How to create a pre-signed URL ", 
"snippet" : "To share an object without granting someone access to your storage account? You can share individual objects with the security and time-sensitivity of a pre-signed URL. In Lyve Cloud, console admins can set permissions to allow S3 clients access to data objects. Objects are only accessible by provid...", 
"body" : "To share an object without granting someone access to your storage account? You can share individual objects with the security and time-sensitivity of a pre-signed URL. In Lyve Cloud, console admins can set permissions to allow S3 clients access to data objects. Objects are only accessible by providing access and secret keys to the S3 client. However, objects can be shared with anyone by providing a pre-signed URL allowing temporary access to the object. Pre-signed URLs are time-sensitive and allow any recipient with the URL to download an object. For example, if you store a video recording in a Lyve Cloud bucket, you can share the file by creating a pre-signed URL. " }, 
{ "title" : "Pre-requisites ", 
"url" : "how-to-create-a-pre-signed-url.html#pre-requisites", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ How to create a pre-signed URL \/ Pre-requisites ", 
"snippet" : "Download a command line tool such AWS CLI. Provide S3 Client access to Lyve Cloud bucket(s). For instructions, see  Connecting S3 clients....", 
"body" : "Download a command line tool such AWS CLI. Provide S3 Client access to Lyve Cloud bucket(s). For instructions, see  Connecting S3 clients. " }, 
{ "title" : "Creating a pre-signed URL for download operations ", 
"url" : "how-to-create-a-pre-signed-url.html#creatinga-pre-signed-urlfordownloadoperations", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ How to create a pre-signed URL \/ Creating a pre-signed URL for download operations ", 
"snippet" : "To create a pre-signed URL to download an object: Use the S3 client to request an object residing in your Lyve Cloud bucket. The following instructions generate a pre-signed URL to share an object for a designated period of time. Open your command line application (Command Prompt for PC, Terminal fo...", 
"body" : "To create a pre-signed URL to download an object: Use the S3 client to request an object residing in your Lyve Cloud bucket. The following instructions generate a pre-signed URL to share an object for a designated period of time. Open your command line application (Command Prompt for PC, Terminal for Mac) and use the following command to configure your profile: configure --profile (profile name)  C:\\Users\\693611&gt;aws configure --profile adr Enter your bucket’s access key, secret key, region name, and output format: AWS Access Key ID [None]: Enter access key ID. AWS Secret Access Key [None]: Enter secret access key Default region name [None]: Enter default region name (us-east or us-west) Default output format [None]: Enter default output format (optional) Example AWS Access Key ID [None]: **************QGS AWS Secret Access Key [None]: *****************************3CJ Default region name [None]: us-east-1 Default output format [None]: Enter the following command to list your buckets: S3 ls --profile (enter profile name) –-endpoint URL  Example C:\\Users\\693611>aws s3 ls --profile adr --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com    Result 2021-06-08 15:12:58 ahtestbucket  Enter the following command to generate a presigned URL. Default expiration time can vary from client to client. S3 presign s3:\/\/bucketname\/objectfile --profile (profile name) --endpoint URL   Example C:\\Users\\693611>aws s3 presign s3:\/\/ahtestbucket\/certificate.pdf --profile adr  --endpoint https:\/\/s3.us-east-1.lyvecloud.seagate.com Example  Result https:\/\/s3.us-east-1.lyvecloud.seagate.com\/ahtestbucket\/certificate.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=YKANULVJJF5ASGQS%2F20211202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211202T152353Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=e97e4b48c15bfa2f3b724fc9c23b8a4cd8bc324d434d67a6fe31e44a241adaf3  Copy the generated URL and share your object. If you experience errors viewing the URL, check to ensure the bucket and object name in the command line follow the same lowercase and uppercase structure in your bucket. CLI is case sensitive. " }, 
{ "title" : "Conclusion ", 
"url" : "how-to-create-a-pre-signed-url.html#conclusion", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ How to create a pre-signed URL \/ Conclusion ", 
"snippet" : "With pre-signed URLs, you can provide temporary access to an object in Lyve Cloud. Pre-signed URL is an efficient and effective way to provide access to individual files without giving access to your storage account. Easily create and share a URL in minutes, using S3 clients with Lyve Cloud storage....", 
"body" : "With pre-signed URLs, you can provide temporary access to an object in Lyve Cloud. Pre-signed URL is an efficient and effective way to provide access to individual files without giving access to your storage account. Easily create and share a URL in minutes, using S3 clients with Lyve Cloud storage. " }, 
{ "title" : "Video Library ", 
"url" : "video-library.html", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library ", 
"snippet" : "Need more information? Watch instructional videos categorized by topic to accelerate productivity in no time....", 
"body" : "Need more information? Watch instructional videos categorized by topic to accelerate productivity in no time. " }, 
{ "title" : "Getting started ", 
"url" : "video-library.html#getting-started", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started ", 
"snippet" : "The Getting Started playlist provides an overview that covers the initial steps to begin uploading and managing data in Lyve Cloud. For more information, see Quick Start Guide....", 
"body" : "The Getting Started playlist provides an overview that covers the initial steps to begin uploading and managing data in Lyve Cloud. For more information, see Quick Start Guide. " }, 
{ "title" : "Getting started with Lyve Cloud ", 
"url" : "video-library.html#getting-started-with-lyve-cloud", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started \/ Getting started with Lyve Cloud ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to access and manage data? ", 
"url" : "video-library.html#how-to-access-and-manage-data-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started \/ How to access and manage data? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to create a bucket? ", 
"url" : "video-library.html#how-to-create-a-bucket-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started \/ How to create a bucket? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to prevent objects from being deleted? ", 
"url" : "video-library.html#how-to-prevent-objects-from-being-deleted-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started \/ How to prevent objects from being deleted? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to enable log operations? ", 
"url" : "video-library.html#how-to-enable-log-operations-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Getting started \/ How to enable log operations? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Connecting S3 Clients ", 
"url" : "video-library.html#connecting-s3-clients-38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Connecting S3 Clients ", 
"snippet" : "Learn how to connect S3 clients to view, upload, download easily,  and delete objects on Lyve Cloud storage. For more information, see Connecting S3 clients. ...", 
"body" : "Learn how to connect S3 clients to view, upload, download easily,  and delete objects on Lyve Cloud storage. For more information, see Connecting S3 clients.  " }, 
{ "title" : "How to use Cyberduck? ", 
"url" : "video-library.html#how-to-use-cyberduck-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Connecting S3 Clients \/ How to use Cyberduck? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to use S3 Browser? ", 
"url" : "video-library.html#how-to-use-s3-browser-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Connecting S3 Clients \/ How to use S3 Browser? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to use Mountain Duck? ", 
"url" : "video-library.html#how-to-use-mountain-duck-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Connecting S3 Clients \/ How to use Mountain Duck? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Managing Users ", 
"url" : "video-library.html#managing-users", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Managing Users ", 
"snippet" : "Learn how to manage access and data on the console. For more information, see Console high-level workflow....", 
"body" : "Learn how to manage access and data on the console. For more information, see Console high-level workflow. " }, 
{ "title" : "How to add notification recipients? ", 
"url" : "video-library.html#how-to-add-notification-recipients-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Managing Users \/ How to add notification recipients? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to Manage Users and Assign Roles? ", 
"url" : "video-library.html#how-to-manage-users-and-assign-roles-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Managing Users \/ How to Manage Users and Assign Roles? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Sub-Accounts ", 
"url" : "video-library.html#sub-accounts-38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Sub-Accounts ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "How to use Sub-Account Feature of Lyve Cloud? ", 
"url" : "video-library.html#how-to-use-sub-account-feature-of-lyve-cloud-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Sub-Accounts \/ How to use Sub-Account Feature of Lyve Cloud? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud Compute by Zadara ", 
"url" : "video-library.html#lyve-cloud-compute-by-zadara", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Lyve Cloud Compute by Zadara ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Lyve Cloud Compute by Zadara console overview ", 
"url" : "video-library.html#lyve-cloud-compute-by-zadara-console-overview", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Lyve Cloud Compute by Zadara \/ Lyve Cloud Compute by Zadara console overview ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to create an instance? ", 
"url" : "video-library.html#how-to-create-an-instance--38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Lyve Cloud Compute by Zadara \/ How to create an instance? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Support ", 
"url" : "video-library.html#support-38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Support ", 
"snippet" : "Need help? Learn how Lyve Cloud provides support to all customers. For more information, see  Managing support tickets....", 
"body" : "Need help? Learn how Lyve Cloud provides support to all customers. For more information, see  Managing support tickets. " }, 
{ "title" : "How to create a support ticket? ", 
"url" : "video-library.html#how-to-create-a-support-ticket-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Support \/ How to create a support ticket? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Introduction to Customer Success Manager ", 
"url" : "video-library.html#introduction-to-customer-success-manager", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Support \/ Introduction to Customer Success Manager ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Industry Use Cases ", 
"url" : "video-library.html#industry-use-cases", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Industry Use Cases ", 
"snippet" : "Watch the Industry Playlist to learn how Lyve Cloud satisfies a variety of industry verticals....", 
"body" : "Watch the Industry Playlist to learn how Lyve Cloud satisfies a variety of industry verticals. " }, 
{ "title" : "Lyve Cloud for video storage management ", 
"url" : "video-library.html#lyve-cloud-for-video-storage-management", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Industry Use Cases \/ Lyve Cloud for video storage management ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Partner Solution Demos ", 
"url" : "video-library.html#partner-solution-demos", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos ", 
"snippet" : "Learn how to configure Lyve Cloud to a variety of applications step-by-step....", 
"body" : "Learn how to configure Lyve Cloud to a variety of applications step-by-step. " }, 
{ "title" : "Lyve Cloud with Rclone ", 
"url" : "video-library.html#lyve-cloud-with-rclone", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Rclone ", 
"snippet" : "The following videos help you to use, Copy to and Copy sync, list, delete and purge commands using Rclone. For more information on Rclone, see Using Rclone....", 
"body" : "The following videos help you to use, Copy to and Copy sync, list, delete and purge commands using Rclone. For more information on Rclone, see Using Rclone. " }, 
{ "title" : "How to use Rclone? ", 
"url" : "video-library.html#how-to-use-rclone-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Rclone \/ How to use Rclone? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to use Copy-to and Copy-sync commands using Rclone? ", 
"url" : "video-library.html#how-to-use-copy-to-and-copy-sync-commands-using-rclone-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Rclone \/ How to use Copy-to and Copy-sync commands using Rclone? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to list commands using Rclone? ", 
"url" : "video-library.html#how-to-list-commands-using-rclone-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Rclone \/ How to list commands using Rclone? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to use Rclone Delete and Purge commands? ", 
"url" : "video-library.html#how-to-use-rclone-delete-and-purge-commands--38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Rclone \/ How to use Rclone Delete and Purge commands? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud with Cloud boost ", 
"url" : "video-library.html#lyve-cloud-with-cloud-boost", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Cloud boost ", 
"snippet" : "Learn how to configure network settings for the Cloud Boost appliance. For more information, see Lyve Cloud with CloudBoost. You can also Add Lyve Cloud to Data Domain Cloud Tier and Create a Data Domain Cloud Tier device in Networker, For more information, see Lyve Cloud with Dell Networker Data Do...", 
"body" : "Learn how to configure network settings for the Cloud Boost appliance. For more information, see Lyve Cloud with CloudBoost. You can also Add Lyve Cloud to Data Domain Cloud Tier and Create a Data Domain Cloud Tier device in Networker, For more information, see Lyve Cloud with Dell Networker Data Domain. " }, 
{ "title" : "How to create a CloudBoost profile? ", 
"url" : "video-library.html#how-to-create-a-cloudboost-profile-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Cloud boost \/ How to create a CloudBoost profile? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "How to login and add a Cloud Boost device? ", 
"url" : "video-library.html#how-to-login-and-add-a-cloud-boost-device-", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Cloud boost \/ How to login and add a Cloud Boost device? ", 
"snippet" : "...", 
"body" : "[video] " }, 
{ "title" : "Lyve Cloud with Commvault ", 
"url" : "video-library.html#lyve-cloud-with-commvault-38613", 
"breadcrumbs" : "Lyve Cloud Documentation \/ Resources \/ Video Library \/ Partner Solution Demos \/ Lyve Cloud with Commvault ", 
"snippet" : "Installing and Configuring Commvault with Lyve Cloud...", 
"body" : "[video] Installing and Configuring Commvault with Lyve Cloud " }
]
$(document).trigger('search.ready');
});